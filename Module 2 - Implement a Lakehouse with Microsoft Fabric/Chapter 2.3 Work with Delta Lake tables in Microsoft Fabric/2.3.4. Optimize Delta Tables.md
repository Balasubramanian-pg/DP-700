Spark is a parallel-processing framework, with data stored on one or more worker nodes. In addition, Parquet files are immutable, with new files written for every update or delete. This process can result in Spark storing data in a large number of small files, known as the _small file problem._ It means that queries over large amounts of data can run slowly, or even fail to complete.

## OptimizeWrite function

_OptimizeWrite_ is a feature of Delta Lake which reduces the number of files as they're written. Instead of writing many small files, it writes fewer larger files. This helps to prevent the _small files problem_ and ensure that performance isn't degraded.

![Diagram showing how Optimize Write writes fewer large files.](https://learn.microsoft.com/en-us/training/wwl/work-delta-lake-tables-fabric/media/optimize-write.png)

In Microsoft Fabric, `OptimizeWrite` is enabled by default. You can enable or disable it at the Spark session level:



```Python
# Disable Optimize Write at the Spark session level
spark.conf.set("spark.microsoft.delta.optimizeWrite.enabled", False)

# Enable Optimize Write at the Spark session level
spark.conf.set("spark.microsoft.delta.optimizeWrite.enabled", True)

print(spark.conf.get("spark.microsoft.delta.optimizeWrite.enabled"))
```


> [!NOTE]
> `OptimizeWrite` can also be set in Table Properties and for individual write commands.

## Optimize

Optimize is a table maintenance feature that consolidates small Parquet files into fewer large files. You might run Optimize after loading large tables, resulting in:

- fewer larger files
- better compression
- efficient data distribution across nodes

![Diagram showing how Optimize consolidates Parquet files.](https://learn.microsoft.com/en-us/training/wwl/work-delta-lake-tables-fabric/media/optimize-command.png)

To run Optimize:

1. In **Lakehouse Explorer**, select the ... menu beside a table name and select **Maintenance**.
2. Select **Run OPTIMIZE command**.
3. Optionally, select **Apply V-order to maximize reading speeds in Fabric**.
4. Select **Run now**.

### V-Order function

When you run Optimize, you can optionally run V-Order, which is designed for the Parquet file format in Fabric. V-Order enables lightning-fast reads, with in-memory-like data access times. It also improves cost efficiency as it reduces network, disk, and CPU resources during reads.

V-Order is enabled by default in Microsoft Fabric and is applied as data is being written. It incurs a small overhead of about 15% making writes a little slower. However, V-Order enables faster reads from the Microsoft Fabric compute engines, such as Power BI, SQL, Spark, and others.

In Microsoft Fabric, the Power BI and SQL engines use Microsoft Verti-Scan technology which takes full advantage of V-Order optimization to speed up reads. Spark and other engines don't use VertiScan technology but still benefit from V-Order optimization by about 10% faster reads, sometimes up to 50%.

V-Order works by applying special sorting, row group distribution, dictionary encoding, and compression on Parquet files. It's 100% compliant to the open-source Parquet format and all Parquet engines can read it.

V-Order might not be beneficial for write-intensive scenarios such as staging data stores where data is only read once or twice. In these situations, disabling V-Order might reduce the overall processing time for data ingestion.

Apply V-Order to individual tables by using the Table Maintenance feature by running the `OPTIMIZE` command.

![Screen picture of table maintenance with V-order selected](https://learn.microsoft.com/en-us/training/wwl/work-delta-lake-tables-fabric/media/table-maintenance-v-order.png)

## Vacuum

The VACUUM command enables you to remove old data files.

Every time an update or delete is done, a new Parquet file is created and an entry is made in the transaction log. Old Parquet files are retained to enable time travel, which means that Parquet files accumulate over time.

The VACUUM command removes old Parquet data files, but not the transaction logs. When you run VACUUM, you can't time travel back earlier than the retention period.

![Diagram showing how vacuum works.](https://learn.microsoft.com/en-us/training/wwl/work-delta-lake-tables-fabric/media/how-vacuum-works.png)

Data files that aren't currently referenced in a transaction log and that are older than the specified retention period are permanently deleted by running VACUUM. Choose your retention period based on factors such as:

- Data retention requirements
- Data size and storage costs
- Data change frequency
- Regulatory requirements

The default retention period is 7 days (168 hours), and the system prevents you from using a shorter retention period.

You can run VACUUM on an ad-hoc basis or scheduled using Fabric notebooks.

Run VACUUM on individual tables by using the Table maintenance feature:

1. In **Lakehouse Explorer**, select the ... menu beside a table name and select **Maintenance**.
2. Select **Run VACUUM command using retention threshold** and set the retention threshold.
3. Select **Run now**.

![Screen picture showing the table maintenance options.](https://learn.microsoft.com/en-us/training/wwl/work-delta-lake-tables-fabric/media/table-maintenance-vacuum.png)

You can also run **VACUUM** as a SQL command in a notebook:

```SQL
%%sql
VACUUM lakehouse2.products RETAIN 168 HOURS;
```

VACUUM commits to the Delta transaction log, so you can view previous runs in DESCRIBE HISTORY.

```SQL
%%sql
DESCRIBE HISTORY lakehouse2.products;
```

## Partitioning Delta tables

Delta Lake allows you to organize data into partitions. This might improve performance by enabling _data skipping_, which boosts performance by skipping over irrelevant data objects based on an object's metadata.

Consider a situation where large amounts of sales data are being stored. You could partition sales data by year. The partitions are stored in subfolders named "year=2021", "year=2022", etc. If you only want to report on sales data for 2024, then the partitions for other years can be skipped, which improves read performance.

Partitioning of small amounts of data can degrade performance, however, because it increases the number of files and can exacerbate the "small files problem."

Use partitioning when:

- You have very large amounts of data.
- Tables can be split into a few large partitions.

Don't use partitioning when:

- Data volumes are small.
- A partitioning column has high cardinality, as this creates a large number of partitions.
- A partitioning column would result in multiple levels.

![Diagram showing partitioning by one or more columns.](https://learn.microsoft.com/en-us/training/wwl/work-delta-lake-tables-fabric/media/partitioning.png)

Partitions are a fixed data layout and don't adapt to different query patterns. When considering how to use partitioning, think about how your data is used, and its granularity.

In this example, a DataFrame containing product data is partitioned by Category:

```Python
df.write.format("delta").partitionBy("Category").saveAsTable("partitioned_products", path="abfs_path/partitioned_products")
```

In the Lakehouse Explorer, you can see the data is a partitioned table.

- There's one folder for the table, called "partitioned_products."
- There are subfolders for each category, for example "Category=Bike Racks", etc.

![Screen picture of the lakehouse explorer and the product file partitioned by category.](https://learn.microsoft.com/en-us/training/wwl/work-delta-lake-tables-fabric/media/explorer-partitioned-table.png)

We can create a similar partitioned table using SQL:

```SQL
%%sql
CREATE TABLE partitioned_products (
    ProductID INTEGER,
    ProductName STRING,
    Category STRING,
    ListPrice DOUBLE
)
PARTITIONED BY (Category);
```

---
## Understanding Delta Table Partitioning

Partitioning in Delta tables creates a physical data organization strategy that can dramatically impact query performance, but it requires careful consideration of your specific use cases and query patterns.

### Physical Structure of Partitioned Tables

When you partition a table, Delta Lake creates a hierarchical folder structure:

```
partitioned_products/
├── _delta_log/
│   ├── 00000000000000000000.json
│   └── 00000000000000000001.json
├── Category=Bike Racks/
│   ├── part-00000-xxx.parquet
│   └── part-00001-xxx.parquet
├── Category=Helmets/
│   ├── part-00000-xxx.parquet
│   └── part-00001-xxx.parquet
├── Category=Road Bikes/
│   ├── part-00000-xxx.parquet
│   └── part-00002-xxx.parquet
└── Category=Mountain Bikes/
    ├── part-00000-xxx.parquet
    └── part-00003-xxx.parquet
```

This structure enables **partition pruning** - the query engine can skip entire folders when filtering on partition columns, dramatically reducing the amount of data that needs to be scanned.

## Strategic Partitioning Considerations

### Query Pattern Analysis

Before implementing partitioning, analyze your typical query patterns:

```python
# Example: Analyzing query patterns for partitioning decisions

# Pattern 1: Time-based queries (excellent for date partitioning)
spark.sql("""
SELECT SUM(sales_amount) 
FROM sales_data 
WHERE sale_date BETWEEN '2024-01-01' AND '2024-01-31'
""")

# Pattern 2: Category-based analysis (good for category partitioning)
spark.sql("""
SELECT AVG(price) 
FROM products 
WHERE category = 'Electronics'
""")

# Pattern 3: Multi-dimensional filtering (consider multi-level partitioning)
spark.sql("""
SELECT COUNT(*) 
FROM customer_transactions 
WHERE region = 'North America' 
AND transaction_date >= '2024-01-01'
""")
```

### Cardinality and Data Distribution

**High Cardinality Problems**:

```python
# BAD: Too many partitions (creates small files)
df.write.format("delta").partitionBy("customer_id").saveAsTable("bad_partitioned")
# This could create millions of tiny partitions

# BETTER: Partition by a column with moderate cardinality
df.write.format("delta").partitionBy("region").saveAsTable("better_partitioned")
# This creates manageable number of partitions (e.g., 10-50)
```

**Low Cardinality Problems**:

```python
# BAD: Too few partitions (doesn't provide benefits)
df.write.format("delta").partitionBy("active_flag").saveAsTable("ineffective_partitioned")
# Only creates 2 partitions (true/false)

# BETTER: Combine with other columns or use different strategy
df.write.format("delta").partitionBy("region", "product_category").saveAsTable("multi_level_partitioned")
```

## Advanced Partitioning Strategies

### Time-Based Partitioning Patterns

```python
from pyspark.sql.functions import year, month, dayofmonth, date_format

# Yearly partitioning for long-term historical data
df.withColumn("year", year("transaction_date")) \
  .write.format("delta") \
  .partitionBy("year") \
  .saveAsTable("yearly_transactions")

# Monthly partitioning for medium-term analysis
df.withColumn("year_month", date_format("transaction_date", "yyyy-MM")) \
  .write.format("delta") \
  .partitionBy("year_month") \
  .saveAsTable("monthly_transactions")

# Daily partitioning for recent, high-volume data
df.withColumn("date", date_format("transaction_date", "yyyy-MM-dd")) \
  .write.format("delta") \
  .partitionBy("date") \
  .saveAsTable("daily_transactions")

# Hierarchical time partitioning
df.withColumn("year", year("transaction_date")) \
  .withColumn("month", month("transaction_date")) \
  .write.format("delta") \
  .partitionBy("year", "month") \
  .saveAsTable("hierarchical_time_partitioned")
```

### Business Logic Partitioning

```python
# Geographic partitioning
customer_df.write.format("delta") \
  .partitionBy("country", "state") \
  .saveAsTable("geographic_customers")

# Product hierarchy partitioning  
product_df.write.format("delta") \
  .partitionBy("department", "category") \
  .saveAsTable("product_hierarchy")

# Custom business partitioning
sales_df.withColumn("sales_tier", 
    when(col("amount") > 10000, "high_value")
    .when(col("amount") > 1000, "medium_value")
    .otherwise("low_value")
).write.format("delta") \
  .partitionBy("sales_tier", "region") \
  .saveAsTable("business_tiered_sales")
```

### Hash-Based Partitioning for Even Distribution

```python
from pyspark.sql.functions import hash, abs

# Create hash-based partitions for even distribution
df.withColumn("hash_partition", abs(hash("customer_id")) % 10) \
  .write.format("delta") \
  .partitionBy("hash_partition") \
  .saveAsTable("hash_distributed_customers")

# Combine hash partitioning with business logic
df.withColumn("hash_bucket", abs(hash("transaction_id")) % 20) \
  .write.format("delta") \
  .partitionBy("region", "hash_bucket") \
  .saveAsTable("hybrid_partitioned_transactions")
```

## Dynamic Partition Management

### Partition Discovery and Metadata

```python
# Analyze partition structure
def analyze_partition_structure(table_name):
    """Analyze partition distribution and file sizes"""
    
    # Get partition information
    partitions = spark.sql(f"SHOW PARTITIONS {table_name}").collect()
    
    print(f"Total partitions: {len(partitions)}")
    
    # Analyze file counts and sizes per partition
    for partition in partitions[:5]:  # Show first 5 partitions
        partition_path = partition[0]
        
        # Count files in partition
        files = spark.sql(f"""
        DESCRIBE DETAIL {table_name}
        """).select("numFiles").collect()[0][0]
        
        print(f"Partition: {partition_path}")
        print(f"Files: {files}")

analyze_partition_structure("partitioned_products")
```

### Partition Pruning Verification

```python
# Verify partition pruning is working
def verify_partition_pruning(query):
    """Check if partition pruning is effective"""
    
    # Enable query plan logging
    spark.conf.set("spark.sql.adaptive.enabled", "true")
    
    # Execute query and examine plan
    df = spark.sql(query)
    plan = df.explain(extended=True)
    
    # Look for partition pruning indicators in the plan
    return df
```

### Partition Evolution Strategies

```python
def evolve_partitioning_strategy(source_table, new_partition_cols):
    """Migrate to new partitioning strategy"""
    
    # Read existing data
    df = spark.table(source_table)
    
    # Create new partitioned version
    temp_table = f"{source_table}_temp"
    df.write.format("delta") \
      .mode("overwrite") \
      .partitionBy(*new_partition_cols) \
      .saveAsTable(temp_table)
    
    # Atomic swap (would need additional orchestration in production)
    print(f"New partitioned version created as {temp_table}")
    print("Manual verification and swap required")
```

## Performance Optimization with Partitioning

### Optimal File Sizing

```python
# Configure for optimal file sizes within partitions
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.databricks.delta.optimizeWrite.enabled", "true")

# Write with file size considerations
large_df.coalesce(100) \
  .write.format("delta") \
  .mode("overwrite") \
  .partitionBy("region") \
  .saveAsTable("optimized_partitioned_table")
```

### Z-Ordering Within Partitions

```python
from delta.tables import DeltaTable

# Apply Z-ordering within partitions for additional performance
delta_table = DeltaTable.forName(spark, "partitioned_products")
delta_table.optimize().executeZOrderBy("price", "brand")

# Z-ordering is particularly effective when combined with partitioning
# Partitioning handles one dimension, Z-ordering optimizes within partitions
```

### Partition-Aware Operations

```python
# Efficient partition-aware updates
delta_table = DeltaTable.forName(spark, "partitioned_sales")

# Update specific partitions efficiently
delta_table.update(
    condition = (col("region") == "North America") & (col("status") == "pending"),
    set = {"status": lit("processed")}
)

# Partition-aware deletes
delta_table.delete(
    condition = (col("transaction_date") < "2023-01-01")
)
```

## Common Partitioning Patterns and Anti-Patterns

### Effective Patterns

```python
# Pattern 1: Time-based with business logic
events_df.withColumn("event_date", col("timestamp").cast("date")) \
  .write.format("delta") \
  .partitionBy("event_date", "event_type") \
  .saveAsTable("events_partitioned")

# Pattern 2: Geographic with temporal
sales_df.write.format("delta") \
  .partitionBy("country", "sale_month") \
  .saveAsTable("geographic_temporal_sales")

# Pattern 3: Balanced hierarchy
user_activity_df.write.format("delta") \
  .partitionBy("app_version", "user_segment") \
  .saveAsTable("balanced_user_activity")
```

### Anti-Patterns to Avoid

```python
# ANTI-PATTERN 1: Too many partition columns
# df.write.format("delta").partitionBy("year", "month", "day", "hour", "region", "category")

# ANTI-PATTERN 2: High cardinality partitioning
# df.write.format("delta").partitionBy("user_id")  # Millions of partitions

# ANTI-PATTERN 3: Uneven data distribution
# df.write.format("delta").partitionBy("is_premium")  # Only 2 partitions, likely uneven

# ANTI-PATTERN 4: Partitioning on frequently changing values
# df.write.format("delta").partitionBy("last_modified_timestamp")
```

## Monitoring and Maintenance

### Partition Health Monitoring

```python
def monitor_partition_health(table_name):
    """Monitor partition distribution and file health"""
    
    # Get table details
    table_detail = spark.sql(f"DESCRIBE DETAIL {table_name}").collect()[0]
    
    print(f"Table: {table_name}")
    print(f"Total files: {table_detail['numFiles']}")
    print(f"Total size: {table_detail['sizeInBytes'] / (1024**3):.2f} GB")
    
    # Check for small files (potential performance issue)
    avg_file_size = table_detail['sizeInBytes'] / table_detail['numFiles']
    
    if avg_file_size < 10 * 1024 * 1024:  # Less than 10MB
        print("WARNING: Small file problem detected")
        print("Consider running OPTIMIZE operation")
    
    # Check partition count
    partition_count = spark.sql(f"SHOW PARTITIONS {table_name}").count()
    print(f"Partition count: {partition_count}")
    
    if partition_count > 1000:
        print("WARNING: High partition count may impact metadata operations")

monitor_partition_health("partitioned_products")
```

### Automated Optimization

```python
def optimize_partitioned_table(table_name, z_order_columns=None):
    """Automated optimization for partitioned tables"""
    
    delta_table = DeltaTable.forName(spark, table_name)
    
    # Basic optimization
    optimize_result = delta_table.optimize()
    
    # Apply Z-ordering if specified
    if z_order_columns:
        optimize_result = optimize_result.executeZOrderBy(*z_order_columns)
    else:
        optimize_result.execute()
    
    # Clean up old files
    delta_table.vacuum(retentionHours=168)  # 7 days
    
    print(f"Optimization completed for {table_name}")
```

## Decision Framework for Partitioning

### Partitioning Decision Tree

```python
def recommend_partitioning_strategy(df, query_patterns):
    """Recommend partitioning strategy based on data and query patterns"""
    
    recommendations = []
    
    # Analyze data volume
    row_count = df.count()
    if row_count < 1_000_000:
        recommendations.append("Consider no partitioning for small datasets")
    
    # Analyze column cardinality
    for col_name in df.columns:
        distinct_count = df.select(col_name).distinct().count()
        cardinality_ratio = distinct_count / row_count
        
        if 0.001 < cardinality_ratio < 0.1:  # 0.1% to 10% unique values
            recommendations.append(f"Good partitioning candidate: {col_name}")
    
    # Analyze query patterns
    if "time_range_queries" in query_patterns:
        recommendations.append("Consider time-based partitioning")
    
    if "geographic_queries" in query_patterns:
        recommendations.append("Consider geographic partitioning")
    
    return recommendations
```

## Best Practices Summary

### 1. Start Simple

Begin with single-column partitioning on your most common filter column, then evolve based on performance observations.

### 2. Monitor and Measure

Implement monitoring to track query performance, file sizes, and partition distribution.

### 3. Balance Granularity

Aim for partitions that contain 1-10GB of data and result in 10-1000 total partitions.

### 4. Consider Evolution

Design partitioning strategies that can evolve as your data and query patterns change.

### 5. Test with Real Workloads

Always test partitioning strategies with realistic data volumes and actual query patterns.

Partitioning is a powerful optimization technique, but it requires careful planning and ongoing maintenance to be effective. The key is understanding your specific use case and continuously monitoring the impact on performance.