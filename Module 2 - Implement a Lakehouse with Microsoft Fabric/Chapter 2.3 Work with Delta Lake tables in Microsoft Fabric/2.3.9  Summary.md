# The Powerhouse Behind Your Lakehouse: Delta Lake

**Delta Lake** is the open-source technology that serves as the transactional storage layer for Apache Spark. **All tables in a Microsoft Fabric lakehouse are Delta Lake tables by default.** This is not just a technical detail; it's the core feature that infuses your data lake with the reliability, performance, and advanced capabilities of a traditional relational database.

By building on Delta Lake, Fabric enables you to use the powerful Delta Lake API to manage and query your data in sophisticated ways.

<div align-center">
<svg width="200" height="120" viewBox="0 0 200 120" fill="none" xmlns="http://www.w.w3.org/2000/svg">
<!-- Parquet Files -->
<rect x="60" y="80" width="80" height="15" rx="2" fill="#80CBC4"/>
<rect x="60" y="63" width="80" height="15" rx="2" fill="#4DB6AC"/>
<rect x="60" y="46" width="80" height="15" rx="2" fill="#26A69A"/>
<!-- Delta Lake Layer -->
<path d="M50 40 L 150 40 L 100 100 Z" fill="#B2DFDB" stroke="#004D40" stroke-width="3" stroke-linejoin="round" opacity="0.8"/>
<text x="100" y="70" font-family="Segoe UI, sans-serif" font-size="18" font-weight="bold" fill="#004D40" text-anchor="middle">Î”</text>
<text x="100" y="25" font-family="Segoe UI, sans-serif" font-size="14" font-weight="bold" text-anchor="middle">Delta Lake Layer</text>
</svg>
</div>

### What This Means for You: Database Power on the Lake

When we say Delta Lake adds "relational database semantics," it means your lakehouse tables gain critical features that were once only available in traditional data warehouses:

-   **Reliable Transactions (ACID):** Your data operations are atomic, consistent, isolated, and durable. This prevents data corruption and ensures that your data is always in a valid state, even if multiple users or jobs are writing to a table at the same time.
-   **Data Quality and Consistency:** With **schema enforcement**, Delta Lake prevents bad data from being written to your tables. With **schema evolution**, you can easily and safely add new columns to your table as your data changes over time.
-   **Performance Optimization:** Delta Lake automatically organizes and indexes your data, using techniques like compaction (combining small files into larger ones) and data skipping to make your queries run significantly faster.

### Key Delta Lake Features at Your Fingertips

The Delta Lake API unlocks a powerful set of tools for advanced data management:

| Feature | Description |
| :--- | :--- |
| **Time Travel** | Query previous versions of your data. This is invaluable for auditing, rolling back accidental changes, or reproducing experiments and reports. |
| **MERGE Statement** | Perform complex "upsert" operations (a mix of `UPDATE`, `INSERT`, and `DELETE`) in a single, atomic transaction. This is essential for Change Data Capture (CDC) and slowly changing dimension (SCD) scenarios. |
| **Streaming Ingestion & Consumption** | Use a Delta table as both a source and a sink for streaming data, creating a simple and reliable streaming architecture. |
| **DELETE & UPDATE** | Run standard SQL `DELETE` and `UPDATE` commands directly on your data lake files, something that is impossible with standard Parquet files. |

> [!NOTE]
> ### Dive Deeper into Delta Lake
> The features discussed here are just the beginning. To explore advanced topics like performance tuning, Change Data Capture (CDC), generated columns, and more, the official documentation is an invaluable resource.
>
> For more information, see the [**Delta Lake documentation**](https://docs.delta.io/latest/index.html).
