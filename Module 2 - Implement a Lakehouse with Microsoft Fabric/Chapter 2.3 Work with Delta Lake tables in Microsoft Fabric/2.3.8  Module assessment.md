## **Check Your Knowledge: Delta Lake Fundamentals**

<div align="center">
<svg width="100" height="100" viewBox="0 0 100 100" fill="none" xmlns="http://www.w3.org/2000/svg">
<rect x="20" y="20" width="60" height="60" rx="5" fill="#E3F2FD" stroke="#1E88E5" stroke-width="2"/>
<path d="M40 35 H 60 M40 45 H 60 M40 55 H 50" stroke="#1976D2" stroke-width="2.5" stroke-linecap="round"/>
<circle cx="50" cy="70" r="5" fill="#BBDEFB" stroke="#1976D2" stroke-width="1.5"/>
</svg>
</div>

Test your understanding of the core concepts of Delta Lake tables and their behavior in a Spark-based environment like Microsoft Fabric.

#### **1. Which of the following descriptions best fits Delta Lake?**

<div align="center">
<svg width="100" height="80" viewBox="0 0 100 80" fill="none" xmlns="http://www.w3.org/2000/svg">
<rect x="20" y="50" width="60" height="10" rx="2" fill="#B2DFDB"/>
<rect x="20" y="38" width="60" height="10" rx="2" fill="#80CBC4"/>
<rect x="20" y="26" width="60" height="10" rx="2" fill="#4DB6AC"/>
<path d="M15 20 L 85 20 L 85 65 L 15 65 Z" stroke="#004D40" stroke-width="2.5" fill="none"/>
</svg>
</div>

*   A. A Spark API for exporting data from a relational database into CSV files.
*   **B. A relational storage layer for Spark that supports tables based on Parquet files.**
*   C. A synchronization solution that replicates data between SQL Server and Spark.

> [!TIP]
> **Correct Answer: B**
>
> **Explanation:** Delta Lake is an open-source storage layer that brings ACID transactions and reliability to data lakes. It sits on top of existing storage (like OneLake) and uses a combination of **Parquet files** for the data and a **transaction log (`_delta_log`)** to manage versions and schema. This combination provides the reliability of a relational database table directly on your data lake files, making it a "relational storage layer" for Spark.

#### **2. You've loaded a Spark dataframe with data that you now want to use in a delta table. What format should you use to write the dataframe to storage?**

<div align="center">
<svg width="100" height="80" viewBox="0 0 100 80" fill="none" xmlns="http://www.w3.org/2000/svg">
<rect x="10" y="30" width="30" height="20" rx="3" fill="#E3F2FD" stroke="#1E88E5" stroke-width="2"/>
<path d="M45 40 L 65 40" stroke="#78909C" stroke-width="2.5"/>
<path d="M60 35 L 70 40 L 60 45" stroke="#78909C" stroke-width="2.5" fill="none"/>
<path d="M75 25 L 95 40 L 75 55 Z" fill="#4DB6AC" stroke="#004D40" stroke-width="2"/>
</svg>
</div>

*   A. CSV
*   B. PARQUET
*   **C. DELTA**

> [!TIP]
> **Correct Answer: C**
>
> **Explanation:** To create a Delta Lake table, you must explicitly write the dataframe using the `delta` format (e.g., `df.write.format("delta").save(path)`). While the underlying data files are stored in the highly efficient Parquet format, specifying `delta` is what instructs Spark to create the crucial `_delta_log` transaction log folder alongside them. This log is what enables all of Delta Lake's features, such as transactions, versioning, and time travel.

#### **3. You have a *managed* table based on a folder that contains data files in delta format. If you `DROP` the table, what happens?**

<div align="center">
<svg width="100" height="80" viewBox="0 0 100 80" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M30 20 H 70" stroke="#757575" stroke-width="3" stroke-linecap="round"/>
<path d="M35 20 V 15 H 65 V 20" stroke="#757575" stroke-width="3" stroke-linecap="round"/>
<path d="M40 20 L 42 70 H 58 L 60 20" fill="#EEEEEE" stroke="#757575" stroke-width="2"/>
<path d="M45 30 L 55 30 M 46 45 L 54 45 M 47 60 L 53 60" stroke="#BDBDBD" stroke-width="2" stroke-linecap="round"/>
</svg>
</div>

*   **A. The table metadata and data files are deleted.**
*   B. The table definition is removed from the metastore, but the data files remain intact.
*   C. The table definition remains in the metastore, but the data files are deleted.

> [!TIP]
> **Correct Answer: A**
>
> **Explanation:** This question highlights the critical difference between **managed** and **external** tables.
> *   For a **managed table**, Spark (or the Fabric metastore) controls both the table's metadata (its definition, schema, etc.) *and* the underlying data files. Therefore, when you execute `DROP TABLE`, you are telling Spark to delete everything associated with that table, including both the metadata and all the physical data files.
> *   For an **external table**, Spark only manages the metadata, while the data files reside in an external location you specify. Dropping an external table only removes the metadata pointer, leaving your data files untouched.
