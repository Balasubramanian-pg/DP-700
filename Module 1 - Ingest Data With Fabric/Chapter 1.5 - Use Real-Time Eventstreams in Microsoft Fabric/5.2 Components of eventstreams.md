Microsoft Fabric eventstreams work by creating a pipeline of events from multiple internal and external sources to different destinations. You can think of it as a conveyor belt that moves data from one place to another. You can also add some transformations to the data along the way, such as filtering, aggregating, or enriching.

![Screenshot of an eventstream.](https://learn.microsoft.com/en-us/training/wwl/explore-event-streams-microsoft-fabric/media/real-time-intelligence-eventstream-workflow.png)

You can use the eventstream visual editor to design your pipeline by dragging and dropping different nodes, such as sources, destinations, and transformations. You can also see the event data flowing through the pipeline in real-time. Microsoft Fabric eventstream handles the scaling, reliability, and security of your event stream automatically. You don’t need to write any code or manage any infrastructure to use Microsoft Fabric Eventstream.

The main components of an eventstream are:

- **Sources**: Sources are where your event data comes from. You can pick Azure Event Hubs, sample data, or custom app as your source type. You can also choose the data format and the consumer group of your source.

- **Transformations**: You can add transformations that filter or aggregate the data as is processed from the stream. Transformations include common data operations such as filtering, joining, aggregating, and grouping as well as temporal windowing functions that enable you to analyze data events within discrete time periods.

- **Destinations**: Destinations are where your transformed event data is stored. For example, you can store the results of your stream processing in a table in an eventhouse or a lakehouse, or you could redirect data to another eventstream for further processing or to an activator to trigger an action.

You can use the eventstream editing canvas to add and manage your sources and destinations. You can also see the event data, check the data insights, and view logs for each source or destination.

---

Think of an Eventstream as a **digital post office and sorting facility for real-time data**. It receives massive volumes of small data packets (events), allows you to inspect and sort them on the fly, and then routes them to their final destinations.

Here are the core components, broken down into a logical flow from input to output.

---

### **The Overall Workflow**

The fundamental flow of any Eventstream is:

`[Source]  ->  [Eventstream (Ingestion & Processing)]  ->  [Destination]`

Let's look at the components within each stage.

### **Component 1: Sources (The Inputs)**

Sources are the services and applications that generate and send real-time data *into* your Eventstream. Fabric provides several built-in source types.

*   **Azure Event Hubs:**
    *   **What it is:** The primary, high-throughput data streaming service in Azure. It's designed to ingest millions of events per second.
    *   **Use Case:** Connecting to existing data pipelines, receiving logs from other Azure services, or getting data from large-scale applications. This is your go-to for serious, production-grade data streams.

*   **Azure IoT Hub:**
    *   **What it is:** A specialized Azure service for managing and communicating with millions of Internet of Things (IoT) devices.
    *   **Use Case:** Directly ingesting telemetry from sensors, smart devices, factory machinery, vehicles, etc. It handles device management, security, and two-way communication, while forwarding the telemetry data to your Eventstream.

*   **Custom App:**
    *   **What it is:** This provides you with a connection string (compatible with Azure Event Hubs SDKs). Your own application code can use this string to send data directly to the Eventstream.
    *   **Use Case:** The most flexible option. You can instrument your own web applications, backend services, games, or any custom code (in Python, C#, Java, etc.) to send real-time events to Fabric. For example, sending a "user-clicked-button" event from your website.

*   **Sample Data:**
    *   **What it is:** A built-in data generator that produces a continuous stream of sample events (e.g., taxi data, stock data).
    *   **Use Case:** Crucial for learning, development, and testing. It allows you to build and test your entire streaming pipeline (processing, destinations, dashboards) without needing to set up a real data source first.

### **Component 2: The Eventstream Itself (The Central Hub)**

This is the core of the feature, the central pipe where all the data flows.

*   **Main Stream:**
    *   This is the primary, unfiltered flow of data coming from your source. When you add a source, its data immediately starts populating the main stream.
    *   **Data Preview:** The Eventstream UI provides a live, continuously updating view of the raw data flowing through the stream. This is invaluable for inspecting the schema (field names, data types) and content of your events in real-time, helping you debug and build your processing logic.

*   **Derived Stream:**
    *   A derived stream is a **new, secondary stream that you create by applying processing logic to the main stream**. You don't create it manually; it is the *output* of an Event Processor.
    *   **Use Case:** This is how you implement routing. For example, your main stream has all IoT data. You can create a "High Temperature Alerts" derived stream that contains *only* the events where temperature > 90°F. This derived stream can then be sent to a specific destination.

### **Component 3: In-Stream Processing (The Sorting Facility)**

This is where Eventstreams becomes truly powerful. Instead of just passing data through, you can process it *in-flight* using the **Event Processor**.

*   **Event Processor Editor:**
    *   This is a no-code/low-code graphical editor where you define your real-time data transformations. When you create a processing rule, its output becomes a new **Derived Stream**.
    *   The operations you can perform include:
        *   **Filter:** The most common operation. You can specify conditions to include only certain events. (e.g., `status = 'error'` or `value > 100`).
        *   **Manage fields (Projection):**
            *   Select only the columns you need.
            *   Rename columns.
            *   Change data types.
            *   Add new columns with static values or simple expressions.
        *   **Aggregate (Group by):** This is a key stream-processing function. You can perform calculations over **time windows**.
            *   **Tumbling Window:** A series of fixed-size, non-overlapping time intervals. (e.g., "Calculate the `COUNT` of events every 5 minutes").
            *   **Hopping Window:** A series of fixed-size, overlapping time intervals. (e.g., "Calculate the `AVERAGE` value over a 10-minute window, updated every 1 minute").
            *   **Sliding Window:** Computes an aggregate whenever a new event arrives, over the preceding time interval. (e.g., "Calculate the `MAX` value over the last 1 minute").

### **Component 4: Destinations (The Outputs)**

Destinations are the Fabric items or external applications where you send your processed (or raw) data for storage, analysis, or action.

*   **Lakehouse:**
    *   **What it is:** The primary data lake storage in Fabric.
    *   **Use Case:** Storing the raw or transformed event data for long-term historical analysis, ad-hoc exploration with Spark/SQL, and as a source for machine learning model training. Data is stored in the efficient Delta Lake format. This is your destination for "big data" analytics.

*   **KQL Database:**
    *   **What it is:** A high-performance, read-optimized database designed for querying massive volumes of structured and semi-structured data, especially time-series data.
    *   **Use Case:** The perfect destination for building real-time operational dashboards and performing lightning-fast interactive analytics. When you send data from an Eventstream to a KQL DB, you can query it with sub-second latency. This is the best choice for live dashboards.

*   **Power BI:**
    *   **What it is:** The direct connection to Power BI for visualization.
    *   **Use Case:** While you can build a Power BI report on top of a Lakehouse or KQL DB, this destination allows you to create a *streaming dataset* directly in Power BI for dashboards that update in real-time without needing to hit a database first. This is for the most immediate, low-latency visuals.

*   **Custom App:**
    *   **What it is:** Just like the source, this allows another application to *read* data from a stream.
    *   **Use Case:** Triggering downstream workflows. You could have a custom application listening to an "alerts" stream, which then sends an email, calls a webhook, or starts a process in another system. This is how you integrate Eventstreams with external action-oriented systems.

---

### **Summary Table**

| Component Group | Component Name | Role | Example |
| :--- | :--- | :--- | :--- |
| **Input** | **Source** (Event Hub, IoT Hub, Custom App) | To get data *into* Fabric. | An IoT sensor sends temperature readings. |
| **Core** | **Main Stream** | The central, unfiltered pipe for all incoming data. | All temperature and pressure readings flow here. |
| **Processing** | **Event Processor** | A no-code editor to Filter, Transform, or Aggregate data. | A rule is set to `FILTER` for `temperature > 90`. |
| **Processing** | **Derived Stream** | The output of the Event Processor; a new, refined stream. | A new stream called "HighTempAlerts" is created. |
| **Output** | **Destination** (Lakehouse, KQL DB, Power BI) | To store, analyze, or visualize the data from a stream. | The "HighTempAlerts" stream is sent to a KQL Database for a live alert dashboard. |