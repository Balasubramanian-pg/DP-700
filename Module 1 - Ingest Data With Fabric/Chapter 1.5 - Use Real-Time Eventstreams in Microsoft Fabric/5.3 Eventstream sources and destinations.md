At their simplest, eventstreams capture real-time data from a source and load the captured data into a destination.

## Eventstream sources

Event sources in Fabric Real-Time Hub can include (among others):

- **Azure Event Hub**: Azure Event Hubs is a source that allows you to get event data from an Azure event hub. You can specify the data format and the consumer group of your Azure event hub. You can also create a cloud connection to your Azure event hub with the appropriate authentication and privacy level.
- **Azure IoT Hub**: An SaaS service used to connect, monitor, and manage millions of IoT assets with a no-code experience.
- **Azure SQL Database Change Data Capture (CDC)**: Software process that identifies and tracks changes to data in a database, enabling real-time or near-real-time data movement.
- **PostgreSQL Database CDC**: The PostgreSQL Database CDC (change data capture) connector for Microsoft Fabric Eventstreams captures a current snapshot and tracks future changes in a PostgreSQL database, allowing real-time processing and analysis of this data within Fabric.
- **MySQL Database CDC**: The Azure MySQL Database CDC Source connector for Microsoft Fabric Eventstreams captures a snapshot of your MySQL database and tracks table changes, enabling real-time processing and analysis of this data in Fabric.
- **Azure Cosmos DB CDC**: The Azure Cosmos DB CDC connector for Microsoft Fabric Eventstreams captures and tracks real-time data changes in Cosmos DB for analysis and processing within Fabric.
- **Google Cloud Pub/Sub**: A messaging service for exchanging event data among applications and services.
- **Amazon Kinesis Data Streams**: Collect, process, and analyze real-time, streaming data.
- **Confluent Cloud Kafka**: A fully managed service based on Apache Kafka for stream processing.
- **Fabric workspace events**: Fabric workspace events are events triggered by changes in your Fabric Workspace, such as creating, updating, or deleting items. With Fabric event streams, you can capture, transform, and route these events for in-depth analysis and monitoring within Fabric. This integration offers enhanced flexibility in tracking and understanding workspace activities.
- **Azure blob storage events**: Azure Blob Storage events are system triggers for actions like creating, replacing, or deleting a blob. Microsoft Fabric eventstreams link these actions to Fabric events, allowing you to process Blob Storage events as continuous data streams for routing and analysis within Fabric. One unique trait of the Azure Blob Storage Events is their support for _**streamed**_ or _**unstreamed**_ events.
- **Custom endpoint**: You can use the REST API or software development kits (SDKs) provided by Microsoft Fabric to send event data from your custom app to your eventstream. You can also specify the data format and the consumer group of your custom app.
- **Sample data**: Sample data is a source that allows you to use the out-of-box sample data provided by Microsoft Fabric. You can choose from different sample data sets, such as IoT, Retail, or Finance. You can also adjust the frequency and duration of the sample data generation.

> [!NOTE]
> For more information about supported sources, see **[Supported sources for Fabric Real-Time hub](https://learn.microsoft.com/en-us/fabric/real-time-hub/supported-sources)**.

## Eventstream destinations

Microsoft Fabric event streams also supports the ability to send data to the following destinations in the enhanced mode.

- **Eventhouse**: This destination offers the capability to funnel your real-time event data into a KQL database, enabling you to use Kusto Query Language (KQL) for data interrogation and analysis. Storing your data in a KQL database unlocks the potential for enhanced comprehension of your event data and the creation of comprehensive reports and dashboards.
- **Lakehouse**: This destination empowers you to preprocess your real-time events before their ingestion into your lakehouse. The events are transformed into Delta Lake format and later stored in specific lakehouse tables, facilitating your data warehousing needs.
- **Custom endpoint**: With this feature, you can seamlessly direct your real-time event traffic to a bespoke application. It enables the integration of your proprietary applications with the event stream, allowing for the immediate consumption of event data. This feature is advantageous when you aim to transfer real-time data to an independent system not hosted on the Microsoft Fabric.
- **Derived Stream**: The derived stream is a specialized destination created post-application of stream operations like Filter or Manage Fields to an eventstream. It represents the altered default stream after processing, which can be routed to various destinations within Fabric and monitored in the Real-Time hub.
- **Fabric Activator**: This destination enables you to use Fabric Activator to trigger automated actions based on values in your streaming data.

> [!Abstract]
> For more information about supported sources, see **[Add and manage a destination in an eventstream](https://learn.microsoft.com/en-us/fabric/real-time-intelligence/event-streams/add-manage-eventstream-destinations)**.

You can attach to multiple destinations within an event stream at the same time without impacting or colliding with each other.

---
### **The Big Picture: On-Ramps and Off-Ramps**

Think of an Eventstream as a high-speed data highway.
*   **Sources** are the **on-ramps**. They are the methods you use to get different types of data traffic *onto* the highway.
*   **Destinations** are the **off-ramps**. They are the specific places where you direct the data traffic *off* the highway for storage, analysis, or action.

You can have one source feeding multiple destinations, and you can process the data on the highway before it reaches its destination.

---

## **Eventstream Sources (The On-Ramps)**

These are the services that send data *into* your Eventstream.

### **1. Azure Event Hubs**

*   **What It Is:** Microsoft's flagship, planet-scale data streaming service. It's a highly reliable and scalable platform designed to ingest millions of events per second from any source.
*   **Best For / Use Case:**
    *   **Integrating with existing Azure infrastructure:** If you already have applications, services (like Azure Functions), or systems that are sending data to an Azure Event Hub, you can connect it directly to Fabric without changing your existing code.
    *   **High-volume, high-throughput scenarios:** When you need a robust, battle-tested ingestion point for massive streams of data, like server logs, application telemetry, or financial transactions.
    *   **Consolidating multiple streams:** You can use an Event Hub as a central point in Azure to gather data from various sources before forwarding it all into a single Fabric Eventstream.
*   **Key Characteristic:** Enterprise-grade reliability and scalability.

### **2. Azure IoT Hub**

*   **What It Is:** A specialized Azure service built specifically for managing and communicating with Internet of Things (IoT) devices. It provides much more than just data ingestion; it handles device identity, security (per-device credentials), and two-way communication (sending commands back to devices).
*   **Best For / Use Case:**
    *   **Connecting fleets of IoT devices:** The only choice when your data is coming from sensors, factory machinery, smart meters, connected vehicles, or any physical device.
    *   **When you need device management:** If you need to know which devices are online, provision new ones securely, or send firmware updates, IoT Hub is the right tool. The telemetry from these devices is then forwarded to your Eventstream.
*   **Key Characteristic:** Secure, bi-directional communication and management for IoT devices.

### **3. Custom App**

*   **What It Is:** This isn't a pre-built service but rather an endpoint and a connection string that Fabric provides for you. You use this connection string in your own application code (e.g., Python, C#, Java, Node.js) using standard Azure Event Hubs SDKs.
*   **Best For / Use Case:**
    *   **Instrumenting your own software:** Sending real-time events directly from your website backend, mobile app, desktop software, or microservices.
    *   **Maximum flexibility:** When your data source is a proprietary system or an application where you have full control over the code.
    *   **Examples:** Sending a `productAddedToCart` event from an e-commerce site, a `playerAction` event from a game server, or a `loginAttempt` event from a security service.
*   **Key Characteristic:** Ultimate flexibility for any application that can run code.

### **4. Sample Data**

*   **What It Is:** A built-in data generator provided by Fabric. You can choose from a few sample datasets (like NYC Taxi data or Stock data), and it will start generating a continuous stream of events for you instantly.
*   **Best For / Use Case:**
    *   **Learning and Development:** The perfect way to start using Eventstreams without needing to set up a real data source.
    *   **Testing and Prototyping:** You can build and test your entire downstream pipeline (processing logic, KQL databases, Power BI dashboards) using realistic sample data before you connect your real, live source.
*   **Key Characteristic:** Instant, zero-setup data stream for development.

---

## **Eventstream Destinations (The Off-Ramps)**

These are the Fabric items where you send the data from your Eventstream for further use. You can send data from the main, raw stream or from a processed, derived stream.

### **1. Lakehouse**

*   **What It Is:** The primary data lake storage architecture in Fabric. Data is stored in the open-source Delta Lake format, which adds reliability, performance, and ACID transactions to a standard data lake.
*   **Best For / Use Case:**
    *   **Creating a "Single Source of Truth":** Landing all your raw, unfiltered event data for long-term, affordable storage. This becomes your historical archive.
    *   **Big Data Analytics & Data Science:** The Lakehouse is the foundation for running large-scale data engineering jobs with Spark (Notebooks) and training machine learning models.
    *   **Ad-hoc Data Exploration:** Analysts can query the data in the Lakehouse tables using standard SQL.
*   **Key Characteristic:** Built for massive scale, long-term storage, and advanced analytics/ML. **Think of this as the warehouse.**

### **2. KQL Database**

*   **What It Is:** A high-performance, read-optimized analytical database. It uses the Kusto Query Language (KQL) and is engineered for near-instantaneous querying over massive volumes of time-series data.
*   **Best For / Use Case:**
    *   **Real-time Operational Dashboards:** This is the premier destination for building live, interactive dashboards that need sub-second query performance.
    *   **Time-Series Analysis & Anomaly Detection:** KQL has powerful built-in functions specifically for analyzing time-stamped data, like logs, IoT telemetry, and application events.
    *   **Interactive Exploration of recent data:** When an analyst needs to quickly slice, dice, and investigate real-time data to troubleshoot an issue.
*   **Key Characteristic:** Extreme query speed for real-time analytics and visualization. **Think of this as the high-tech command center.**

### **3. Power BI**

*   **What It Is:** This destination creates a special type of Power BI dataset called a "streaming dataset."
*   **Best For / Use Case:**
    *   **The lowest-latency visuals:** When you need a dashboard tile (e.g., a card or a line chart) to update in *seconds* or less after an event occurs. The data is pushed directly to the visual without being stored in a database first.
    *   **Simple, real-time monitoring:** Good for very simple, high-level KPIs.
*   **Key Limitation:** Streaming datasets are very limited. They have a short retention period (data is not stored long-term) and have limited analytical capabilities compared to a full model built on a KQL DB or Lakehouse. For most rich, interactive reports, you will send data to a KQL DB first and then build your Power BI report on top of that.

### **4. Custom App**

*   **What It Is:** Similar to the source, this allows another application to connect to and *read* from your Eventstream.
*   **Best For / Use Case:**
    *   **Triggering downstream business processes:** A custom application can listen to a specific derived stream (e.g., an "Alerts" stream). When it receives an event, it can trigger an action in another system—like calling an API, sending a push notification, or creating a ticket in a support system.
    *   **Integration with non-Fabric systems:** If you need to forward the real-time data to an external application or a different cloud service for processing.
*   **Key Characteristic:** Enables event-driven architecture and integration with any external system.