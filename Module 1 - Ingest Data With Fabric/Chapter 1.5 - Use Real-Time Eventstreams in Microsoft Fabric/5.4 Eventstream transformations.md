The drag and drop eventstream editing interface offers a straightforward and user-friendly method for constructing complex event data processing workflows.

## Eventstream transformations

You can transform data in an eventstream by applying the following transformations:

### 1. **Filter**

Used to **keep or discard events** based on a field’s value.  
Examples:

- `Status is not null` → only keeps rows where `Status` exists
    
- `Amount > 1000` → only keeps high-value transactions

Think of it like a `WHERE` clause in SQL.

---

### 2. **Manage Fields**

Lets you **modify the schema**:

- Add new fields (e.g., calculated columns)
    
- Remove unnecessary ones
    
- Rename for clarity
    
- Change data type (e.g., string to int)
    

Like cleaning up a raw dataset.

---

### 3. **Aggregate**

Used for **running totals or metrics** over time.  
Example:

- `SUM(Sales)` every 5 seconds
    
- `AVG(Speed)` per device  
    It's triggered **every time a new event hits**—this is real-time aggregation.
    

---

### 4. **Group By**

Like `Aggregate` but better for **time windowed aggregations**.  
Example:

- `MAX(Temp)` every 10 minutes
    
- Group by `DeviceID`
    

Supports tumbling, hopping, or sliding windows—use when you care about **batch-style time windows**.

---

### 5. **Union**

Combines **multiple event sources** into one, but only **keeps matching columns**.  
Think of it like `UNION ALL` in SQL where schema mismatches are dropped silently.

---

### 6. **Expand**

Used when a field contains an **array**, and you want **one row per value**.  
Example:

- Event: `{User: A, Items: [X, Y, Z]}`
    
- Output:
    
    - `{User: A, Item: X}`
        
    - `{User: A, Item: Y}`
        
    - `{User: A, Item: Z}`
        

---

### 7. **Join**

Used to **merge two streams** on matching fields.  
Example:

- Stream A: Orders
    
- Stream B: Customer Info  
    Join on `CustomerID` to enrich events.
    

You can use **inner**, **left**, or **outer joins** depending on what you want to retain.

---

## Windowing functions in Eventstream

Windowing functions are a way to perform operations on the data contained in temporal windows, such as aggregating, filtering, or transforming streaming events that occur within a specified time period. Windowing functions are useful for analyzing streaming data that changes over time, such as sensor readings, web-clicks, on-line transactions, and more and provide great flexibility to keep an accurate record of events as they occur.

Earlier, we mentioned the **Group by** operator in the event processor editor. This graphical interface allows us to define the logic we need for processing, transforming, and routing event data. There are four parameters that need specified in the **Group by** operator settings to use these windowing functions that include:

- The **window type**, which can be **tumbling, sliding, snapshot, hopping**, or **session**.
    
    - **tumbling windows** divides incoming events into fixed and nonoverlapping intervals based on arrival time.
        
        This window shows a count of tweets per time zone every 10 seconds apart.
        
        ![Screenshot of the tumbling window concept.](https://learn.microsoft.com/en-us/training/wwl/explore-event-streams-microsoft-fabric/media/tumbling-window.png)
        
    - **sliding windows** takes the events into fixed and overlapping intervals based on time and divides them.
        
        A window is a 10-second sliding window that alerts the user whenever an article is mentioned more than three times in under 10 seconds
        
        ![Screenshot of the sliding window concept.](https://learn.microsoft.com/en-us/training/wwl/explore-event-streams-microsoft-fabric/media/sliding-window.png)
        
        All of the tweets belong to the same topic in this scenario
        
    - **Session windows** simply divides events into variable and nonoverlapping intervals that are based on a gap of lack of activity.
        
        A window, which shows the total count of tweets that occur within five minutes of one another
        
        ![Screenshot of the session window concept.](https://learn.microsoft.com/en-us/training/wwl/explore-event-streams-microsoft-fabric/media/session-window.png)
        
    - **Hopping windows** hopping windows are different from **tumbling** windows as they model scheduled overlapping window.
        
        A 10 second hopping window, which refreshes every 5 seconds and provides the total count of tweets over the past 10 seconds.
        
        ![Screenshot of the hopping window concept.](https://learn.microsoft.com/en-us/training/wwl/explore-event-streams-microsoft-fabric/media/hopping-window.png)
        
    - **Snapshot windows** snapshot windows group event stream events that have the same timestamp and are unlike the other windowing functions, which require the function to be named. In snapshot windows you add the `System.Timestamp()` to the `GROUP BY` clause.
        
        This window provides a count of tweets with the same article type that occur at exactly the same time.
        
        ![Screenshot of the snapshot window concept.](https://learn.microsoft.com/en-us/training/wwl/explore-event-streams-microsoft-fabric/media/snapshot-window.png)
        
- The **window duration** is the length of each window interval, which can be in seconds, minutes, hours, and even days. An example duration, such as 10 minutes, means simply that each window covers 10 minutes of event data.
    
- The **window offset** is an optional parameter that shifts the start and end of each window interval by a specified amount of time. An example of when this optional parameter is set might be a window offset of 2 minutes, which means that each window starts and ends 2 minutes later than usual.
    
- The **grouping key** is one or more columns in your event data that you wish to group by. For example, by sensor ID, or item category.
    
- The **Aggregation function** is one or more of the functions that you want to apply to each group of events in each window. Where the counts, sums, averages, min/max, and even custom functions become useful.

### **The Core Idea: Processing Data "In-Flight"**

Before listing the transformations, it's crucial to understand the concept.

Without transformations, you would stream raw data from a source (like an IoT device) directly into a destination (like a Lakehouse). You would then have to run a separate job (e.g., a Spark Notebook) to clean, filter, and aggregate that data *after* it has landed. This adds latency.

**Eventstream transformations allow you to perform these operations *as the data is flowing through the stream***. This is called "in-flight" or "real-time" processing.

The benefits are significant:
*   **Reduced Latency:** Decisions and alerts can be triggered in near real-time.
*   **Efficiency:** You filter out "noise" or unnecessary data early, so you only store what you need, saving storage costs and improving query performance at the destination.
*   **Data Routing:** You can create different transformed "views" of your stream and send them to different destinations.

### **The Tool: The Event Processor**

All transformations are configured within the **Event Processor**. This is a no-code/low-code graphical editor where you chain together operations.

A key principle is that **transformations are non-destructive**. You don't modify the original, raw stream. Instead, each set of transformations you define creates a **new Derived Stream**. This new stream contains the output of your processing logic.

`[Main Stream] -> [Event Processor Operations] -> [New Derived Stream]`

You then connect your destinations to these new, refined derived streams.

---

### **The Specific Transformation Operations**

Here are the key operations available in the Event Processor, from simplest to most complex.

#### **1. Filter**

*   **What it does:** Selects or discards events based on a condition applied to their content. This is the most common first step in any stream processing pipeline.
*   **Why you use it:**
    *   **To reduce noise:** Remove irrelevant events (e.g., keep only sensor readings, discard heartbeat/status messages).
    *   **To route data:** Create separate streams for different types of events (e.g., one stream for "errors," one for "warnings," one for "info").
*   **Example:**
    *   **Input:** A stream of application logs with a `level` field (`'INFO'`, `'ERROR'`, `'DEBUG'`).
    *   **Operation:** `FILTER level = 'ERROR'`
    *   **Output (Derived Stream):** A new stream containing only the error events.

#### **2. Manage fields**

*   **What it does:** This single operation lets you modify the schema (the structure) of your events. It's like a multi-tool for shaping your data.
*   **Sub-Operations:**
    *   **Select Columns (Projection):** Choose which fields to keep and discard the rest. This is great for reducing the size of your events.
    *   **Rename Columns:** Change field names to be more descriptive or to match a target schema (e.g., change `dev_id` to `DeviceID`).
    *   **Change Data Type:** Convert a field from one type to another (e.g., `string` to `integer`, `unix_timestamp` to `datetime`). This is critical for ensuring data quality before loading into a database.
    *   **Add New Column:** Add a new field with a static value or a value derived from an expression.
*   **Example:**
    *   **Input:** Event with fields `ts` (string), `id` (integer), `val` (string).
    *   **Operation:**
        1.  **Rename** `ts` to `EventTime` and `val` to `Value`.
        2.  **Change Type** of `EventTime` to `datetime` and `Value` to `float`.
        3.  **Discard** the `id` field.
    *   **Output (Derived Stream):** A clean stream with just `EventTime` (datetime) and `Value` (float).

#### **3. Aggregate (Group by over a Time Window)**

*   **What it does:** This is the most powerful stream processing operation. It performs calculations (like `SUM`, `COUNT`, `AVG`) on groups of events over a specific **window of time**.
*   **Why you use it:** To summarize high-frequency data into lower-frequency, meaningful insights. Instead of a thousand individual events, you get one summary event.
*   **Key Concept: Windows:**
    *   **Tumbling Window:** A series of fixed-size, non-overlapping time intervals.
        *   **Analogy:** A series of separate buckets. When one 5-minute bucket is full, you calculate its total and start a new one.
        *   **Example:** "Calculate the `COUNT` of website clicks every 1 minute." You'll get one result at 12:01, another at 12:02, etc.
    *   **Hopping Window:** A series of fixed-size, overlapping time intervals. It has a *window size* (how much data to look at) and a *hop size* (how often to calculate).
        *   **Analogy:** A sliding window that "hops" forward at a set interval.
        *   **Example:** "Calculate the `AVERAGE` transaction value over a 10-minute window, updating the result every 1 minute." At 12:10, you calculate the average from 12:00-12:10. At 12:11, you calculate the average from 12:01-12:11.
    *   **Sliding Window:** Computes an aggregate whenever a new event arrives, looking back over a fixed duration.
        *   **Analogy:** The most "continuous" window.
        *   **Example:** "What is the `MAX` temperature recorded from any sensor over the last 60 seconds?" This value is recalculated every time a new temperature event arrives.
*   **Example:**
    *   **Input:** A stream of sales events, each with a `SaleAmount`.
    *   **Operation:** `AGGREGATE SUM(SaleAmount) AS TotalSales` using a `Tumbling Window` of `5 minutes`.
    *   **Output (Derived Stream):** A single event every 5 minutes containing the total sales for that period.

#### **4. Expand**

*   **What it does:** Unnests an array within an event into multiple, separate events.
*   **Why you use it:** Some sources send complex, nested JSON where one event might contain a list of items. To load this into a flat table structure (like in a KQL DB), you need to "explode" or "unwind" the array.
*   **Example:**
    *   **Input:** A single event for an order: `{ "OrderID": 123, "Items": [ { "SKU": "A", "Qty": 2 }, { "SKU": "B", "Qty": 5 } ] }`
    *   **Operation:** `EXPAND` on the `Items` array.
    *   **Output (Derived Stream):** Two separate events:
        1.  `{ "OrderID": 123, "SKU": "A", "Qty": 2 }`
        2.  `{ "OrderID": 123, "SKU": "B", "Qty": 5 }`

#### **5. Join**

*   **What it does:** Combines events from two different streams based on a common key and a time condition.
*   **Why you use it:** To enrich one stream with contextual data from another. For example, joining a stream of anonymous sensor readings with a stream of sensor metadata.
*   **Key Concept:** Because events may not arrive at the exact same time, you must specify a time window for the join (e.g., "join an order event with a shipment event if they have the same ID and arrive within 30 minutes of each other").
*   **Example:**
    *   **Input Stream 1 (Orders):** `{ "OrderID": 101, "ProductID": "P50" }`
    *   **Input Stream 2 (ProductInfo):** `{ "ProductID": "P50", "ProductName": "Wireless Mouse" }`
    *   **Operation:** `JOIN` Stream 1 and Stream 2 on `ProductID` within a 10-minute window.
    *   **Output (Derived Stream):** A new, enriched event: `{ "OrderID": 101, "ProductID": "P50", "ProductName": "Wireless Mouse" }`