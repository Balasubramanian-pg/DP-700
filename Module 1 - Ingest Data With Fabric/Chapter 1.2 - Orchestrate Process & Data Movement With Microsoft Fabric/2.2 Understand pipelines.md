Pipelines in Microsoft Fabric encapsulate a sequence of _activities_ that perform data movement and processing tasks. You can use a pipeline to define data transfer and transformation activities, and orchestrate these activities through control flow activities that manage branching, looping, and other typical processing logic. The graphical pipeline _canvas_ in the Fabric user interface enables you to build complex pipelines with minimal or no coding required.

![Screenshot of a pipeline in Microsoft Fabric.](https://learn.microsoft.com/en-us/training/wwl/use-data-factory-pipelines-fabric/media/pipeline.png)

## Core Pipeline Concepts in Microsoft Fabric

In Microsoft Fabric, a **Pipeline** is a logical grouping of activities that together perform a task. Think of it as a workflow or a recipe. The pipeline itself doesn't perform the data processing; instead, it acts as an **orchestrator**. It defines *what* to do (the activities), *in what order*, and *under what conditions*. This orchestration is fundamental to automating complex data integration and transformation processes.

Let's dive deeper into the core building blocks: Activities.

---

### Activities: The Workhorses of the Pipeline

Activities are the individual processing steps within a pipeline. They are the "verbs" of your data workflow—copy, transform, run, wait, decide. Each activity is a self-contained unit of work. The real power comes from how you chain them together, using their outcomes (Success, Failure, Completion, or Skipped) to create dynamic and robust data flows.

A key concept is the **dependency chain**. You connect activities with arrows that define the execution path. An activity will only run after the preceding activity it depends on has finished with a specific outcome.

As the original text states, activities fall into two main categories.

---

### 1. Data Transformation Activities

These activities are focused on moving and manipulating your data. They are the core of any ETL (Extract, Transform, Load) or ELT (Extract, Load, Transform) process.

#### **Copy Data**
*   **What it is:** The most fundamental data movement activity. It is highly optimized for copying large volumes of data from a source data store to a destination (or "sink").
*   **When to use it:**
    *   Loading raw data from external sources (like an on-premises SQL Server, an S3 bucket, or a Salesforce API) into your OneLake landing zone (e.g., a Lakehouse Files area).
    *   Moving data between different layers within Fabric, such as from a "bronze" raw data table to a "silver" staging table in a Lakehouse.
    *   Archiving data from a hot-path Warehouse to a cold-storage location.
*   **Key Features:** It's schema-aware and can perform implicit data type mapping. You can configure performance settings like parallel copy to speed up transfers.

#### **Dataflow (Gen2)**
*   **What it is:** This is where the "T" in ETL happens in a low-code/no-code environment. A Dataflow Gen2 activity executes a dataflow you've designed using the Power Query Online experience.
*   **When to use it:**
    *   When you need to clean, reshape, merge, or enrich data *in-flight* as it moves from source to destination.
    *   For tasks like filtering rows, splitting columns, pivoting/unpivoting data, joining datasets, and applying business logic.
*   **Key Features:**
    *   **Visual Interface:** Uses the familiar Power Query editor, making it accessible to a wide range of users, from data analysts to engineers.
    *   **Scalable Execution:** Behind the scenes, Fabric translates your Power Query steps into scalable Spark code, running it on the Fabric compute engine.
    *   **Data Destination:** You configure the output destination (e.g., a Lakehouse table, a Warehouse table) directly within the dataflow itself. The pipeline activity simply triggers its execution.

#### **Notebook**
*   **What it is:** This activity executes a Fabric Notebook, allowing you to run custom code written in languages like PySpark, Spark SQL, Scala, or R.
*   **When to use it:**
    *   For complex transformations that are difficult or impossible to express in Power Query.
    *   To perform advanced data science tasks, such as training a machine learning model, running predictions, or performing statistical analysis.
    *   When you need to use specific Python/Spark libraries for data cleansing or enrichment.
    *   To implement custom logging or complex data validation logic.
*   **Key Features:** You can pass parameters from the pipeline into the notebook, making it a reusable and dynamic component of your workflow.

#### **Stored Procedure**
*   **What it is:** Executes a stored procedure in a supported SQL data store, most notably a Fabric Warehouse or a SQL Database.
*   **When to use it:**
    *   To leverage existing, pre-optimized SQL logic that already resides in your data warehouse. This is a core principle of the ELT pattern (Extract, Load, *then* Transform).
    *   For performing complex set-based operations, merges (UPSERTs), or aggregations directly within the warehouse engine, which is often the most performant way to do so.
    *   To encapsulate business logic and maintain a clean separation of concerns, keeping the SQL transformations within the database layer.

---

### 2. Control Flow Activities

These activities don't manipulate the data itself. Instead, they manage and control the execution flow of the pipeline, enabling you to build sophisticated, dynamic, and resilient workflows.

#### **For Each (Looping)**
*   **What it is:** Iterates over a collection of items (like a list of files, tables, or API endpoints) and executes a set of activities for each item in the collection.
*   **When to use it:**
    *   **Example:** You have 100 new CSV files dropped into a folder each day. Instead of creating 100 copy activities, you use a `For Each` loop. The loop gets the list of filenames and then executes a single `Copy Data` activity inside it, once for each file.
*   **How it works:** You provide it with an array of items (which can be hardcoded or, more commonly, the dynamic output of another activity like `Lookup`).

#### **If Condition**
*   **What it is:** A classic `if-then-else` block. It evaluates a boolean expression and, based on the result (true or false), executes one of two different sets of activities.
*   **When to use it:**
    *   **Example:** Before running a `Copy Data` activity, you can check if the source file actually exists. *If true*, run the copy. *If false*, send an email notification and skip the rest of the flow.

#### **Set Variable / Append Variable**
*   **What it is:** Allows you to create and manipulate variables within the scope of a pipeline run. Variables are used to store state. `Set Variable` assigns or overwrites a value, while `Append Variable` adds a value to an existing array variable.
*   **When to use it:**
    *   To store a high-watermark value (e.g., the latest `LastModifiedDate` from a table) to ensure you only process new or updated records in the next run.
    *   To build a dynamic string, like an email body that lists all the files that were processed successfully.
    *   To count iterations in a loop.

#### **Lookup**
*   **What it is:** A powerful activity that retrieves a small dataset from any data source. Its output can then be used by subsequent activities in the pipeline.
*   **When to use it:**
    *   **Example:** You have a "control table" in your warehouse that lists all the tables that need to be copied daily. The `Lookup` activity reads this control table. Its output (an array of table names) is then fed directly into a `For Each` loop to process each table.
    *   To retrieve a single configuration value, like an API key or a threshold, from a configuration file or table.

#### **Web / Webhook**
*   **What it is:** `Web` allows your pipeline to call a custom REST API endpoint. `Webhook` is a special variant that pauses the pipeline and waits for an external service to call it back before proceeding.
*   **When to use it:**
    *   **Web:** Trigger a Power Automate or Logic Apps workflow, post a status message to Microsoft Teams, or retrieve data from an external web service.
    *   **Webhook:** Integrate with an external system that requires human approval. The pipeline can pause, wait for a user to click "Approve" in another application, which then calls the webhook URL to resume the pipeline.

### Putting It All Together: A Sample Scenario

Imagine a daily sales data pipeline:

1.  **Trigger:** The pipeline runs automatically every morning.
2.  **Lookup Activity:** It queries a control table in a Warehouse to get the list of regional sales databases that need to be processed.
3.  **For Each Activity:** It loops through the list of databases provided by the `Lookup` activity.
4.  **Inside the For Each Loop:**
    *   A **Copy Data** activity connects to the current regional database in the loop and copies the `dbo.DailySales` table into a raw table in the Fabric Lakehouse.
5.  **After the Loop Completes (On Success):**
    *   A **Dataflow Gen2** activity runs. It sources all the raw daily sales tables from the Lakehouse, standardizes the column names, joins them with a `Dim_Products` table, and loads the final, clean, and enriched dataset into a `Fact_Sales` table in the Warehouse.
    *   A **Stored Procedure** activity then runs against the Warehouse to calculate and update an aggregate summary table.
6.  **Failure Path:** If any activity in the main flow fails, a dependency arrow for `On Failure` points to a **Web Activity** that sends an error notification to a Teams channel with the specific error message.
    

> [!NOTE]
> For details about the complete set of pipeline activities available in Microsoft Fabric, see [Activity overview](https://learn.microsoft.com/en-us/fabric/data-factory/activity-overview) in the Microsoft Fabric documentation.

### Parameters

Think of a pipeline as a reusable recipe. A parameter is like an ingredient in that recipe that you can change each time you cook. Instead of writing a separate recipe for "Chicken Noodle Soup" and "Beef Noodle Soup," you write one "Noodle Soup" recipe with a parameter called `ProteinType`. When you decide to cook, you specify whether `ProteinType` is "Chicken" or "Beef."

In Microsoft Fabric, **parameters are variables defined at the pipeline level that allow you to pass in external values at runtime.** They are the primary mechanism for making pipelines dynamic, flexible, and, most importantly, reusable.

#### Why Parameters are Essential

*   **Reusability:** This is the most significant benefit. Instead of creating nearly identical pipelines for different environments or data sources, you create a single, parameterized pipeline.
    *   **Example:** You need to ingest data from 15 different sales regions, each with its own source table (`Sales_US`, `Sales_UK`, `Sales_DE`, etc.). Instead of building 15 separate pipelines, you build **one** pipeline with a parameter called `RegionCode`. You can then run this same pipeline 15 times, passing in "US", "UK", "DE", etc., as the parameter value for each run.

*   **Environment Management (Dev/Test/Prod):** Parameters are crucial for promoting code through different stages of a development lifecycle.
    *   **Example:** You can have parameters for your Lakehouse name (`Lakehouse_DEV`, `Lakehouse_PROD`) or a Warehouse connection string. The core pipeline logic remains identical, but when you run it in the development environment, you provide the "DEV" values, and in production, you provide the "PROD" values. This avoids hardcoding environment-specific details.

*   **Flexibility for Ad-Hoc Runs:** They empower users to run a standardized process with custom inputs without needing to modify the pipeline's design.
    *   **Example:** A business analyst needs a data extract for a specific date range. You can have a pipeline with `StartDate` and `EndDate` parameters. The analyst can trigger the pipeline, enter the desired dates in the prompt, and get a tailored data file without ever opening the pipeline editor.

#### How Parameters Work in Practice

1.  **Declaration:** You define parameters in the pipeline's settings canvas. For each parameter, you specify:
    *   **Name:** A descriptive name (e.g., `SourceTableName`).
    *   **Type:** A data type like String, Integer, Boolean, Array, or Object.
    *   **Default Value:** An optional value that will be used if no other value is provided when the pipeline is run.

2.  **Usage:** Inside the pipeline, you reference a parameter's value using the expression syntax: `@pipeline().parameters.ParameterName`. You can use this expression almost anywhere an activity needs a value:
    *   In a **Copy Data** activity's source query: `SELECT * FROM dbo.Products WHERE Category = '@{pipeline().parameters.ProductCategory}'`
    *   In a **Notebook** activity, passing it as a notebook parameter.
    *   In an **If Condition** activity's expression to control the execution path.

3.  **Execution:** When you trigger the pipeline manually, Fabric presents a dialog box prompting you to enter values for all declared parameters. If you trigger it via a schedule or another automated method, you configure the parameter values within that trigger's definition.

---

### Pipeline Runs

If a pipeline is the **blueprint**, then a pipeline run is the **actual, physical execution** of that blueprint at a specific point in time. Every time you start a pipeline—whether manually or on a schedule—you create a new, distinct instance called a **pipeline run**.

Each run is a self-contained event with its own unique context, including the specific parameter values used, a start and end time, and a final status. This concept is fundamental to monitoring, debugging, and auditing your data operations.

#### Initiating a Pipeline Run

A run can be started in several ways:

*   **On-Demand (Manual):** Clicking the "Run" button in the Fabric UI. This is typically used for development, testing, or executing one-off, ad-hoc tasks. This is when you are prompted to enter parameter values manually.
*   **Scheduled:** Configuring a schedule to automatically trigger the pipeline at a recurring interval. This is the standard for production workflows. For example:
    *   Every 15 minutes.
    *   Daily at 2:00 AM UTC.
    *   Weekly on Sunday nights.
    When you set up the schedule, you also define the static parameter values that will be used for every scheduled run.

#### The Anatomy and Importance of the Run History

The Fabric UI maintains a detailed history of every pipeline run. Clicking on a specific run ID opens a monitoring view that is invaluable for several reasons:

*   **Debugging and Troubleshooting:** This is the primary use case. The run view shows a visual representation of your pipeline's execution.
    *   **Activity Status:** You can see which activities `Succeeded`, which `Failed`, and which were `Skipped`.
    *   **Error Details:** If an activity fails, you can click on it to see the specific error code and message, telling you exactly what went wrong (e.g., "Login failed for user 'X'," "File not found at path 'Y'," "Column 'Z' does not exist").
    *   **Inputs and Outputs:** You can inspect the detailed JSON input and output for each activity. This lets you see the exact parameter values, file paths, and row counts that were processed, which is critical for understanding why a run behaved in a certain way.

*   **Performance Tuning:** The run history logs the duration of each individual activity. If your pipeline is running slowly, you can easily identify the bottleneck—the specific activity that is taking the most time—and focus your optimization efforts there.

*   **Auditing and Compliance:** The run history provides an immutable log of all data orchestration jobs. You have a record of:
    *   **What** ran (pipeline name).
    *   **When** it ran (start and end times).
    *   **What data it affected** (by inspecting the parameters and activity details).
    *   **Whether it was successful.**
    This is essential for data governance and meeting compliance requirements.

*   **Operational Monitoring:** By observing the run history, you can proactively manage the health of your data platform. A pattern of failures can indicate an underlying issue with a source system, credentials, or infrastructure that needs to be addressed.