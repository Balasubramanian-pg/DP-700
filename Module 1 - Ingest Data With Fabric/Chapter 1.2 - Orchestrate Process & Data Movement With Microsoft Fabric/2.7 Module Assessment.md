![[Pasted image 20250802122543.png]]


**1. What is a data pipeline?** 
**Answer: A sequence of activities to orchestrate a data ingestion or transformation process**

A data pipeline is a series of connected activities that move and transform data from source to destination. It's not just a storage folder or a saved query, but rather an orchestrated workflow that can include copying data, transforming it, running notebooks, calling APIs, and other data processing activities.

**2. You want to use a pipeline to copy data to a folder with a specified name for each run. What should you do?** 
**Answer: Add a parameter to the pipeline and use it to specify the folder name for each run**

Parameters make pipelines flexible and reusable. By adding a parameter, you can dynamically specify the folder name when triggering the pipeline, rather than creating multiple static pipelines or using a Dataflow Gen2 (which is more for data transformation than dynamic folder naming).

**3. You have previously run a pipeline containing multiple activities. What's the best way to check how long each individual activity took to complete?** 
**Answer: View the run details in the run history**

The pipeline run history provides detailed execution information including start/end times and duration for each activity within the pipeline. This gives you the precise timing data you need without having to rerun anything or check dataset refresh times (which wouldn't show individual activity durations).

These questions appear to be related to Microsoft Fabric data pipelines, which are commonly used for data engineering and ETL processes in the Microsoft ecosystem.