When you have completed a pipeline, you can use the **Validate** option to check that the configuration is valid, and then either run it interactively or specify a schedule.

![Screenshot of a the Run menu for a pipeline in Microsoft Fabric.](https://learn.microsoft.com/en-us/training/wwl/use-data-factory-pipelines-fabric/media/run-pipeline.png)

## View run history

You can view the run history for a pipeline to see details of each run, either from the pipeline canvas or from the pipeline item listed in the page for the workspace.

![Screenshot of a pipeline run history in Microsoft Fabric.](https://learn.microsoft.com/en-us/training/wwl/use-data-factory-pipelines-fabric/media/pipeline-runs.png)

# **Pipeline Execution, Validation, and Monitoring in Microsoft Fabric**

When working with pipelines in **Microsoft Fabric's Data Factory**, it's crucial to properly **validate, execute, and monitor** your workflows to ensure reliability and performance. Below is an expanded guide on these key aspects:

---

## **1. Validating Your Pipeline**
Before running a pipeline, always **validate** its configuration to catch errors early.

### **How to Validate:**
1. **In the Pipeline Editor**:
   - Click the **"Validate"** button in the top toolbar (✓ icon).
   - Fabric checks for:
     - Syntax errors in expressions
     - Missing linked service connections
     - Invalid dataset references
     - Activity dependency issues
   - Errors appear in the **"Output"** panel at the bottom.

2. **Common Validation Errors**:
   - `Invalid parameter reference` (e.g., `@{pipeline().parameters.X}` where `X` doesn’t exist).
   - `Linked service not found` (if a dataset references a deleted service).
   - `Activity dependency loop` (if activities depend on each other in a cycle).

### **Best Practices:**
✅ **Validate after every major change** (e.g., adding a new activity).  
✅ **Use parameterization** to avoid hardcoded values that might fail validation.  
✅ **Test in a development workspace** before deploying to production.  

---

## **2. Running Your Pipeline**
Once validated, you can execute the pipeline in multiple ways:

### **A. Interactive (Manual) Run**
- Click **"Run"** in the pipeline editor (▶️ icon).
- For parameterized pipelines, you’ll be prompted to enter values:
  ```json
  {
    "StartDate": "2024-01-01",
    "ForceRefresh": true
  }
  ```
- **Use cases**:
  - Testing during development.
  - On-demand data loads (e.g., ad-hoc reports).

### **B. Scheduled Trigger**
1. **Create a trigger**:
   - Go to **"Manage" → "Triggers" → "New"**.
   - Choose:
     - **Schedule**: Recurring (e.g., daily at 2 AM).
     - **Tumbling Window**: For incremental loads (e.g., every 15 mins).
     - **Event-based**: Run when a file arrives in Blob Storage.

2. **Attach to pipeline**:
   - Link the trigger to your pipeline.
   - Set **concurrency limits** (e.g., max 5 runs at once).

### **C. Programmatic Execution**
- **Via REST API**:
  ```powershell
  POST https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/pipelines/{pipelineId}/runs
  Headers: { "Authorization": "Bearer {token}" }
  Body: { "parameters": { "Environment": "Prod" } }
  ```
- **From Azure Logic Apps/Power Automate** (for event-driven workflows).

---

## **3. Monitoring Pipeline Runs**
After execution, track progress and troubleshoot using:

### **A. Run History**
- Accessible from:
  - The **pipeline editor** (bottom panel).
  - The **workspace item page** (under "Pipeline Runs").
- **Key details per run**:
  - **Status** (Succeeded/Failed/Cancelled).
  - **Start/End time** and duration.
  - **Activity-level metrics** (rows copied, data volume).
  - **Error messages** (if failed).

### **B. Gantt Chart View**
- Visualizes **activity timing and dependencies**.
- Helps identify bottlenecks (e.g., a slow notebook activity).

### **C. Advanced Monitoring**
1. **Power BI Dashboard**:
   - Connect to the **Fabric Activity Log** for trend analysis.
   - Track metrics like:
     - Pipeline success rate.
     - Average run duration.
     - Resource consumption.

2. **Alerts**:
   - Configure in **Fabric Portal → Monitoring → Alerts**.
   - Notify teams via email/Teams if:
     - A pipeline fails.
     - Runs exceed expected duration.

---

## **4. Debugging Failed Runs**
### **Common Issues & Fixes**
| **Issue** | **Root Cause** | **Solution** |
|-----------|---------------|--------------|
| `Timeout` | Long-running query or resource contention. | Increase timeout or optimize SQL. |
| `AuthenticationFailed` | Expired credentials in linked service. | Refresh the connection. |
| `ColumnNotFound` | Schema drift in source data. | Use schema mapping in Copy Data. |
| `SparkJobFailed` | Notebook code error. | Check Spark UI logs. |

### **Steps to Investigate:**
1. Open the failed run in history.
2. Click the **failed activity** to see error details.
3. Check **execution logs** (e.g., Spark driver logs for notebooks).
4. **Rerun from failure** (if supported by the activity).

---

## **5. Best Practices for Reliable Pipelines**
1. **Idempotency**:
   - Use watermarks for incremental loads to avoid duplicates.
   - Implement `DELETE` before full refreshes.

2. **Logging**:
   - Log key metrics (e.g., rows processed) to a control table.

3. **Retry Policies**:
   - Set retries (3x) for transient errors (e.g., network timeouts).

4. **Notifications**:
   - Add a **webhook activity** to post failures to Teams/Slack.

---

## **Summary**
- **Validate** before running to catch configuration errors.  
- **Run interactively** for testing, **schedule** for production.  
- **Monitor** runs via history, Gantt charts, or Power BI.  
- **Debug** using activity logs and alerts.  

