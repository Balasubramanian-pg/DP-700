# Lab: Building an End-to-End Sales Dataflow in Microsoft Fabric

In this hands-on lab, you will create a complete, end-to-end data engineering solution using Dataflows Gen2 and Data Pipelines in Microsoft Fabric. We will build a reusable semantic model by ingesting, transforming, and loading real-world sales data into a Lakehouse.

### **The Scenario**

You are a data engineer tasked with creating a clean, report-ready sales table for your analytics team. The raw data comes from a public OData feed containing related information about orders, order details, and products. Your mission is to combine this data, perform calculations, clean the result, load it into a Fabric Lakehouse, and automate the entire process.

<div align="center">
<svg width="500" height="120" viewBox="0 0 500 120" fill="none" xmlns="http://www.w3.org/2000/svg">
<!-- Sources -->
<rect x="10" y="30" width="100" height="60" rx="5" fill="#E1F5FE" stroke="#01579B" stroke-width="2"/>
<text x="60" y="55" font-family="Segoe UI, sans-serif" font-size="12" text-anchor="middle">OData Source</text>
<text x="60" y="70" font-family="Segoe UI, sans-serif" font-size="10" text-anchor="middle">(Orders, Products, etc.)</text>
<!-- Arrow -->
<path d="M115 60 H 155" stroke="#01579B" stroke-width="2" stroke-dasharray="4 4"/>
<path d="M150 55 L 160 60 L 150 65" stroke="#01579B" stroke-width="2" fill="none"/>
<!-- Dataflow -->
<rect x="165" y="20" width="150" height="80" rx="8" fill="#FFF8E1" stroke="#F57F17" stroke-width="2"/>
<text x="240" y="45" font-family="Segoe UI, sans-serif" font-size="14" font-weight="bold" text-anchor="middle">Dataflow Gen2</text>
<text x="240" y="65" font-family="Segoe UI, sans-serif" font-size="11" text-anchor="middle">Calculate, Merge, & Clean</text>
<!-- Arrow -->
<path d="M320 60 H 360" stroke="#01579B" stroke-width="2" stroke-dasharray="4 4"/>
<path d="M355 55 L 365 60 L 355 65" stroke="#01579B" stroke-width="2" fill="none"/>
<!-- Lakehouse -->
<rect x="370" y="30" width="120" height="60" rx="5" fill="#E8F5E9" stroke="#2E7D32" stroke-width="2"/>
<text x="430" y="60" font-family="Segoe UI, sans-serif" font-size="12" text-anchor="middle">Lakehouse Table</text>
</svg>
</div>

**We will perform these steps:**
1.  Set up a Fabric Workspace and a Lakehouse.
2.  Get data from three related sources: `Orders`, `Order_Details`, and `Products`.
3.  Calculate a `LineTotal` for each item in an order (`Quantity` * `UnitPrice`).
4.  Combine (Merge) the three queries into a single, denormalized table.
5.  Clean up the combined table by removing unneeded columns.
6.  Load the final result into a new Lakehouse table called `FactSales`.
7.  Automate the dataflow execution using a Data Pipeline.

### **Part 1: Prerequisites - Setting Up Your Environment**

Before working with data, you need a Fabric workspace and a Lakehouse to store the output.

**Step 1.1: Create a Fabric Workspace**
1.  Navigate to the [Microsoft Fabric home page](https://app.fabric.microsoft.com/home?experience=fabric) and sign in.
2.  In the menu bar on the left, select **Workspaces**.
3.  Click **+ New workspace**. Name it `SalesAnalyticsLab` and select a licensing mode that includes Fabric capacity (_Trial_, _Premium_, or _Fabric_). Click **Apply**.
    
    

**Step 1.2: Create a Lakehouse**
1.  Inside your new workspace, select **+ New**.
2.  Under the _Data Engineering_ section, select **Lakehouse**.
3.  Give it a unique name, for example, `SalesLakehouse`, and click **Create**.
    
    

    After a minute, your new empty Lakehouse will be created and ready to receive data.
    
    
### **Part 2: Create the Dataflow and Get Data**

**Goal:** Start a new Dataflow Gen2 and connect to the public Northwind OData service to pull in our three source tables.

**Step 2.1: Create a New Dataflow Gen2**
*   In your Fabric workspace, click **+ New** > **Dataflow Gen2**. A new Power Query editor will open.  

**Step 2.2: Get Data from an OData Feed**
1.  In the Power Query editor, select **Get data** > **More...**.
2.  In the search box, type `OData` and select the **OData** connector. Click **Connect**.
    
    
3.  A dialog box will appear. In the **URL** field, paste the public URL for the Northwind service:
    ```
    https://services.odata.org/V3/Northwind/Northwind.svc/
    ```
4.  Leave **Connection** as is, and **Authentication kind** as `Anonymous`. Click **Next**. 

**Step 2.3: Select Your Tables**
1.  A **Navigator** window will appear, showing all available tables in the OData feed.
2.  Check the boxes for `Orders`, `Order_Details`, and `Products`
    
3.  Click the **Create** button in the bottom right.

**Result:** The Power Query editor now shows three separate queries in the **Queries** pane on the left.

### **Part 3: Transform and Shape the Data**

**Goal:** Perform initial transformations on the individual queries before combining them.

**Step 3.1: Calculate a `LineTotal` in `Order_Details`**
1.  In the **Queries** pane, select the **Order_Details** query.
2.  Go to the **Add column** tab and select **Custom column**.
3.  In the Custom Column dialog:
    *   **New column name:** `LineTotal`
    *   **Custom column formula:** `[UnitPrice] * [Quantity]`
        
        
4.  Click **OK**. A new `LineTotal` column will appear.
    

**Step 3.2: Set the Data Type (Best Practice)**
1.  The `LineTotal` column has an "Any" (`ABC 123`) data type. This should be corrected for accuracy.
2.  Click the data type icon on the `LineTotal` column header and select **Fixed decimal number**. This is the best type for currency values.
    
    

> [!NOTE]
> The **Applied Steps** pane on the right tracks every transformation. You've just added "Added custom" and "Changed type" steps. This is the power of Power Query's repeatable logic.

### **Part 4: Merge Queries into a Single Table**

**Goal:** Combine the three queries into one unified table using a "Merge" operation (similar to a SQL JOIN).

**Step 4.1: Start the Merge (Order_Details + Products)**
1.  Ensure the **Order_Details** query is selected.
2.  Go to the **Home** tab, click the `Merge queries` dropdown, and select **Merge queries as new**.
    
    
3.  In the Merge dialog:
    *   The top table is already `Order_Details`. Select `Products` from the dropdown for the bottom table.
    *   Click the `ProductID` column header in *both* tables to define the join key.
    *   Leave the **Join kind** as `Left Outer`. Click **OK**.
        
        

**Step 4.2: Expand the Merged `Products` Data**
1.  A new query (`Merge1`) is created with a `Products` column containing nested `[Table]` data.
2.  Click the **expand icon** (two diverging arrows) on the `Products` column header.
3.  Uncheck `(Select All Columns)` and then select only `ProductName`.
4.  **Important:** Uncheck "Use original column name as prefix". Click **OK**.    

**Step 4.3: Merge the Result with `Orders`**
1.  With your `Merge1` query selected, click **Merge queries** on the **Home** tab (use the main button this time, as we are modifying the current query).
2.  The top table is `Merge1`. For the bottom table, select `Orders`.
3.  Join them by clicking on the `OrderID` column in both tables. Click **OK**.

**Step 4.4: Expand `Orders` Data and Final Cleanup**
1.  Click the **expand icon** on the new `Orders` column.
2.  Select the columns you want: `CustomerID`, `OrderDate`, and `ShipCountry`. Uncheck the prefix box. Click **OK**. 
    
3.  **Final Cleanup:** You now have a wide table. Right-click the headers of columns you don't need (like the original `ProductID` and `OrderID`) and select `Remove`.
4.  **Rename Query:** Right-click `Merge1` in the Queries pane and rename it to **FactSales**.

### **Part 5: Set the Destination and Publish**

**Goal:** Tell Fabric to load the output of our new `FactSales` query into a table in our Lakehouse.

**Step 5.1: Configure the Data Destination**
1.  Select the **FactSales** query.
2.  In the **Query settings** pane (bottom-right), click the **Add data destination** dropdown and select **Lakehouse**.
    
    
3.  In the dialog, sign in if prompted. Select **Next**.
4.  Select your workspace and the `SalesLakehouse` you created.
5.  Enter a **New table name**: `fact_sales`.
    
    [![Selecting the target Lakehouse and table name.](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/data-destination-target.png)](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/data-destination-target.png)
6.  Click **Next**. On the **Choose destination settings** page, select the **Replace** update method. This ensures the table is fully refreshed each time. Click **Save settings**.
    
    [![Choosing destination settings with Replace method.](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/destination-settings.png)](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/destination-settings.png)

**Step 5.2: Publish the Dataflow**
*   In the bottom-right corner of the editor, click the **Publish** button. A spinner will indicate that your dataflow is being saved and the first refresh is starting.

### **Part 6: Verify the Output in the Lakehouse**

**Goal:** Confirm that your data has landed correctly and is ready for use.

**Step 6.1: Check the Lakehouse Table**
1.  Navigate back to your `SalesLakehouse`.
2.  In the **Explorer** pane, click the `...` menu for **Tables** and select **Refresh**.
3.  Expand **Tables** and select the `fact_sales` table. You should see a preview of the clean, merged data you created.   

**Step 6.2: Query with SQL (Optional but powerful)**
1.  From the Lakehouse view, switch from the "Lakehouse" explorer to the **SQL analytics endpoint** using the dropdown in the top right.
2.  Your `fact_sales` table is visible here too. Right-click it and select `New SQL query -> Select TOP 100 rows`. This proves your data is in a fully queryable, high-performance format.

### **Part 7: Orchestrate with a Data Pipeline**

**Goal:** Automate the execution of your dataflow using a pipeline for scheduled, repeatable ingestion.

**Step 7.1: Create a New Pipeline**
1.  In your workspace, select **+ New** > **Data pipeline**.
2.  Create a new pipeline named **Load Sales Data**. 
3.  The pipeline editor opens. If the Copy Data wizard appears, close it.
    
    [![An empty data pipeline canvas.](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/new-pipeline.png)](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/new-pipeline.png)

**Step 7.2: Add and Configure the Dataflow Activity**
1.  On the **Activities** tab, select and add a **Dataflow** activity to the pipeline canvas.
2.  With the new **Dataflow1** activity selected, go to the **Settings** tab below.
3.  In the **Dataflow** drop-down list, select the dataflow you created (it will likely be named `Dataflow 1`).  

**Step 7.3: Save and Run the Pipeline**
1.  On the **Home** tab, click the **Save** icon.
2.  Click the **Run** button to execute the pipeline and wait for it to complete. It may take a few minutes.
    
    [![A pipeline with a dataflow activity that has completed successfully.](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/dataflow-pipeline-succeeded.png)](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/dataflow-pipeline-succeeded.png)

You have now successfully built an automated, orchestrated data ingestion process. This pipeline can be scheduled to run daily, weekly, or on any trigger to keep your `fact_sales` table up-to-date.

### **Part 8: Clean Up Resources**

If you have finished exploring, you can delete the workspace to remove all associated resources.

1.  Navigate to your `SalesAnalyticsLab` workspace.
2.  Select **Workspace settings**. In the **General** section, select **Remove this workspace**.
3.  Confirm the deletion. This will permanently remove the workspace and all its items.
