In Microsoft Fabric, Dataflows (Gen2) connect to various data sources and perform transformations in Power Query Online. They can then be used in Data Pipelines to ingest data into a lakehouse or other analytical store, or to define a dataset for a Power BI report.

Let's create a complete, step-by-step exercise. We will use a real-world public data source to make it practical.

### **The Scenario**

You are a sales analyst. Your task is to retrieve live product, order, and order detail information from a public OData feed (a common way to expose data over the web). You need to combine this data, calculate a total price for each order line, and load the final, clean result into a table in your Fabric Lakehouse for reporting.

We will perform these steps:
1.  Get data from three related sources: `Orders`, `Order_Details`, and `Products`.
2.  Calculate a `LineTotal` for each item in an order (`Quantity` * `UnitPrice`).
3.  Combine (Merge) the three queries into a single, wide table.
4.  Clean up the combined table by removing unneeded columns.
5.  Load the final result into a new Lakehouse table called `FactSales`.

---

### **Prerequisites**
Before working with data in Fabric, create a workspace with the Fabric trial enabled.

1. Navigate to the [Microsoft Fabric home page](https://app.fabric.microsoft.com/home?experience=fabric) at `https://app.fabric.microsoft.com/home?experience=fabric` in a browser, and sign in with your Fabric credentials.
2. In the menu bar on the left, select **Workspaces** (the icon looks similar to ðŸ—‡).
3. Create a new workspace with a name of your choice, selecting a licensing mode that includes Fabric capacity (_Trial_, _Premium_, or _Fabric_).
4. When your new workspace opens, it should be empty.
		![[Pasted image 20250802101653.png]]
		
## Create a lakehouse

Now that you have a workspace, itâ€™s time to create a data lakehouse into which youâ€™ll ingest data.

1. On the menu bar on the left, select **Create**. In the _New_ page, under the _Data Engineering_ section, select **Lakehouse**. Give it a unique name of your choice.
	![[Pasted image 20250802101818.png]]
    > **Note**: If the **Create** option is not pinned to the sidebar, you need to select the ellipsis (**â€¦**) option first.
    
    After a minute or so, a new empty lakehouse will be created.
    ![[Pasted image 20250802101835.png]]
---

### **Part 1: Create the Dataflow and Get Data**

**Goal:** Start a new Dataflow Gen2 and connect to the public Northwind OData service to pull in our three source tables.

**Step 1.1: Create a New Dataflow Gen2**
*   In your Fabric workspace, click `New` in the top left.
*   Select `Dataflow Gen2`. A new Power Query editor will open in your browser.
![[Pasted image 20250802101921.png]]
**Step 1.2.1: Get Data from an CSV**
1. Select **Import from a Text/CSV file**, and create a new data source with the following settings:
    - **Link to file**: _Selected_
    - **File path or URL**: `https://raw.githubusercontent.com/MicrosoftLearning/dp-data/main/orders.csv`
    - **Connection**: Create new connection
    - **data gateway**: (none)
    - **Authentication kind**: Anonymous (it is advised to keep it anonymous only for test | demo scenarios, not with confidential information)
    ![[Pasted image 20250802102216.png]]
2. Select **Next** to preview the file data, and then **Create** the data source. The Power Query editor shows the data source and an initial set of query steps to format the data, as shown here:
		![[Pasted image 20250802102444.png]]
		1. On the toolbar ribbon, select the **Add column** tab. Then select **Custom column** and create a new column.
			![[Pasted image 20250802102627.png]]
		2. Set the _New column name_ to `MonthNo` , set the _Data type_ to **Whole Number** and then add the following formula: `Date.Month([OrderDate])` - as shown here:
			![[custom-column.png]]

![[Pasted image 20250802102748.png]]

>In the Query Settings pane on the right side, notice the **Applied Steps** include each transformation step. At the bottom, you can also toggle the **Diagram flow** button to turn on the Visual Diagram of the steps. Steps can be moved up or down, edited by selecting the gear icon, and you can select each step to see the transformations apply in the preview pane.

Check and confirm that the data type for the **OrderDate** column is set to **Date** and the data type for the newly created column **MonthNo** is set to **Whole Number**.

## Add data destination for Dataflow

![[Pasted image 20250802103528.png]]

1. On the toolbar ribbon, select the **Home** tab. Then in the **Add data destination** drop-down menu, select **Lakehouse**.

> [!NOTE]
> If this option is grayed out, you may already have a data destination set. Check the data destination at the bottom of the Query settings pane on the right side of the Power Query editor. If a default destination is already set, you can remove it and add a new one.

2. In the **Connect to data destination** dialog box, edit the connection and sign in using your Power BI organizational account to set the identity that the dataflow uses to access the lakehouse.

[![Data destination configuration page.](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/dataflow-connection.png)](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/dataflow-connection.png)

1. Select **Next** and in the list of available workspaces, find your workspace and select the lakehouse you created in it at the start of this exercise. Then specify a new table named **orders**:
    
    [![Data destination configuration page.](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/data-destination-target.png)](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/data-destination-target.png)
    
2. Select **Next** and on the **Choose destination settings** page, disable the **Use automatic settings** option, select **Append** and then **Save settings**.
    
    > **Note:** We suggest using the _Power query_ editor for updating data types, but you can also do so from this page, if you prefer.
    
    [![Data destination settings page.](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/destination-settings.png)](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/destination-settings.png)
    
3. On the Menu bar, open **View** and select **Diagram view**. Notice the **Lakehouse** destination is indicated as an icon in the query in the Power Query editor.
    
    [![Query with a lakehouse destination.](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/lakehouse-destination.png)](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/lakehouse-destination.png)
4. On the toolbar ribbon, select the **Home** tab. Then select **Save & run** and wait for the **Dataflow 1** dataflow to be created in your workspace.
After you have done all of these steps it is recommended to use "Check Validation"
![[Pasted image 20250802104144.png]]
This helps us understand the status  of the Query
You can then Click on **Save & Run**
## Add a dataflow to a pipeline

You can include a dataflow as an activity in a pipeline. Pipelines are used to orchestrate data ingestion and processing activities, enabling you to combine dataflows with other kinds of operation in a single, scheduled process. Pipelines can be created in a few different experiences, including Data Factory experience.
![[Pasted image 20250802104301.png]]

1. From your Fabric-enabled workspace, select **+ New item** > **Data pipeline**, then when prompted, create a new pipeline named **Load data**.
    ![[Pasted image 20250802104418.png]]
	Connect to data source
	![[Pasted image 20250802104500.png]]  Since we have not created any tables we can choose to "Load to new table"
	![[Pasted image 20250802104735.png]]  I have enabled partitions but you can choose not to.
		![[Pasted image 20250802105038.png]]
	You can also import schemas if you are migrating from legacy systems. (More on that [here](https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping))
    ![[Pasted image 20250802105111.png]]
    The pipeline editor opens.
    
    [![Empty data pipeline.](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/new-pipeline.png)](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/new-pipeline.png)

> 	[!Tip]If the Copy Data wizard opens automatically, close it!
    ![[Pasted image 20250802105209.png]]
2. Select **Pipeline activity**, and add a **Dataflow** activity to the pipeline.
    
3. With the new **Dataflow1** activity selected, on the **Settings** tab, in the **Dataflow** drop-down list, select **Dataflow 1** (the data flow you created previously)
	    ![[Pasted image 20250802105255.png]]    
4. On the **Home** tab, save the pipeline using the **ðŸ–«** (_Save_) icon.
5. Use the **â–· Run** button to run the pipeline, and wait for it to complete. It may take a few minutes.
    
    [![Pipeline with a dataflow that has completed successfully.](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/dataflow-pipeline-succeeded.png)](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/dataflow-pipeline-succeeded.png)
    
6. In the menu bar on the left edge, select your lakehouse.
7. In the **â€¦** menu for **Tables**, select **refresh**. Then expand **Tables** and select the **orders** table, which has been created by your dataflow.
    
    [![Table loaded by a dataflow.](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/loaded-table.png)](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/loaded-table.png)
    

> **Tip**: In Power BI Desktop, you can connect directly to the data transformations done with your dataflow by using the _Power BI dataflows (Legacy)_ connector.
> 
> You can also make additional transformations, publish as a new dataset, and distribute with intended audience for specialized datasets.
> 
> [![Power BI data source connectors](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/pbid-dataflow-connectors.png)](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/pbid-dataflow-connectors.png)

## Clean up resources

If youâ€™ve finished exploring dataflows in Microsoft Fabric, you can delete the workspace you created for this exercise.

1. Navigate to Microsoft Fabric in your browser.
2. In the bar on the left, select the icon for your workspace to view all of the items it contains.
3. Select **Workspace settings** and in the **General** section, scroll down and select **Remove this workspace**.
4. Select **Delete** to delete the workspace.
---
# Project 2 : Combining Multiple Sources
Everything till Part 1 Remains the same. It is recommended to create a new workspace.

**Step 1.2.2: Get Data from an OData Feed** (If you feel extra motivational you can choose this method too)
*   In the Dataflow editor, click the `Get data` button in the ribbon and choose `More...`.
*   In the search box, type `OData` and select the **OData** connector. Click **Connect**.
* ![[Pasted image 20250802110307.png]]
*   A dialog box will appear. In the **URL** field, paste the following public URL for the Northwind service:
    ```
    https://services.odata.org/V3/Northwind/Northwind.svc/
    ```
*   Click **Next**.
* ![[Pasted image 20250802110338.png]]
	If done correctly this is what it looks likes
		![[Pasted image 20250802110415.png]]
**Step 1.3: Select Your Tables**
*   A **Navigator** window will appear, showing all the available tables in the OData feed.
*   Check the boxes for the following three tables:
    *   `Orders`
    *   `Order_Details`
    *   `Products`
    * ![[Pasted image 20250802110449.png]]
*   Click the **Create** button in the bottom right.
		![[Pasted image 20250802110515.png]]
**Result:** The Power Query editor will now load, and you will see three separate queries listed in the **Queries** pane on the left, one for each table you selected.

---

### **Part 2: Transform and Shape the Data**

**Goal:** Perform initial transformations on the individual queries before we combine them. We'll start by calculating the total for each line item in the `Order_Details` query.

**Step 2.1: Select the `Order_Details` Query**
*   In the **Queries** pane on the left, click on `Order_Details` to make it the active query. You will see its data in the main window.
	
**Step 2.2: Calculate a New Column**
*   We want to create a `LineTotal` column by multiplying `UnitPrice` and `Quantity`.
*   Click on the **Add column** tab in the top ribbon.
*   Click **Custom column**.
*   In the Custom Column dialog:
    *   **New column name:** `LineTotal`
    *   **Custom column formula:** Enter the following formula. You can double-click the column names from the list on the right to insert them.
        ```powerquery
        [UnitPrice] * [Quantity]
        ```
	![[Pasted image 20250802110613.png]]  
* Click **OK**. A new `LineTotal` column will appear at the end of the table.
		![[Pasted image 20250802110715.png]]
**Step 2.3: Set the Data Type (Best Practice)**
*   The new `LineTotal` column has an "Any" (ABC 123) data type. Let's make it a currency.
	![[Pasted image 20250802110749.png]]
*   Click the data type icon on the `LineTotal` column header.
*   Select **Fixed decimal number**. This is the best type for currency values.

> [!NOTE]
> 	If you want to handle performance go ahead and apply locale currency to match the currency type you want
> 	
	
**Why this works:** The "Applied Steps" pane on the right tracks every transformation you make. You've just added "Added custom" and "Changed type" steps. This is the power of Power Query.
![[Pasted image 20250802111005.png]]

---
### **Part 3: Merge Queries into a Single Table**

**Goal:** Combine the three queries into one unified table (`Orders` + `Order_Details` + `Products`) using a "Merge" operation (similar to a SQL JOIN).

**Step 3.1: Start the Merge**
*   Make sure the `Order_Details` query is still selected.
*   Go to the **Home** tab in the ribbon.
*   Click the `Merge queries` dropdown and select **Merge queries as new**. This will create a brand-new query for the result.
![[Pasted image 20250802111055.png]]
**Step 3.2: Merge `Order_Details` with `Products`**
*   The Merge dialog opens.
*   The top table is already `Order_Details`.
*   For the bottom table, select `Products` from the dropdown.
*   Now, tell Power Query how to link them. Click on the `ProductID` column header in the `Order_Details` table, then click on the `ProductID` column header in the `Products` table. They will both highlight.
	![[Pasted image 20250802111228.png]]
*   Leave the **Join kind** as `Left Outer`. Click **OK**.
		![[Pasted image 20250802111145.png]]
**Step 3.3: Expand the Merged Data**
*   A new query (`Merge1`) is created. It has a final column called `Products` with `[Table]` in each cell. We need to expand this to get the columns we want.
*   Click the expand icon (two diverging arrows) on the `Products` column header.
*   Uncheck "(Select All Columns)".
*   Select only `ProductName`.
* ![[Pasted image 20250802111442.png]]
*   Uncheck "Use original column name as prefix".
*   Click **OK**. You will now see the `ProductName` column added to your table.

**Step 3.4: Merge the Result with `Orders`**
*   Now we repeat the process. With your `Merge1` query selected, click `Merge queries` on the **Home** tab (this time, just the main button, not "as new," as we want to continue transforming this query).
*   The top table is `Merge1`. For the bottom table, select `Orders`.
*   Join them by clicking on the `OrderID` column in both tables. Click **OK**.
	![[Pasted image 20250802111534.png]]

**Step 3.5: Expand Again and Clean Up**
*   You now have a new `Orders` column with `[Table]` in it. Click the expand icon.
*   Select the columns you want from the `Orders` table, such as `CustomerID`, `OrderDate`, and `ShipCountry`.
*   Uncheck "Use original column name as prefix". Click **OK**.
	*![[Pasted image 20250802111643.png]]
*   **Final Cleanup:** You now have a wide table with all the data. You can right-click on the headers of columns you don't need (like the original `ProductID` and `OrderID`) and select `Remove`.
*   Finally, rename the query. Right-click `Merge1` in the Queries pane and rename it to `FactSales`.
	*![[Pasted image 20250802111807.png]]

**Part 4: Set the Destination and Publish**

**Goal:** Tell Fabric to load the output of our new `FactSales` query into a table in our Lakehouse.

**Step 4.1: Configure the Data Destination**
*   Select the `FactSales` query.
*   In the bottom-right corner, you'll see the **Data destination** icon. Click it. (If you don't see it, make sure your query is selected).
*   A dialog will appear. Choose **Lakehouse**. Click **Next**.
*   Select your workspace and the `SalesLakehouse` you created.
*   Enter a **New table name**: `fact_sales`.
	![[Pasted image 20250802111939.png]]
	For Destination Settings, let it stay defaulted
	* ![[Pasted image 20250802112027.png]]
* For the **Update method**, choose **Replace**. This means every time the dataflow runs, it will delete the old table and create a new one with the fresh data.
*   Click **Save settings**.

**Step 4.2: Publish the Dataflow**
*   In the bottom-right corner of the editor, click the **Publish** button.
*   A spinner will appear. This saves your Dataflow and immediately starts the first refresh. You can close the editor.

---

### **Part 5: Verify and Use the Output**

**Goal:** Confirm that your data has landed correctly in the Lakehouse and is ready for use.

**Step 5.1: Check the Lakehouse**
*   Navigate to your `SalesLakehouse`.
*   In the Lakehouse Explorer, you will see your new `fact_sales` table under the `Tables` section.
*   Click on the table to preview the data. You should see the clean, merged data you created in the dataflow.
![[Pasted image 20250802112517.png]]
**Step 5.2: Query with SQL (Optional but powerful)**
*   From the Lakehouse view, switch from the "Lakehouse" explorer to the **SQL analytics endpoint** using the dropdown in the top right.
*   Your `fact_sales` table is visible here too. You can right-click it and select `New SQL query -> Select TOP 100 rows`.
*   This proves your data is now in a fully queryable format.

You have now successfully created a Dataflow Gen2 that ingests, transforms, and loads data into a Lakehouse table, making it ready for analysis with SQL or Power BI.

---
