Dataflows Gen2 provide an excellent option for data transformations in Microsoft Fabric. The combination of dataflows and pipelines is useful when you need to perform additional operations on the transformed data.

Data pipelines are a common concept in data engineering and offer a wide variety of activities to orchestrate. Some common activities include:

## Incorporate Dataflow

In Microsoft Fabric, incorporating a Dataflow Gen2 into a pipeline means adding it as an activity within your data pipeline orchestration. When you incorporate a dataflow:

- You can trigger dataflow execution as part of a larger data processing workflow
- The dataflow becomes a reusable component that can be called from multiple pipelines
- You can pass parameters to the dataflow to make it dynamic and configurable
- The dataflow's refresh status can be monitored and used to control subsequent pipeline activities

This allows you to combine the visual, low-code data transformation capabilities of Dataflows Gen2 with the orchestration power of Pipelines.

## Add Notebook

Adding a notebook to a pipeline integrates Spark-based data processing and advanced analytics into your workflow. When you add a notebook activity:

- You can execute Python, R, or Scala code for complex data transformations
- Machine learning models can be trained or applied to your data
- Custom business logic that goes beyond what's possible in dataflows can be implemented
- Notebooks can process data that was prepared by upstream dataflows in the same pipeline

This enables hybrid scenarios where you use dataflows for standard ETL operations and notebooks for advanced analytics or custom processing.

## Get Metadata

The Get Metadata activity retrieves information about your data sources, datasets, or other Fabric items without actually processing the data. In the context of dataflows and pipelines:

- You can check if source files exist before running a dataflow
- File properties like size, modification date, or row count can be retrieved
- Schema information can be validated before processing
- This metadata can drive conditional logic in your pipeline (e.g., only run certain activities if specific conditions are met)

Get Metadata acts as a "sensor" that helps make your pipelines more intelligent and robust by allowing them to adapt based on current data conditions.

## Execute a Script or Stored Procedure

This activity allows you to run custom SQL scripts or stored procedures as part of your pipeline. In relation to dataflows and pipelines:

- You can perform database operations that aren't easily handled by dataflows
- Complex SQL logic, triggers, or database-specific functions can be executed
- Data preparation or cleanup can be done in the database before a dataflow processes the data
- Post-processing operations can be performed after a dataflow completes

This provides flexibility to leverage existing SQL investments and handle scenarios where SQL is more appropriate than the visual dataflow interface.

## Integration Scenarios

These components work together in Microsoft Fabric pipelines to create comprehensive data processing workflows. For example:

1. **Get Metadata** checks if new source files are available
2. **Execute Script** performs any necessary database preparation
3. **Incorporate Dataflow** transforms and loads the data using visual ETL
4. **Add Notebook** applies machine learning models to the processed data

This integration allows you to combine the strengths of different tools within a single, orchestrated workflow in Microsoft Fabric.

Pipelines provide a visual way to complete activities in a specific order. You can use a dataflow for data ingestion, transformation, and landing into a Fabric data store. Then incorporate the dataflow into a pipeline to orchestrate extra activities, like execute scripts or stored procedures after the dataflow has completed.

## Copy Data

The Copy Data activity is a fundamental component for moving data between different sources and destinations within your pipeline. In the context of Dataflows Gen2 and Pipelines integration:

- **Raw data ingestion**: Copy data from external sources (databases, files, APIs) into your Fabric workspace before processing with dataflows
- **Staging operations**: Move data to intermediate storage locations for preprocessing or archival purposes
- **High-volume transfers**: Handle large-scale data movement that might be more efficient than processing through dataflows
- **Cross-workspace movement**: Transfer data between different Fabric workspaces or environments
- **Backup and archival**: Create copies of processed data for compliance or disaster recovery
- **Format conversion**: Convert data between different file formats (CSV to Parquet, JSON to Delta, etc.)

Copy Data activities are particularly useful when you need simple, high-performance data movement without transformation, or when you want to separate data ingestion from data transformation concerns.

## Updated Integration Scenarios

Here's how all five components work together in comprehensive Microsoft Fabric workflows:

1. **Get Metadata** - Check if source data files exist and validate their properties
2. **Copy Data** - Ingest raw data from external sources into Fabric storage (OneLake)
3. **Execute Script** - Run any necessary database preparation or cleanup operations
4. **Incorporate Dataflow** - Transform and cleanse the copied data using visual ETL capabilities
5. **Add Notebook** - Apply advanced analytics, machine learning, or custom business logic to the processed data

## Common Integration Patterns

**Pattern 1: Batch Processing Pipeline**

- Copy Data → Get Metadata → Dataflow Gen2 → Notebook

**Pattern 2: Conditional Processing**

- Get Metadata → (If conditions met) → Copy Data → Dataflow Gen2

**Pattern 3: Hybrid Processing**

- Copy Data → Execute Script → Dataflow Gen2 → Copy Data (to final destination)

**Pattern 4: Multi-source Integration**

- Multiple Copy Data activities → Dataflow Gen2 (combines sources) → Notebook → Copy Data (output)

This combination of activities provides a complete toolkit for building sophisticated data integration and processing workflows that leverage both the simplicity of visual tools (Dataflows Gen2) and the power of code-based processing (Notebooks, Scripts) within Microsoft Fabric's unified platform.

---
### Scheduled Pipelines
Pipelines can also be scheduled or activated by a trigger to run your dataflow. By using a pipeline to run your dataflow, you can have the data refreshed when you need it instead of having to manually run the dataflow. When you're dealing with enterprise or frequently changing data, automation allows you to focus on other responsibilities.

![Screenshot of the pipeline schedule window for a dataflow.](https://learn.microsoft.com/en-us/training/wwl/use-dataflow-gen-2-fabric/media/dataflow-schedule-pipeline.png)

## Pipeline Scheduling Options

**Time-Based Scheduling**

- **Fixed intervals**: Run dataflows every hour, daily, weekly, or monthly
- **Specific times**: Execute at precise times (e.g., 2:00 AM daily when system load is low)
- **Complex schedules**: Multiple runs per day, weekends only, business days only
- **Time zone awareness**: Schedule based on business time zones regardless of where Fabric is hosted

**Advanced Scheduling Patterns**

- **Cron expressions**: Use sophisticated scheduling patterns for complex business requirements
- **Calendar-based**: Skip execution on holidays or specific dates
- **Conditional scheduling**: Run only if certain conditions are met (file availability, previous job success)

## Trigger Types

**File-Based Triggers**

- **File arrival**: Automatically start when new files appear in specific folders
- **File modification**: Trigger when existing files are updated
- **File size thresholds**: Execute when files reach certain sizes
- **Multiple file coordination**: Wait for all expected files before processing

**Event-Based Triggers**

- **Data source changes**: React to database changes or API updates
- **External system notifications**: Respond to webhooks or messages from other systems
- **Dependency completion**: Start after upstream processes finish successfully

**Manual and On-Demand Triggers**

- **API calls**: Trigger pipelines programmatically from external applications
- **User-initiated**: Allow business users to start processes when needed
- **Parameter-driven**: Pass different parameters for various execution scenarios

## Enterprise Benefits

**Operational Efficiency**

- **Hands-off operation**: Data teams can focus on analysis rather than manual data refreshes
- **Consistent execution**: Eliminates human error from manual processes
- **Resource optimization**: Schedule intensive operations during off-peak hours
- **Parallel processing**: Run multiple dataflows simultaneously when resources allow

**Business Continuity**

- **Reliability**: Automated retries and error handling ensure data availability
- **SLA compliance**: Meet business requirements for data freshness automatically
- **Monitoring and alerts**: Get notified when processes fail or complete successfully
- **Audit trails**: Maintain logs of when and how data was processed

## Real-World Scenarios

**Financial Services Example**

```
Daily Schedule:
- 6:00 AM: Copy overnight transaction files
- 6:30 AM: Run dataflow to process transactions
- 7:00 AM: Execute notebook for fraud detection
- 7:30 AM: Copy results to reporting database
```

**Retail Analytics Example**

```
Event-Driven:
- New sales file arrives → Trigger pipeline
- Process sales data through dataflow
- Update inventory levels via stored procedure
- Generate executive dashboard in notebook
```

**Manufacturing IoT Example**

```
Continuous Processing:
- Every 15 minutes: Copy sensor data
- Hourly: Aggregate data through dataflow
- Daily: Run predictive maintenance notebook
- Weekly: Archive processed data
```

## Advanced Automation Features

**Dependency Management**

- **Pipeline chaining**: One pipeline triggers another upon completion
- **Cross-workspace dependencies**: Coordinate processes across different Fabric workspaces
- **External system integration**: Wait for or notify external systems

**Error Handling and Recovery**

- **Automatic retries**: Retry failed activities with exponential backoff
- **Failure notifications**: Alert administrators when issues occur
- **Partial recovery**: Resume from the last successful step rather than starting over
- **Dead letter queues**: Handle problematic data separately

**Performance Optimization**

- **Load balancing**: Distribute processing across available resources
- **Adaptive scheduling**: Adjust timing based on system performance
- **Resource scaling**: Automatically provision more compute power when needed
- **Cost optimization**: Schedule resource-intensive operations during lower-cost periods

## Monitoring and Governance

**Real-Time Visibility**

- **Pipeline dashboards**: Monitor execution status, duration, and success rates
- **Data lineage**: Track how data flows through your automated processes
- **Performance metrics**: Identify bottlenecks and optimization opportunities
- **Resource utilization**: Monitor compute and storage consumption

**Compliance and Security**

- **Access control**: Ensure only authorized personnel can modify automation
- **Change tracking**: Maintain history of pipeline modifications
- **Data governance**: Apply consistent security and privacy policies across automated workflows
- **Compliance reporting**: Generate audit reports for regulatory requirements

This automation transforms data operations from reactive, manual processes to proactive, intelligent systems that adapt to business needs while maintaining reliability and governance standards. The result is a more strategic role for data professionals, focusing on insights and innovation rather than routine maintenance tasks.