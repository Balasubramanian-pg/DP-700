# Orchestration Power: Combining Dataflows and Pipelines

While Dataflows Gen2 are excellent for data transformation, their true power in an enterprise setting is unlocked when combined with **Data Pipelines**. Pipelines act as the conductor, orchestrating a series of activities to create a complete, automated, and robust data processing workflow.

Let's explore the key activities you can orchestrate within a pipeline.

### Key Pipeline Activities

#### 1. Incorporate Dataflow

<div align="center">
<svg width="100" height="100" viewBox="0 0 100 100" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M10 50 H 90" stroke="#880E4F" stroke-width="3" stroke-dasharray="6 3"/>
<rect x="30" y="30" width="40" height="40" rx="5" fill="#FFF8E1" stroke="#F57F17" stroke-width="3"/>
<path d="M40 45 L 60 45 M40 55 L 55 55" stroke="#F9A825" stroke-width="2.5" stroke-linecap="round"/>
<circle cx="10" cy="50" r="5" fill="#FCE4EC" stroke="#880E4F" stroke-width="2"/>
<circle cx="90" cy="50" r="5" fill="#FCE4EC" stroke="#880E4F" stroke-width="2"/>
</svg>
</div>

This is the core integration point. Adding a Dataflow Gen2 as a pipeline activity allows you to run it as part of a larger orchestration.

-   **Trigger Execution:** Run your dataflow as one step in a multi-step workflow.
-   **Create Reusable Components:** Call the same dataflow from multiple pipelines, promoting logic reuse.
-   **Parameterize:** Pass parameters from the pipeline to the dataflow to make it dynamic.
-   **Control Flow:** Use the dataflow's success or failure status to control subsequent pipeline activities.

#### 2. Copy Data

<div align="center">
<svg width="100" height="100" viewBox="0 0 100 100" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M15 30 L 15 70 L 40 70 L 40 30 Z" fill="#E1F5FE" stroke="#0288D1" stroke-width="2"/>
<path d="M60 30 L 60 70 L 85 70 L 85 30 Z" fill="#E8F5E9" stroke="#2E7D32" stroke-width="2"/>
<path d="M42 45 L 58 45" stroke="#37474F" stroke-width="3" stroke-linecap="round"/>
<path d="M55 40 L 60 45 L 55 50" fill="none" stroke="#37474F" stroke-width="3" stroke-linecap="round" stroke-linejoin="round"/>
<path d="M42 55 L 58 55" stroke="#37474F" stroke-width="3" stroke-linecap="round"/>
<path d="M55 50 L 60 55 L 55 60" fill="none" stroke="#37474F" stroke-width="3" stroke-linecap="round" stroke-linejoin="round"/>
</svg>
</div>

The Copy Data activity is your workhorse for high-performance data movement between sources and destinations.

-   **Raw Data Ingestion:** Use it to land raw data from external sources into your Fabric OneLake before a dataflow cleanses it (an ELT pattern).
-   **High-Volume Transfers:** Efficiently move massive datasets where transformation is not immediately needed.
-   **Format Conversion:** Easily convert data between formats like CSV, Parquet, and JSON during the copy process.
-   **Archival:** Move processed data to cold storage for archival or compliance.

#### 3. Add Notebook

<div align="center">
<svg width="100" height="100" viewBox="0 0 100 100" fill="none" xmlns="http://www.w3.org/2000/svg">
<rect x="25" y="30" width="50" height="40" rx="4" fill="#FBE9E7" stroke="#D84315" stroke-width="2"/>
<path d="M35 42 L 45 50 L 35 58" stroke="#BF360C" stroke-width="2.5" stroke-linecap="round" stroke-linejoin="round"/>
<path d="M50 60 H 70" stroke="#BF360C" stroke-width="2.5" stroke-linecap="round"/>
</svg>
</div>

Integrate Spark-based processing for tasks that go beyond standard ETL.

-   **Advanced Analytics:** Execute Python, R, or Scala code for complex data science tasks.
-   **Machine Learning:** Train or apply ML models to data prepared by a dataflow.
-   **Custom Logic:** Implement business logic that is too complex for the Power Query interface.

#### 4. Get Metadata

<div align="center">
<svg width="100" height="100" viewBox="0 0 100 100" fill="none" xmlns="http://www.w3.org/2000/svg">
<rect x="20" y="40" width="30" height="40" rx="3" fill="#E0F2F1" stroke="#00695C" stroke-width="2"/>
<circle cx="65" cy="50" r="18" fill="#FFF" stroke="#00695C" stroke-width="2"/>
<circle cx="65" cy="50" r="10" fill="#B2DFDB"/>
<path d="M78 63 L 88 73" stroke="#00695C" stroke-width="3" stroke-linecap="round"/>
</svg>
</div>

This activity acts as a "sensor" for your pipeline, retrieving information about data without processing it.

-   **Pre-flight Checks:** Verify if a source file exists before launching a data-intensive dataflow.
-   **Validation:** Check file properties like size or modification date to ensure data is fresh.
-   **Dynamic Control Flow:** Use the retrieved metadata to drive conditional logic in your pipeline (e.g., `If file exists, then run Dataflow`).

#### 5. Execute a Script or Stored Procedure

<div align="center">
<svg width="100" height="100" viewBox="0 0 100 100" fill="none" xmlns="http://www.w3.org/2000/svg">
<ellipse cx="50" cy="75" rx="25" ry="10" fill="#EDE7F6" stroke="#5E35B1" stroke-width="2"/>
<path d="M25 45 C 25 35, 75 35, 75 45 V 75 C 75 85, 25 85, 25 75 Z" fill="#EDE7F6" stroke="#5E35B1" stroke-width="2"/>
<path d="M35 55 L 50 65 L 65 55" fill="none" stroke="#4527A0" stroke-width="3" stroke-linecap="round" stroke-linejoin="round"/>
</svg>
</div>

Leverage the power of your database engine by running SQL scripts or stored procedures.

-   **Database Prep:** Perform pre-processing or staging table cleanup in the database before a dataflow runs.
-   **Post-Processing:** Run operations like indexing or merging after a dataflow has loaded data.
-   **Complex SQL Logic:** Execute database-specific functions or complex transactional logic that is best handled in SQL.

### Integration Scenarios & Common Patterns

These activities combine to form powerful, end-to-end workflows.

> [!TIP]
> **The Complete Orchestration Blueprint**
> A common, robust pipeline follows this logical sequence:
> 1.  `Get Metadata` → Check if new source files are available.
> 2.  `Copy Data` → Ingest the raw data into Fabric OneLake.
> 3.  `Execute Script` → Prepare the target database tables.
> 4.  `Incorporate Dataflow` → Transform and load the data.
> 5.  `Add Notebook` → Apply ML models or generate advanced analytics.

#### Common Integration Patterns

-   **Pattern 1: Batch Processing Pipeline**
    `Copy Data` → `Get Metadata` → `Dataflow Gen2` → `Notebook`

-   **Pattern 2: Conditional Processing**
    `Get Metadata` → (If file exists) → `Copy Data` → `Dataflow Gen2`

-   **Pattern 3: Hybrid SQL/Dataflow Processing**
    `Copy Data` → `Execute Script` → `Dataflow Gen2` → `Copy Data` (to final destination)

### Automating Your Workflow with Scheduled Pipelines

Manually running pipelines isn't scalable. Automation ensures your data is consistently refreshed, reliable, and available when the business needs it. You can schedule a pipeline or activate it with a trigger.

<div align="center">

![Screenshot of the pipeline schedule window for a dataflow.](https://learn.microsoft.com/en-us/training/wwl/use-dataflow-gen-2-fabric/media/dataflow-schedule-pipeline.png)
*Configuring a time-based schedule for your pipeline.*
</div>

#### Trigger Types

| Trigger Type | Description | Use Case Example |
| :--- | :--- | :--- |
| **Time-Based** | Runs at fixed intervals (hourly, daily, etc.) or on a complex cron schedule. | Refreshing a sales report every morning at 8:00 AM. |
| **Event-Based**| Starts automatically when a specific event occurs, like a new file arriving in storage. | Processing an order file as soon as it's uploaded by a vendor. |
| **Manual / API** | Triggered on-demand by a user or an external application via an API call. | Allowing a business analyst to refresh a dataset for an urgent ad-hoc report. |

#### The Enterprise Advantage

Automating your pipelines provides significant benefits beyond just convenience:

-   **Operational Efficiency:** Frees up your data team to focus on high-value analysis instead of manual data refreshes.
-   **Reliability & Consistency:** Eliminates human error and ensures data processing adheres to business SLAs.
-   **Resource Optimization:** Schedule resource-intensive jobs during off-peak hours to manage costs and capacity.
-   **Monitoring & Governance:** Gain a complete audit trail and real-time visibility into your data processing jobs with centralized monitoring and alerting.
