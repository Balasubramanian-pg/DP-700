## Lab Overview
- **Duration**: Approximately 45 minutes
- **Prerequisites**: Access to a Microsoft Fabric tenant with Fabric capacity enabled

## 1. Workspace Setup

### Create a Workspace
1. Navigate to [Microsoft Fabric home page](https://app.fabric.microsoft.com/home?experience=fabric-developer)
2. Select **Workspaces** from the left menu (ðŸ—‡ icon)
3. Create a new workspace:
   - Choose any name
   - In Advanced section, select licensing mode with Fabric capacity (Trial, Premium, or Fabric)

## 2. Lakehouse Creation and Data Upload

### Create a Lakehouse
1. Select **Create** from left menu
2. Under **Data Engineering**, select **Lakehouse**
3. Give it a unique name

### Upload Data Files
1. Download data files from: [orders.zip](https://github.com/MicrosoftLearning/dp-data/raw/main/orders.zip)
2. Extract the archive to get an "orders" folder containing:
   - 2019.csv
   - 2020.csv
   - 2021.csv
3. In your lakehouse:
   - Expand **Files** folder
   - Select **...** menu â†’ **Upload** â†’ **Upload folder**
   - Select the local "orders" folder

## 3. Notebook Creation

### Create a Notebook
1. Select **Create** from left menu
2. Under **Data Engineering**, select **Notebook**
3. Rename the notebook to something descriptive (e.g., "Sales Order Analysis")

### Add Markdown Cell
1. Convert first cell to markdown using **Mâ†“** button
2. Edit content to:
   ```
   # Sales order data exploration
   Use this notebook to explore sales order data
   ```

## 4. Data Exploration with PySpark

### Create Initial DataFrame
1. In lakehouse Explorer, select **2019.csv** â†’ **...** â†’ **Load data** â†’ **Spark**
2. This generates:
   ```python
   df = spark.read.format("csv").option("header","true").load("Files/orders/2019.csv")
   display(df)
   ```
3. Run the cell (first run starts Spark session)

### Fix Schema Issues
1. Modify code to:
   ```python
   df = spark.read.format("csv").option("header","false").load("Files/orders/2019.csv")
   ```

### Define Proper Schema
Replace code with:
```python
from pyspark.sql.types import *

orderSchema = StructType([
    StructField("SalesOrderNumber", StringType()),
    StructField("SalesOrderLineNumber", IntegerType()),
    StructField("OrderDate", DateType()),
    StructField("CustomerName", StringType()),
    StructField("Email", StringType()),
    StructField("Item", StringType()),
    StructField("Quantity", IntegerType()),
    StructField("UnitPrice", FloatType()),
    StructField("Tax", FloatType())
])

df = spark.read.format("csv").schema(orderSchema).load("Files/orders/2019.csv")
display(df)
```

### Load All Files
Modify path to use wildcard:
```python
df = spark.read.format("csv").schema(orderSchema).load("Files/orders/*.csv")
```

## 5. Data Analysis

### Filtering Data
```python
customers = df.select("CustomerName", "Email").where(df['Item']=='Road-250 Red, 52')
print(customers.count())
print(customers.distinct().count())
display(customers.distinct())
```

### Aggregation
```python
productSales = df.select("Item", "Quantity").groupBy("Item").sum()
display(productSales)
```

### Yearly Sales
```python
from pyspark.sql.functions import *
yearlySales = df.select(year(col("OrderDate")).alias("Year")).groupBy("Year").count().orderBy("Year")
display(yearlySales)
```

## 6. Data Transformation

### Transform Columns
```python
from pyspark.sql.functions import *

# Create Year and Month columns
transformed_df = df.withColumn("Year", year(col("OrderDate"))).withColumn("Month", month(col("OrderDate")))

# Create FirstName and LastName fields
transformed_df = transformed_df.withColumn("FirstName", split(col("CustomerName"), " ").getItem(0)) \
                             .withColumn("LastName", split(col("CustomerName"), " ").getItem(1))

# Filter and reorder columns
transformed_df = transformed_df["SalesOrderNumber", "SalesOrderLineNumber", "OrderDate", 
                               "Year", "Month", "FirstName", "LastName", "Email", 
                               "Item", "Quantity", "UnitPrice", "Tax"]

display(transformed_df.limit(5))
```

## 7. Data Storage

### Save as Parquet
```python
transformed_df.write.mode("overwrite").parquet('Files/transformed_data/orders')
print("Transformed data saved!")
```

### Read Parquet
```python
orders_df = spark.read.format("parquet").load("Files/transformed_data/orders")
display(orders_df)
```

### Partitioned Storage
```python
orders_df.write.partitionBy("Year","Month").mode("overwrite").parquet("Files/partitioned_data")
print("Transformed data saved!")
```

### Read Partitioned Data
```python
orders_2021_df = spark.read.format("parquet").load("Files/partitioned_data/Year=2021/Month=*")
display(orders_2021_df)
```

## 8. Working with Tables and SQL

### Create Table
```python
# Create a new table
df.write.format("delta").saveAsTable("salesorders")

# Get the table description
spark.sql("DESCRIBE EXTENDED salesorders").show(truncate=False)
```

### SQL Query in PySpark
```python
df = spark.sql("SELECT * FROM [your_lakehouse].salesorders LIMIT 1000")
display(df)
```

### Pure SQL Cell
```python
%%sql
SELECT YEAR(OrderDate) AS OrderYear,
       SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue
FROM salesorders
GROUP BY YEAR(OrderDate)
ORDER BY OrderYear;
```

## 9. Data Visualization

### Built-in Charting
1. Run SQL query:
   ```python
   %%sql
   SELECT * FROM salesorders
   ```
2. In results, select **+ New chart**
3. Configure as bar chart:
   - X-axis: Item
   - Y-axis: Quantity (Sum)

### Matplotlib
```python
%%sql
SELECT CAST(YEAR(OrderDate) AS CHAR(4)) AS OrderYear,
       SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue,
       COUNT(DISTINCT SalesOrderNumber) AS YearlyCounts
FROM salesorders
GROUP BY CAST(YEAR(OrderDate) AS CHAR(4))
ORDER BY OrderYear
```

```python
from matplotlib import pyplot as plt

# Convert to Pandas DataFrame
df_sales = df_spark.toPandas()

# Create bar plot
plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')

# Customize
plt.title('Revenue by Year')
plt.xlabel('Year')
plt.ylabel('Revenue')
plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)
plt.xticks(rotation=45)

plt.show()
```

### Seaborn
```python
import seaborn as sns

# Set theme
sns.set_theme(style="whitegrid")

# Create chart
ax = sns.barplot(x="OrderYear", y="GrossRevenue", data=df_sales)
plt.show()
```

## 10. Cleanup

1. Stop the Spark session from notebook menu
2. Navigate to workspace settings
3. Select **Remove this workspace** â†’ **Delete**

This structured guide covers all steps from workspace creation through data analysis and visualization in Microsoft Fabric using PySpark.