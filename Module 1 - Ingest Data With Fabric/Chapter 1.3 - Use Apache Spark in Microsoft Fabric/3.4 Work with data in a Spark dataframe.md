Natively, Spark uses a data structure called a _resilient distributed dataset_ (RDD); but while you _can_ write code that works directly with RDDs, the most commonly used data structure for working with structured data in Spark is the _dataframe_, which is provided as part of the _Spark SQL_ library. Dataframes in Spark are similar to those in the ubiquitous _Pandas_ Python library, but optimized to work in Spark's distributed processing environment.

> [!NOTE]
> In addition to the Dataframe API, Spark SQL provides a strongly-typed _Dataset_ API that is supported in Java and Scala. We'll focus on the Dataframe API in this module.

# Loading Data into DataFrames in Spark

Let's explore the comprehensive process of loading data into Spark DataFrames, building on your hypothetical products example.

## Basic CSV Loading

The example you provided shows a fundamental way to load CSV data:

```python
%%pyspark
df = spark.read.load('Files/data/products.csv',
    format='csv',
    header=True
)
display(df.limit(10))
```

### Key Parameters for CSV Loading

1. **Schema Inference**:
   - By default, Spark infers schema from data
   - For explicit schema (better performance):
     ```python
     from pyspark.sql.types import *
     
     product_schema = StructType([
         StructField("ProductID", IntegerType()),
         StructField("ProductName", StringType()),
         StructField("Category", StringType()),
         StructField("ListPrice", DoubleType())
     ])
     
     df = spark.read.load('Files/data/products.csv',
         format='csv',
         header=True,
         schema=product_schema
     )
     ```

2. **Common CSV Options**:
   - `sep` or `delimiter`: Specify delimiter (default ',')
   - `inferSchema`: Boolean for schema inference (default often False)
   - `nullValue`: String to treat as null
   - `dateFormat`: For parsing dates
   - `quote`: Character used for quoted values (default '"')
   - `escape`: Escape character (default '\')
   - `encoding`: File encoding (default 'UTF-8')

## Alternative Loading Methods

### 1. Using Format-Specific Shortcuts
```python
# Equivalent to previous example
df = spark.read.csv('Files/data/products.csv', header=True)
```

### 2. Loading from Other Formats
```python
# JSON
df_json = spark.read.json('Files/data/products.json')

# Parquet
df_parquet = spark.read.parquet('Files/data/products.parquet')

# JDBC (database)
df_jdbc = spark.read.format("jdbc").options(
    url="jdbc:postgresql://server/db",
    dbtable="products",
    user="user",
    password="password"
).load()
```

## Handling Complex CSV Cases

### 1. Embedded Commas (like in your ProductName)
```python
df = spark.read.csv('Files/data/products.csv',
    header=True,
    escape='"',  # Handle quoted values
    multiLine=True  # For values with newlines
)
```

### 2. Malformed Records
```python
# Option 1: Drop malformed
df = spark.read.option("mode", "DROPMALFORMED").csv(...)

# Option 2: Keep in separate column
df = spark.read.option("mode", "PERMISSIVE").option("columnNameOfCorruptRecord", "_corrupt_record").csv(...)
```

## Language Interoperability

As shown in your example, Spark supports multiple languages:

### Python (PySpark)
```python
%%pyspark
df = spark.read.csv('Files/data/products.csv', header=True)
```

### Scala
```scala
%%spark
val df = spark.read.option("header", "true").csv("Files/data/products.csv")
```

### SQL (after creating a view)
```sql
%%sql
SELECT * FROM products WHERE Category = 'Mountain Bikes' LIMIT 10
```

## Performance Considerations

1. **Schema Inference Overhead**:
   - For large files, provide explicit schema
   - Or sample the file first:
     ```python
     schema = spark.read.csv('sample.csv', header=True).schema
     df = spark.read.schema(schema).csv('full_file.csv')
     ```

2. **Partitioning During Read**:
   ```python
   df = spark.read.option("partitionColumn", "ProductID")
                  .option("lowerBound", "1")
                  .option("upperBound", "1000")
                  .option("numPartitions", "10")
                  .csv(...)
   ```

3. **Caching**:
   ```python
   df.cache()  # For iterative operations
   ```

## Complete Example with Error Handling

```python
from pyspark.sql.types import *

# Define schema
product_schema = StructType([
    StructField("ProductID", IntegerType(), False),
    StructField("ProductName", StringType(), False),
    StructField("Category", StringType(), True),
    StructField("ListPrice", DecimalType(10,2), True)
])

try:
    # Load with explicit schema and error handling
    df = (spark.read
          .format("csv")
          .option("header", "true")
          .option("escape", "\"")
          .option("mode", "FAILFAST")  # Fail on malformed data
          .schema(product_schema)
          .load("Files/data/products.csv"))
    
    # Print schema and show data
    df.printSchema()
    display(df.limit(10))
    
except Exception as e:
    print(f"Error loading file: {str(e)}")
```

Both of these code samples would produce output like this:

|ProductID|ProductName|Category|ListPrice|
|---|---|---|---|
|771|Mountain-100 Silver, 38|Mountain Bikes|3399.9900|
|772|Mountain-100 Silver, 42|Mountain Bikes|3399.9900|
|773|Mountain-100 Silver, 44|Mountain Bikes|3399.9900|
|...|...|...|...|

### Specifying an explicit schema

In the previous example, the first row of the CSV file contained the column names, and Spark was able to infer the data type of each column from the data it contains. You can also specify an explicit schema for the data, which is useful when the column names aren't included in the data file, like this CSV example:

```
771,"Mountain-100 Silver, 38",Mountain Bikes,3399.9900
772,"Mountain-100 Silver, 42",Mountain Bikes,3399.9900
773,"Mountain-100 Silver, 44",Mountain Bikes,3399.9900
...
```

The following PySpark example shows how to specify a schema for the dataframe to be loaded from a file named **product-data.csv** in this format:

```Python
from pyspark.sql.types import *
from pyspark.sql.functions import *

productSchema = StructType([
    StructField("ProductID", IntegerType()),
    StructField("ProductName", StringType()),
    StructField("Category", StringType()),
    StructField("ListPrice", FloatType())
    ])

df = spark.read.load('Files/data/product-data.csv',
    format='csv',
    schema=productSchema,
    header=False)
display(df.limit(10))
```

The results would once again be similar to:

|ProductID|ProductName|Category|ListPrice|
|---|---|---|---|
|771|Mountain-100 Silver, 38|Mountain Bikes|3399.9900|
|772|Mountain-100 Silver, 42|Mountain Bikes|3399.9900|
|773|Mountain-100 Silver, 44|Mountain Bikes|3399.9900|
|...|...|...|...|


> [!Tip]
> Specifying an explicit schema also improves performance!

## Filtering and grouping dataframes

You can use the methods of the Dataframe class to filter, sort, group, and otherwise manipulate the data it contains. For example, the following code example uses the **select** method to retrieve the **ProductID** and **ListPrice** columns from the **df** dataframe containing product data in the previous example:
```python
pricelist_df = df.select("ProductID", "ListPrice")
```

The results from this code example would look something like this:

|ProductID|ListPrice|
|---|---|
|771|3399.9900|
|772|3399.9900|
|773|3399.9900|
|...|...|

In common with most data manipulation methods, **select** returns a new dataframe object.

> [!Tip]
> Selecting a subset of columns from a dataframe is a common operation, which can also be achieved by using the following shorter syntax:

`pricelist_df = df["ProductID", "ListPrice"]`

You can "chain" methods together to perform a series of manipulations that results in a transformed dataframe. For example, this example code chains the **select** and **where** methods to create a new dataframe containing the **ProductName** and **ListPrice** columns for products with a category of **Mountain Bikes** or **Road Bikes**:

```python
bikes_df = df.select("ProductName", "Category", "ListPrice").where((df["Category"]=="Mountain Bikes") | (df["Category"]=="Road Bikes"))
display(bikes_df)
```

The results from this code example would look something like this:

|ProductName|Category|ListPrice|
|---|---|---|
|Mountain-100 Silver, 38|Mountain Bikes|3399.9900|
|Road-750 Black, 52|Road Bikes|539.9900|
|...|...|...|

To group and aggregate data, you can use the **groupBy** method and aggregate functions. For example, the following PySpark code counts the number of products for each category:
```python
counts_df = df.select("ProductID", "Category").groupBy("Category").count()
display(counts_df)
```

The results from this code example would look something like this:

|Category|count|
|---|---|
|Headsets|3|
|Wheels|14|
|Mountain Bikes|32|
|...|...|

## Saving a dataframe

You'll often want to use Spark to transform raw data and save the results for further analysis or downstream processing. The following code example saves the dataFrame into a _parquet_ file in the data lake, replacing any existing file of the same name.

```python
bikes_df.write.mode("overwrite").parquet('Files/product_data/bikes.parquet')
```


> [!NOTE]
> The Parquet format is typically preferred for data files that you will use for further analysis or ingestion into an analytical store. Parquet is a very efficient format that is supported by most large scale data analytics systems. In fact, sometimes your data transformation requirement may simply be to convert data from another format (such as CSV) to Parquet!

# Writing Partitioned Data in Spark

Partitioning output files is a powerful optimization technique in Spark that organizes data on disk to enable more efficient querying. Let's expand on the concept of writing partitioned data:

## Basic Partitioned Write Operation

The core syntax for writing partitioned data is:
```python
df.write.partitionBy("column1", "column2", ...)
       .format("parquet")  # or other formats
       .mode("overwrite")  # or "append", "ignore", "error"
       .save("output_path")
```

## Key Aspects of Partitioned Writes

### 1. Directory Structure
When you write with `partitionBy`, Spark creates a directory hierarchy:
```
bike_data/
├── Category=Mountain Bikes/
│   ├── part-00000-<hash>.parquet
│   ├── part-00001-<hash>.parquet
├── Category=Road Bikes/
│   ├── part-00000-<hash>.parquet
```

### 2. Partitioning Benefits
- **Query Performance**: Filters on partition columns can skip entire directories
- **Parallelism**: Each partition can be processed independently
- **Organization**: Data is logically grouped by partition values

### 3. Multiple Level Partitioning
You can partition by multiple columns:
```python
bikes_df.write.partitionBy("Category", "Year")
       .mode("overwrite")
       .parquet("Files/bike_data")
```
This creates a structure like:
```
bike_data/
├── Category=Mountain Bikes/
│   ├── Year=2023/
│   ├── Year=2024/
├── Category=Road Bikes/
│   ├── Year=2023/
│   ├── Year=2024/
```

## Advanced Partitioning Options

### 1. Controlling File Size
```python
bikes_df.write.partitionBy("Category")
       .option("maxRecordsPerFile", 100000)  # Control file size
       .parquet("Files/bike_data")
```

### 2. Partition Pruning
When reading, Spark will only scan relevant partitions:
```python
# Only reads from Category=Road Bikes directory
spark.read.parquet("Files/bike_data/Category=Road Bikes")
```

### 3. Bucketing (for even more optimization)
Combine partitioning with bucketing for joins:
```python
bikes_df.write.partitionBy("Category")
       .bucketBy(10, "ProductID")
       .saveAsTable("bucketed_bikes")
```

## Best Practices for Partitioning

1. **Choose Appropriate Partition Columns**:
   - Columns frequently used in WHERE clauses
   - Columns with moderate cardinality (not too many unique values)
   - Columns that evenly distribute data

2. **Avoid Over-Partitioning**:
   - Too many partitions create many small files
   - This causes overhead in HDFS and Spark scheduling

3. **Partition Size Considerations**:
   - Aim for partition sizes between 100MB-1GB
   - Adjust using `repartition()` before writing if needed

4. **File Formats**:
   - Parquet is generally best for analytical queries
   - ORC is another good columnar format
   - CSV/JSON if needed for compatibility

5. **Compression**:
   ```python
   df.write.partitionBy("Category")
          .option("compression", "snappy")
          .parquet("Files/bike_data")
   ```

## Common Pitfalls

1. **Small File Problem**: Too many partitions with little data
2. **Skewed Partitions**: Uneven data distribution across partitions
3. **Schema Evolution**: Adding new columns can complicate partitioned data
4. **Metadata Overhead**: Each partition creates additional metadata

## Example: Comprehensive Write Operation

```python
(bikes_df
  .repartition(10, "Category")  # Control number of files per partition
  .write
  .partitionBy("Category", "Year")
  .option("compression", "snappy")
  .option("maxRecordsPerFile", 100000)
  .mode("overwrite")
  .parquet("Files/bike_data")
)
```

> [!NOTE]
> You can partition the data by multiple columns, which results in a hierarchy of folders for each partitioning key. For example, you might partition sales order data by year and month, so that the folder hierarchy includes a folder for each year value, which in turn contains a subfolder for each month value.

# Loading Partitioned Data in Spark

Partitioned data is a common way to organize large datasets for efficient querying. When data is partitioned, it's stored in a directory hierarchy where each level represents a partitioning column and its values. Here's a deeper explanation of working with partitioned data in Spark:

## Understanding Partitioned Data Structure

A typical partitioned directory structure looks like:
```
bike_data/
├── Category=Road Bikes/
│   ├── Year=2023/
│   │   ├── part-0001.parquet
│   │   ├── part-0002.parquet
│   ├── Year=2024/
│   │   ├── part-0001.parquet
├── Category=Mountain Bikes/
│   ├── Year=2023/
│   │   ├── part-0001.parquet
```

## Loading Partitioned Data

### 1. Loading Specific Partition
```python
road_bikes_df = spark.read.parquet('Files/bike_data/Category=Road Bikes')
```
- This loads only data from the "Road Bikes" category
- The `Category` column won't appear in the DataFrame (all values would be "Road Bikes")

### 2. Loading Multiple Partitions
```python
# Load data for multiple specific categories
selected_categories_df = spark.read.parquet(
    'Files/bike_data/Category=Road Bikes',
    'Files/bike_data/Category=Mountain Bikes'
)
```

### 3. Using Wildcards
```python
# Load all data from 2023 regardless of category
df_2023 = spark.read.parquet('Files/bike_data/*/Year=2023')
```

### 4. Loading All Partitioned Data
```python
# Load all data from all partitions
all_bikes_df = spark.read.parquet('Files/bike_data')
```

## Important Notes About Partitioned Data

1. **Column Exclusion**: Partition columns are not stored in the individual files - they're inferred from the directory structure. When you read the data, Spark automatically knows the partition values but doesn't include them as columns unless you explicitly request it.

2. **Including Partition Columns**: If you need the partition columns in your DataFrame:
   ```python
   spark.conf.set("spark.sql.sources.partitionColumnTypeInference.enabled", "true")
   full_df = spark.read.option("mergeSchema", "true").parquet('Files/bike_data')
   ```

3. **Performance Benefits**: Partition pruning allows Spark to skip reading irrelevant partitions, significantly improving query performance for filtered queries.

4. **Schema Evolution**: If different partitions have slightly different schemas, you may need to use:
   ```python
   df = spark.read.option("mergeSchema", "true").parquet('Files/bike_data')
   ```

5. **Hive-Style Partitioning**: This is the most common format (key=value), but Spark also supports other formats.

## Best Practices

1. Filter early by specifying partitions in the path when possible
2. Be mindful of partition column data types (they're often inferred as strings)
3. Consider the cardinality of partition columns - high cardinality can lead to many small files
4. For complex queries, you may get better performance by reading all data and filtering in Spark rather than through path patterns

> [!NOTE]
> The partitioning columns specified in the file path are omitted in the resulting dataframe. The results produced by the example query would not include a **Category** column - the category for all rows would be _Road Bikes_.