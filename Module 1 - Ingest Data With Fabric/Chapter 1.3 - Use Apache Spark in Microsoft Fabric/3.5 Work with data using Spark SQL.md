# Expanding Spark SQL and the Spark Catalog

## Understanding Spark SQL and the DataFrame API

Spark SQL is a Spark module for structured data processing that:
- Provides a unified interface for working with structured data
- Allows mixing SQL queries with Spark programs
- Supports various data sources through a common API
- Includes an optimized execution engine

### Key Components:

1. **DataFrame API**: Object-oriented, language-integrated API (Python, Scala, Java, R)
2. **SQL Interface**: Full ANSI SQL compatibility
3. **Catalyst Optimizer**: Query optimization framework
4. **Tungsten Execution Engine**: High-performance binary processing

## Working with the Spark Catalog

The Spark catalog serves as a metadata repository, tracking:
- Databases
- Tables
- Views
- Functions
- Temporary objects

### Creating and Managing Database Objects

#### 1. Temporary Views
```python
# Create a temporary view
df.createOrReplaceTempView("products_view")

# Create a global temporary view (visible across all sessions)
df.createOrReplaceGlobalTempView("global_products_view")
```

Key characteristics of temporary views:
- Exist only for the current Spark session
- Not persisted to metastore
- Automatically dropped when session ends
- Bound to a specific Spark session

#### 2. Persistent Tables
```python
# Save as a managed table (Spark controls data and metadata)
df.write.saveAsTable("products_table")

# Save as external table (Spark controls metadata only)
df.write.option("path", "/data/products").saveAsTable("external_products")
```

#### 3. Catalog Operations
```python
# List all databases
spark.catalog.listDatabases()

# List tables in current database
spark.catalog.listTables()

# Get table metadata
spark.catalog.listColumns("products_table")

# Create a database
spark.sql("CREATE DATABASE IF NOT EXISTS product_db")

# Set current database
spark.catalog.setCurrentDatabase("product_db")
```

## SQL Integration Patterns

### 1. Mixing SQL and DataFrame API
```python
# Register a temporary view
df.createOrReplaceTempView("products")

# Run SQL query
sql_df = spark.sql("SELECT * FROM products WHERE ListPrice > 1000")

# Continue with DataFrame API
filtered_df = sql_df.filter(col("Category") == "Mountain Bikes")
```

### 2. Creating Permanent SQL Views
```sql
%%sql
-- Create a persistent view
CREATE VIEW IF NOT EXISTS expensive_products AS
SELECT * FROM products WHERE ListPrice > 1000;

-- Create a materialized view (Spark 3.0+)
CREATE MATERIALIZED VIEW IF NOT EXISTS product_stats AS
SELECT Category, AVG(ListPrice) as avg_price
FROM products
GROUP BY Category;
```

### 3. UDF Registration
```python
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

# Define a Python function
def price_category(price):
    return "High" if price > 1000 else "Low"

# Register as UDF
price_category_udf = udf(price_category, StringType())

# Register in catalog
spark.udf.register("price_category", price_category_udf)

# Use in SQL
spark.sql("SELECT ProductName, price_category(ListPrice) FROM products")
```

## Catalog Persistence Options

| Object Type       | Storage Location             | Lifetime               | Visibility            |
|-------------------|------------------------------|------------------------|-----------------------|
| Temporary View    | Memory                       | Session                | Session only          |
| Global Temp View  | Memory                       | Session                | All sessions          |
| Managed Table     | Spark warehouse directory    | Until explicitly dropped | All sessions         |
| External Table    | User-specified location      | Until explicitly dropped | All sessions         |

## Best Practices

1. **View Naming Conventions**:
   - Use `tmp_` prefix for temporary views
   - Consider schema prefixes for permanent objects (`prod.products`, `analytics.products`)

2. **Lifecycle Management**:
   ```python
   # Clean up temporary views
   spark.catalog.dropTempView("products_view")
   
   # Drop tables
   spark.sql("DROP TABLE IF EXISTS products_table")
   ```

3. **Cross-Language Integration**:
   - Views created in Python/Scala are available to SQL and vice versa
   - Temporary views are session-specific but global temp views are cluster-wide

4. **Metadata Operations**:
   ```python
   # Refresh table metadata
   spark.catalog.refreshTable("products_table")
   
   # Clear all cached data
   spark.catalog.clearCache()
   ```

5. **ACID Transactions** (Delta Lake):
   ```python
   # Create Delta table
   df.write.format("delta").saveAsTable("products_delta")
   
   # Time travel
   spark.sql("SELECT * FROM products_delta VERSION AS OF 12")
   ```

## Example Workflow

```python
# Load data
df = spark.read.csv("products.csv", header=True)

# Create temporary view
df.createOrReplaceTempView("products_staging")

# Transform using SQL
clean_df = spark.sql("""
    SELECT 
        ProductID,
        TRIM(ProductName) as ProductName,
        INITCAP(Category) as Category,
        ROUND(ListPrice, 2) as Price
    FROM products_staging
    WHERE ListPrice IS NOT NULL
""")

# Save as managed table
clean_df.write.saveAsTable("product_db.products")

# Register UDF
spark.udf.register("price_range", lambda p: "High" if p > 1000 else "Medium" if p > 500 else "Low")

# Create analytical view
spark.sql("""
    CREATE VIEW product_db.product_analysis AS
    SELECT 
        Category,
        AVG(Price) as avg_price,
        price_range(AVG(Price)) as price_category
    FROM product_db.products
    GROUP BY Category
""")
```

This comprehensive approach demonstrates how Spark SQL and the catalog system enable seamless transitions between procedural code and declarative SQL, while providing robust metadata management capabilities.

---
A _view_ is temporary, meaning that it's automatically deleted at the end of the current session. You can also create _tables_ that are persisted in the catalog to define a database that can be queried using Spark SQL.

Tables are metadata structures that store their underlying data in the storage location associated with the catalog. In Microsoft Fabric, data for _managed_ tables is stored in the **Tables** storage location shown in your data lake, and any tables created using Spark are listed there.

You can create an empty table by using the `spark.catalog.createTable` method, or you can save a dataframe as a table by using its `saveAsTable` method. Deleting a managed table also deletes its underlying data.

For example, the following code saves a dataframe as a new table named **products**:

```python
df.write.format("delta").saveAsTable("products")
```

> [!NOTE]
> The Spark catalog supports tables based on files in various formats. The preferred format in Microsoft Fabric is **delta**, which is the format for a relational data technology on Spark named _Delta Lake_. Delta tables support features commonly found in relational database systems, including transactions, versioning, and support for streaming data.

Additionally, you can create _external_ tables by using the `spark.catalog.createExternalTable` method. External tables define metadata in the catalog but get their underlying data from an external storage location; typically a folder in the **Files** storage area of a lakehouse. Deleting an external table doesn't delete the underlying data.


> [!Tip]
> You can apply the same partitioning technique to delta lake tables as discussed for parquet files in the previous unit. Partitioning tables can result in better performance when querying them.

# Advanced Spark SQL Query Techniques

Spark SQL provides a powerful interface for querying structured data with SQL syntax. Let's expand on the basic example and explore more comprehensive query capabilities.

## Basic SQL Query Syntax

The foundation of Spark SQL queries follows ANSI SQL standards:

```python
query = """
SELECT ProductID, ProductName, ListPrice
FROM products
WHERE Category IN ('Mountain Bikes', 'Road Bikes')
"""
bikes_df = spark.sql(query)
```

## Advanced Query Features

### 1. Complex Filtering
```python
# Multiple conditions with proper parentheses
high_end_bikes = spark.sql("""
SELECT ProductID, ProductName, ListPrice
FROM products
WHERE Category IN ('Mountain Bikes', 'Road Bikes')
  AND ListPrice > 1000
  AND (ProductName LIKE '%Carbon%' OR ProductName LIKE '%Titanium%')
""")
```

### 2. Aggregations and Grouping
```python
# Category price analysis
price_stats = spark.sql("""
SELECT 
    Category,
    COUNT(*) AS product_count,
    ROUND(MIN(ListPrice), 2) AS min_price,
    ROUND(MAX(ListPrice), 2) AS max_price,
    ROUND(AVG(ListPrice), 2) AS avg_price
FROM products
GROUP BY Category
ORDER BY avg_price DESC
""")
```

### 3. Joins Between Tables
```python
# Join products with sales data
product_sales = spark.sql("""
SELECT 
    p.ProductID,
    p.ProductName,
    p.Category,
    SUM(s.Quantity) AS total_units_sold,
    SUM(s.Quantity * p.ListPrice) AS total_revenue
FROM products p
JOIN sales s ON p.ProductID = s.ProductID
GROUP BY p.ProductID, p.ProductName, p.Category
HAVING total_units_sold > 100
ORDER BY total_revenue DESC
""")
```

### 4. Window Functions
```python
# Rank products within categories
ranked_products = spark.sql("""
SELECT 
    ProductID,
    ProductName,
    Category,
    ListPrice,
    RANK() OVER (PARTITION BY Category ORDER BY ListPrice DESC) AS price_rank,
    ROUND(ListPrice - AVG(ListPrice) OVER (PARTITION BY Category), 2) AS price_diff_from_avg
FROM products
""")
```

## Performance Optimization Techniques

### 1. Predicate Pushdown
```python
# Spark will push filters to the data source when possible
efficient_query = spark.sql("""
SELECT ProductName, ListPrice
FROM products
WHERE Category = 'Mountain Bikes'  -- This filter gets pushed down
  AND ListPrice > 500
""")
```

### 2. Partition Pruning
```python
# For partitioned tables, only relevant partitions are read
partitioned_query = spark.sql("""
SELECT *
FROM partitioned_products
WHERE year = 2023  -- Only reads 2023 partition
""")
```

### 3. Caching Frequently Used Tables
```python
# Cache before multiple queries
spark.sql("CACHE TABLE products")

# Clear cache when done
spark.sql("UNCACHE TABLE products")
```

## SQL Functions Reference

Spark SQL supports hundreds of built-in functions:

### String Functions
```python
spark.sql("""
SELECT 
    ProductName,
    UPPER(ProductName) AS upper_name,
    LOWER(ProductName) AS lower_name,
    LENGTH(ProductName) AS name_length,
    SUBSTRING(ProductName, 1, 10) AS short_name,
    SPLIT(ProductName, ' ')[0] AS first_word
FROM products
""")
```

### Date/Time Functions
```python
spark.sql("""
SELECT 
    OrderID,
    date_format(OrderDate, 'yyyy-MM') AS order_month,
    datediff(Current_date(), OrderDate) AS days_since_order
FROM orders
""")
```

### Mathematical Functions
```python
spark.sql("""
SELECT 
    ProductName,
    ListPrice,
    ROUND(ListPrice * 0.9, 2) AS discounted_price,
    LOG(ListPrice) AS log_price
FROM products
""")
```

## Dynamic SQL Generation

For programmatic query building:

```python
def get_products_by_price_range(min_price, max_price):
    return spark.sql(f"""
    SELECT ProductID, ProductName, ListPrice
    FROM products
    WHERE ListPrice BETWEEN {min_price} AND {max_price}
    ORDER BY ListPrice DESC
    """)

# Usage
midrange_products = get_products_by_price_range(500, 1000)
```

## SQL Query Best Practices

1. **Formatting**: Use clear indentation for complex queries
2. **Parameterization**: Use Spark's parameter binding for security
   ```python
   spark.sql("SELECT * FROM products WHERE Category = :category", 
            {"category": "Mountain Bikes"})
   ```
3. **Explain Plans**: Review query execution with `EXPLAIN`
   ```python
   spark.sql("EXPLAIN EXTENDED SELECT * FROM products").show(truncate=False)
   ```
4. **Query Federation**: Join across different data sources
   ```python
   spark.sql("""
   SELECT p.ProductName, s.SalesAmount
   FROM jdbc.`postgresql://db/products` p
   JOIN hive.default.sales s ON p.ProductID = s.ProductID
   """)
   ```

## Complete Example with Error Handling

```python
from pyspark.sql.utils import AnalysisException

try:
    # Complex query with multiple operations
    result_df = spark.sql("""
    WITH ranked_products AS (
        SELECT 
            ProductID,
            ProductName,
            Category,
            ListPrice,
            RANK() OVER (PARTITION BY Category ORDER BY ListPrice DESC) AS price_rank
        FROM products
        WHERE Category IN ('Mountain Bikes', 'Road Bikes')
    )
    SELECT 
        Category,
        AVG(ListPrice) AS avg_price,
        COUNT(*) AS product_count
    FROM ranked_products
    WHERE price_rank <= 10  -- Top 10 products per category
    GROUP BY Category
    ORDER BY avg_price DESC
    """)
    
    display(result_df)
    
except AnalysisException as e:
    print(f"SQL Error: {e.desc}")
except Exception as e:
    print(f"Unexpected error: {str(e)}")
```

This comprehensive approach to Spark SQL querying demonstrates how to leverage both basic and advanced features while maintaining performance and readability.

## Using SQL code

The previous example demonstrated how to use the Spark SQL API to embed SQL expressions in Spark code. In a notebook, you can also use the `%%sql` magic to run SQL code that queries objects in the catalog, like this:


```sql
%%sql
SELECT Category, COUNT(ProductID) AS ProductCount
FROM products
GROUP BY Category
ORDER BY Category
```

The SQL code example returns a resultset that is automatically displayed in the notebook as a table:

|Category|ProductCount|
|---|---|
|Bib-Shorts|3|
|Bike Racks|1|
|Bike Stands|1|
|...|...|

---