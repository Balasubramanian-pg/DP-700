To edit and run Spark code in Microsoft Fabric, you can use _notebooks_, or you can define a _Spark job_.

## Notebooks

When you want to use Spark to explore and analyze data interactively, use a notebook. Notebooks enable you to combine text, images, and code written in multiple languages to create an interactive item that you can share with others and collaborate on.

![Screenshot of a notebook in Microsoft Fabric.](https://learn.microsoft.com/en-us/training/wwl/use-apache-spark-work-files-lakehouse/media/notebook.png)

Notebooks consist of one or more _cells_, each of which can contain markdown-formatted content or executable code. You can run the code interactively in the notebook and see the results immediately.

## Spark job definition

If you want to use Spark to ingest and transform data as part of an automated process, you can define a Spark job to run a script on-demand or based on a schedule.

![Screenshot of a Spark job definition in Microsoft Fabric.](https://learn.microsoft.com/en-us/training/wwl/use-apache-spark-work-files-lakehouse/media/spark-job.png)

To configure a Spark job, create a Spark Job Definition in your workspace and specify the script it should run. You can also specify a reference file (for example, a Python code file containing definitions of functions that are used in your script) and a reference to a specific lakehouse containing data that the script processes.

# Spark Code Execution in Microsoft Fabric: Notebooks vs. Spark Jobs

Microsoft Fabric provides two primary ways to work with Spark code: **Notebooks** for interactive development and **Spark Job Definitions** for automated execution. Below is a detailed comparison, use cases, and best practices for each approach.

---

## **1. Notebooks: Interactive Spark Development**
### **What Are Notebooks?**
Notebooks are interactive documents that combine **executable code**, **visualizations**, and **narrative text** (Markdown) in a single interface. They support multiple languages (Python, SQL, Scala, R) and are ideal for **exploratory data analysis (EDA)**, **prototyping**, and **collaboration**.

### **Key Features**
✅ **Multi-language support** (switch between PySpark, Spark SQL, etc. in the same notebook)  
✅ **Rich output** (tables, charts, images)  
✅ **Real-time execution** (run cells individually or all at once)  
✅ **Collaboration** (comments, version history, sharing)  

### **When to Use Notebooks**
- **Data exploration** (sampling, profiling, visualization)  
- **Developing and testing transformations** before productionizing  
- **Sharing analyses** with stakeholders (combining code + explanations)  
- **Ad-hoc queries** against lakehouse tables  

### **Example Workflow**
1. **Connect to a Lakehouse**:  
   ```python
   df = spark.read.format("delta").load("Tables/sales")
   ```
2. **Explore Data**:  
   ```python
   display(df.groupBy("Region").agg({"Revenue": "sum"}))
   ```
3. **Visualize**:  
   ```sql
   %%sql
   SELECT Year, SUM(Revenue) FROM sales GROUP BY Year
   ```
4. **Save Results**:  
   ```python
   df.write.format("delta").mode("overwrite").saveAsTable("sales_aggregated")
   ```

### **Best Practices**
- **Modularize code** (use functions in a separate `.py` file and import them).  
- **Parameterize notebooks** for reuse (e.g., `widgets` for dynamic inputs).  
- **Limit output size** (use `display(df.limit(1000))` to avoid OOM errors).  
- **Version control** (commit to Git integration).  

---

## **2. Spark Job Definitions: Automated Execution**
### **What Are Spark Jobs?**
Spark Job Definitions are **automated scripts** (Python, JAR, or Scala) that run Spark code **on a schedule** or **as part of a pipeline**. They are headless (no UI) and optimized for production workflows.

### **Key Features**
✅ **Scheduled execution** (via triggers or pipeline orchestration)  
✅ **Resource control** (configure compute size, autoscale, and timeout)  
✅ **Logging & monitoring** (Spark UI, driver logs, Fabric monitoring)  
✅ **Idempotent runs** (ensure reliability in production)  

### **When to Use Spark Jobs**
- **Scheduled data processing** (daily ETL jobs)  
- **Large-scale transformations** (TB-scale data)  
- **Pipeline integration** (run as part of an ADF/Synapse pipeline)  
- **Production workloads** (where reproducibility matters)  

### **Example Workflow**
1. **Create a Python Script** (`transform_sales.py`):  
   ```python
   from pyspark.sql import functions as F
   df = spark.read.format("delta").load("Tables/sales")
   df_clean = df.dropDuplicates().fillna(0, ["Revenue"])
   df_clean.write.format("delta").mode("overwrite").saveAsTable("sales_clean")
   ```
2. **Define a Spark Job**:  
   - **Main script**: `transform_sales.py`  
   - **Linked Lakehouse**: `Sales_Lakehouse`  
   - **Compute**: Medium cluster (autoscale enabled)  
3. **Schedule or Trigger**:  
   - Run daily at 2 AM **or**  
   - Trigger via pipeline (e.g., after data ingestion).  

### **Best Practices**
- **Test scripts in a notebook first** before moving to a job.  
- **Use logging** (`print` or `logging` module for debugging).  
- **Handle errors gracefully** (e.g., try-catch for missing data).  
- **Optimize resource usage** (cache DataFrames, partition writes).  

---

## **3. Notebooks vs. Spark Jobs: Comparison Table**
| Feature                | Notebooks                          | Spark Jobs                          |
|------------------------|-----------------------------------|-------------------------------------|
| **Execution**          | Interactive                       | Automated                           |
| **UI**                 | Visual (cells, plots)             | Headless (log-based)                |
| **Use Case**           | Exploration, prototyping          | Production ETL, scheduled jobs      |
| **Languages**          | Python, SQL, Scala, R             | Python, JAR, Scala                  |
| **Scalability**        | Limited by local session          | Supports large clusters             |
| **Integration**        | Manual runs                       | Pipelines, triggers                 |
| **Cost**               | Higher (interactive clusters)     | Lower (job-optimized clusters)      |

---

## **4. Combining Notebooks and Spark Jobs**
A common pattern is to:  
1. **Develop code interactively** in a notebook.  
2. **Refactor into functions** (e.g., `clean_data.py`).  
3. **Call the functions** in a Spark Job Definition for production.  

### **Example Pipeline Integration**
1. **Notebook Activity** (for complex transformations):  
   ```json
   {
     "type": "SynapseNotebook",
     "notebook": { "path": "Sales_Transformation" },
     "parameters": { "input_path": "Tables/raw_sales" }
   }
   ```
2. **Spark Job Activity** (for lightweight tasks):  
   ```json
   {
     "type": "SparkJob",
     "sparkJob": { "file": "abfss://scripts/transform_sales.py" }
   }
   ```

---

## **5. Advanced Topics**
### **A. Dynamic Notebook Execution**
Use the `mssparkutils.notebook.run()` function to chain notebooks:  
```python
mssparkutils.notebook.run("DataCleaning", timeout_seconds=300, arguments={"input_path": "Tables/raw"})
```

### **B. Spark Job Monitoring**
- **Fabric Portal**: View job status, duration, and resource usage.  
- **Spark UI**: Debug performance bottlenecks (e.g., slow stages).  
- **Log Analytics**: Route logs to Azure Monitor for alerts.  

### **C. Cost Optimization**
- **For notebooks**: Use smaller clusters and stop idle sessions.  
- **For jobs**: Enable autoscale and spot instances.  

---

## **Summary**
- **Use notebooks** for interactive development, exploration, and collaboration.  
- **Use Spark Jobs** for scheduled, automated, and production workloads.  
- **Combine both** for end-to-end workflows (notebooks → jobs → pipelines).  
