

This is an excellent way to learn. Analyzing data with Apache Spark is a core competency in Microsoft Fabric, and breaking it down into a practical exercise is the best approach.

Let's take a common, real-world example: **Analyzing NYC Yellow Taxi trip data.**

This exercise will be broken down into five smaller, digestible parts, just as you would approach a real data analysis project.

### **The Scenario**

You are a data analyst at the NYC Taxi and Limousine Commission. You've been given a dataset of all taxi trips for a specific month. Your goal is to perform an initial exploratory analysis to answer a few key business questions:

1.  What are the busiest times of day for taxi pickups?
2.  How does the trip distance relate to the total fare amount?
3.  What are the most common payment types?

---

### **Prerequisites**

1.  A **Microsoft Fabric workspace**.
2.  A **Lakehouse** created within that workspace (e.g., `TaxiLakehouse`).
3.  The **NYC Yellow Taxi trip data** for a single month (e.g., January 2022). You can download this public data as a Parquet file from the [TLC website](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page).
4.  Upload the downloaded Parquet file to the `Files` section of your `TaxiLakehouse`.

---

### **Part 1: Setup and Data Loading**

**Goal:** Get the raw data from your Lakehouse file into a Spark DataFrame, which is the primary data structure for analysis in Spark.

**Step 1.1: Create a Notebook**
*   In your Fabric workspace, go to the **Data Engineering** experience.
*   Click `New -> Notebook`.
*   A new notebook will open. In the top-left, attach it to your `TaxiLakehouse`. This gives the notebook access to the Lakehouse's files and tables.

**Step 1.2: Load the Data into a DataFrame**
*   In the first cell of the notebook, write the following PySpark code. This code tells Spark to read the Parquet file you uploaded and load it into a variable called `df_raw`.

```python
# The path to your file in the Lakehouse Files section
# The 'abfss://...' path can be found by right-clicking the file in the Lakehouse explorer and choosing 'Copy ABFSS path'
file_path = "abfss://YourWorkspace@onelake.dfs.fabric.microsoft.com/YourLakehouse.Lakehouse/Files/yellow_tripdata_2022-01.parquet"

# Read the parquet file into a Spark DataFrame
df_raw = spark.read.parquet(file_path)

# Show the first 10 rows to verify it loaded correctly
display(df_raw.limit(10))
```

**Why this works:** The `spark.read.parquet()` command is highly optimized for the Parquet format. The `display()` function in Fabric notebooks provides a rich, interactive table view of your DataFrame, which is much better than the standard `show()`.

---

### **Part 2: Initial Exploration and Cleaning**

**Goal:** Understand the structure of the data and clean it up to make it usable for analysis. Raw data is almost never perfect.

**Step 2.1: Inspect the Schema**
*   The "schema" is the blueprint of your dataâ€”it tells you the column names and their data types.

```python
# Print the schema to see column names and data types
df_raw.printSchema()
```

*   You'll notice columns like `tpep_pickup_datetime` and `tpep_dropoff_datetime` are correctly identified as `timestamp`, but `fare_amount` is a `double`, and `trip_distance` is a `double`. This is good, but sometimes dates load as strings, which would need fixing.

**Step 2.2: Handle Bad or nonsensical Data**
*   Let's filter out trips that don't make sense, like those with zero or negative distance/fare.

```python
# Filter out trips with no distance or negative fares
df_cleaned = df_raw.filter(
    (df_raw["trip_distance"] > 0) &
    (df_raw["fare_amount"] > 0)
)

# You can check the count before and after to see how many rows were removed
print(f"Row count before cleaning: {df_raw.count()}")
print(f"Row count after cleaning: {df_cleaned.count()}")
```
**Why this works:** We create a new DataFrame `df_cleaned` containing only the valid rows. This is a core principle in Spark: you create new, transformed DataFrames rather than modifying them in place (immutability).

**Step 2.3: Create New, Useful Columns (Feature Engineering)**
*   To answer "what are the busiest times?", we need to extract the hour from the full timestamp.

```python
from pyspark.sql.functions import hour

# Create a new column 'hour_of_day' by extracting the hour from the pickup timestamp
df_cleaned = df_cleaned.withColumn("hour_of_day", hour(df_cleaned["tpep_pickup_datetime"]))

# Display the result to see the new column at the end
display(df_cleaned.limit(5))
```
**Why this works:** `withColumn()` is the standard way to add or replace a column in a DataFrame. We use the built-in `hour()` function from Spark's SQL functions library for this.

---

### **Part 3: Analysis and Aggregation**

**Goal:** Use the cleaned data to perform calculations that answer our business questions. This is where we use `groupBy()`.

**Question 1: What are the busiest times of day?**
```python
# Group by the new 'hour_of_day' column, count the number of trips in each group, and order the results
busiest_hours_df = df_cleaned.groupBy("hour_of_day").count().orderBy("hour_of_day")

# Display the aggregated results
display(busiest_hours_df)
```

**Question 2: How does trip distance relate to total fare?**
*   For this, we don't need to aggregate yet. We just need the two columns to visualize their relationship. We'll select a sample to avoid plotting millions of points.

```python
# Select the two columns of interest
distance_vs_fare_df = df_cleaned.select("trip_distance", "total_amount").sample(0.01) # Sample 1% of the data

# Display the data, which we will then visualize
display(distance_vs_fare_df)
```

**Question 3: What are the most common payment types?**
*   The `payment_type` column is numerical. We'll group by it and count. (In a real project, you'd join this with a lookup table where `1 = Credit card`, `2 = Cash`, etc.).

```python
from pyspark.sql.functions import desc

# Group by payment type, count the occurrences, and order by the count descending
payment_types_df = df_cleaned.groupBy("payment_type").count().orderBy(desc("count"))

display(payment_types_df)
```

---

### **Part 4: Visualization**

**Goal:** Turn the tables of numbers from Part 3 into insightful charts, right inside the notebook.

**For Busiest Hours:**
1.  Run the code cell for `busiest_hours_df`.
2.  Below the resulting table, click the **Chart** icon.
3.  In the chart options on the right, set:
    *   **Key:** `hour_of_day`
    *   **Values:** `count`
    *   **Chart type:** **Bar chart**
    *   You will instantly see a bar chart showing taxi demand throughout the day.

**For Distance vs. Fare:**
1.  Run the code cell for `distance_vs_fare_df`.
2.  Click the **Chart** icon.
3.  In the chart options, set:
    *   **Chart type:** **Scatter plot**
    *   The notebook will automatically place `trip_distance` on the X-axis and `total_amount` on the Y-axis. You'll see a clear positive correlation.

---

### **Part 5: Saving the Results**

**Goal:** Persist your cleaned data or aggregated results as a table in the Lakehouse so it can be used by other tools, like Power BI or a SQL query.

```python
# Save the main cleaned DataFrame as a new Delta table in your Lakehouse
df_cleaned.write.mode("overwrite").saveAsTable("taxi_trips_cleaned")

# Save the aggregated busiest hours data as another table
busiest_hours_df.write.mode("overwrite").saveAsTable("taxi_busiest_hours")
```
**Why this works:** The `.write.saveAsTable()` command materializes your DataFrame into a high-performance Delta table within the `Tables` section of your Lakehouse. The `.mode("overwrite")` ensures that if you re-run the notebook, it replaces the old table instead of failing.

Now, you can go to your Lakehouse, see the new tables, and even query them using the SQL endpoint! You've successfully completed an end-to-end analysis workflow.
