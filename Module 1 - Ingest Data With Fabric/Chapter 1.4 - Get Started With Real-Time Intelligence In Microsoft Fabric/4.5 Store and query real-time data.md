Eventhouses are where you store real-time data, often ingested by an eventstream and loaded into tables for further processing and analysis.

![Screenshot of an eventhouse in Microsoft Fabric.](https://learn.microsoft.com/en-us/training/wwl/get-started-kusto-fabric/media/eventhouse.png)

Within an eventhouse, you can create:

- **KQL databases**: real-time optimized data stores that host a collection of tables, stored functions, materialized views, and shortcuts.
- **KQL querysets**: Collections of KQL queries that you can use to work with data in KQL database tables. A KQL queryset supports queries written using Kusto Query Language (KQL) and a subset of the Transact-SQL language.

## Querying data

To query data from a table in a KQL database, you can use the **Kusto Query Language (KQL)**, which is used to write queries in Azure Data Explorer, Azure Monitor Log Analytics, Microsoft Sentinel, and Microsoft Fabric. KQL is a read-only request to process data and return results. KQL queries are made of one or more query statements.

### KQL query statements

A query statement consists of a table name followed by one or more operators that `take`, `filter`, `transform`, `aggregate`, or `join` data. For example, the following query retrieves 10 rows from a table named **stock**:

```sql
stock
| take 10
```

A more complex example might aggregate the data to find the average stock price per stock symbol in the last five minutes:
```sql
 stock
 | where ["time"] > ago(5m)
 | summarize avgPrice = avg(todecimal(bidPrice)) by symbol
 | project symbol, avgPrice
```

> [!NOTE]
> To learn more about KQL, see **[Kusto Query Language (KQL) overview](https://learn.microsoft.com/en-us/kusto/query/)**.

### Using SQL

KQL is optimized for querying large volumes of data, particularly with a time-based element; so it's a great fit for real-time data analysis. However, many data professionals are already familiar with SQL syntax; so KQL databases in eventhouses support a subset of common SQL expressions.

For example, the equivalent SQL to the _take 10_ KQL query discussed previously would be:

```sql
SELECT TOP 10 * FROM stock;
```

### Using Copilot to help with queries

Microsoft Fabric includes Copilot for Real-Time Intelligence, which can help you write the queries you need to extract insights from your eventhouse data. Copilot uses AI to understand the information you're looking for and can generate the required query code for you.

> [!NOTE]
> To learn more about Copilot for Real-Time Intelligence, see **[Copilot for Real-Time Intelligence](https://learn.microsoft.com/en-us/fabric/get-started/copilot-real-time-intelligence)**.

### Architecture and Purpose

**Eventhouses** serve as specialized data repositories designed specifically for high-velocity, time-series data ingestion and analysis. Unlike traditional databases optimized for transactional workloads, eventhouses are architected to handle millions of events per second while maintaining query performance for analytical workloads.

**Key Architectural Principles:**

- **Write-Optimized Design**: Optimized for high-throughput data ingestion with minimal latency
- **Columnar Storage**: Data stored in column-oriented format for efficient analytical queries
- **Time-Based Partitioning**: Automatic partitioning based on ingestion time for optimal query performance
- **Compression**: Advanced compression algorithms to minimize storage costs while maintaining query speed
- **Immutable Data**: Append-only architecture ensuring data integrity and enabling time-travel queries

### Data Ingestion Patterns

**Eventstream Integration:** Eventstreams act as the primary conduit for real-time data ingestion into eventhouses, providing:

- **Buffering**: Temporary storage to handle traffic spikes and ensure data reliability
- **Transformation**: Real-time data cleansing, enrichment, and format conversion
- **Routing**: Intelligent data distribution to appropriate tables based on content or metadata
- **Error Handling**: Dead letter queues and retry mechanisms for failed ingestions
- **Schema Evolution**: Automatic handling of schema changes without disrupting data flow

**Batch vs. Streaming Ingestion:**

- **Micro-batching**: Small batches (seconds to minutes) for balanced throughput and latency
- **Streaming**: Individual event processing for ultra-low latency requirements
- **Bulk Loading**: Historical data migration and backfill operations
- **Change Data Capture (CDC)**: Real-time synchronization from operational databases

## KQL Databases: Specialized Real-Time Data Stores

### Advanced Database Features

**Table Management:**

- **Hot/Warm/Cold Tiers**: Automatic data lifecycle management based on age and access patterns
- **Retention Policies**: Automatic data cleanup based on business requirements
- **Caching Policies**: Intelligent caching for frequently accessed data
- **Update Policies**: Real-time data transformation and enrichment rules

**Performance Optimization:**

- **Ingestion Batching**: Automatic optimization of ingestion batch sizes
- **Extent Management**: Automatic data file optimization for query performance
- **Query Acceleration**: In-memory caching and pre-computed aggregations
- **Partitioning Strategies**: Time-based and custom partitioning schemes

### Tables: The Core Data Structures

**Table Design Patterns:**

**Raw Event Tables:**

```kql
// Example: IoT sensor data table
.create table SensorReadings (
    Timestamp: datetime,
    DeviceId: string,
    SensorType: string,
    Value: real,
    Unit: string,
    Location: string,
    Metadata: dynamic
)
```

**Aggregated Tables:**

```kql
// Example: Pre-aggregated hourly metrics
.create table HourlyMetrics (
    TimeWindow: datetime,
    DeviceId: string,
    AvgValue: real,
    MinValue: real,
    MaxValue: real,
    RecordCount: long
)
```

**Lookup Tables:**

```kql
// Example: Device metadata
.create table Devices (
    DeviceId: string,
    DeviceType: string,
    Location: string,
    InstallationDate: datetime,
    Manufacturer: string
)
```

### Stored Functions: Reusable Query Logic

**Function Categories:**

**Data Transformation Functions:**

```kql
.create function NormalizeTemperature(temp:real, fromUnit:string) {
    case(
        fromUnit == "F", (temp - 32) * 5/9,
        fromUnit == "K", temp - 273.15,
        temp  // Assume Celsius if not specified
    )
}
```

**Business Logic Functions:**

```kql
.create function CalculateDeviceHealth(deviceId:string, timeRange:timespan) {
    SensorReadings
    | where DeviceId == deviceId and Timestamp > ago(timeRange)
    | summarize 
        ReadingCount = count(),
        AvgResponseTime = avg(ResponseTime),
        ErrorRate = countif(Status == "Error") * 100.0 / count()
    | extend HealthScore = case(
        ErrorRate > 10, "Poor",
        ErrorRate > 5, "Fair",
        ErrorRate > 1, "Good",
        "Excellent"
    )
}
```

**Parameterized Analytics Functions:**

```kql
.create function GetTopDevicesByMetric(metricName:string, topN:int, timeWindow:timespan) {
    SensorReadings
    | where Timestamp > ago(timeWindow) and SensorType == metricName
    | summarize AvgValue = avg(Value) by DeviceId
    | top topN by AvgValue desc
}
```

### Materialized Views: Pre-Computed Insights

**Real-Time Aggregations:**

```kql
.create materialized-view HourlyDeviceMetrics on table SensorReadings
{
    SensorReadings
    | summarize 
        AvgValue = avg(Value),
        MinValue = min(Value),
        MaxValue = max(Value),
        RecordCount = count()
        by DeviceId, SensorType, bin(Timestamp, 1h)
}
```

**Business KPI Views:**

```kql
.create materialized-view RealtimeDashboardMetrics on table SensorReadings
{
    SensorReadings
    | where Timestamp > ago(24h)
    | summarize 
        ActiveDevices = dcount(DeviceId),
        TotalReadings = count(),
        AvgValue = avg(Value),
        AlertCount = countif(Value > 100)  // Business threshold
        by bin(Timestamp, 15m)
}
```

### Shortcuts: Unified Data Access

**Cross-Database References:**

- **OneLake Integration**: Direct access to data in other Fabric workspaces
- **External Data Sources**: Connections to Azure Data Lake, SQL databases
- **Delta Lake Tables**: Access to data lake tables without data movement
- **Real-Time Streaming**: Live connections to other eventhouses

## KQL Querysets: Collaborative Query Development

### Advanced Query Organization

**Query Categories:**

**Exploratory Queries:**

```kql
// Quick data exploration
SensorReadings
| summarize count() by DeviceId
| order by count_ desc
| take 20
```

**Operational Monitoring:**

```kql
// Real-time system health
SensorReadings
| where Timestamp > ago(5m)
| summarize 
    DevicesReporting = dcount(DeviceId),
    TotalReadings = count(),
    ErrorRate = countif(Status == "Error") * 100.0 / count()
| extend Status = case(
    ErrorRate > 5, "Critical",
    ErrorRate > 1, "Warning",
    "Healthy"
)
```

**Business Intelligence:**

```kql
// Trend analysis for decision making
SensorReadings
| where Timestamp > ago(30d) and SensorType == "Temperature"
| summarize AvgTemp = avg(Value) by bin(Timestamp, 1d), Location
| render timechart
```

**Advanced Analytics:**

```kql
// Anomaly detection using built-in functions
SensorReadings
| where SensorType == "Pressure" and Timestamp > ago(7d)
| make-series PressureValue = avg(Value) on Timestamp step 1h by DeviceId
| extend Anomalies = series_decompose_anomalies(PressureValue, 2.0)
| mv-expand Timestamp, PressureValue, Anomalies
| where Anomalies > 0
```

### Collaboration Features

**Query Sharing and Versioning:**

- **Version Control**: Track changes and roll back to previous versions
- **Comments and Documentation**: Inline documentation for complex queries
- **Parameterization**: Reusable queries with dynamic parameters
- **Scheduled Execution**: Automated query execution for regular reports

## Advanced KQL Query Patterns

### Complex Data Manipulation

**Time-Series Analysis:**

```kql
// Moving averages and trend analysis
SensorReadings
| where DeviceId == "sensor_001" and Timestamp > ago(24h)
| sort by Timestamp asc
| extend 
    MovingAvg5m = avg_prev(Value, 5),
    MovingAvg15m = avg_prev(Value, 15),
    Trend = case(
        Value > prev(Value, 1), "Increasing",
        Value < prev(Value, 1), "Decreasing",
        "Stable"
    )
| project Timestamp, Value, MovingAvg5m, MovingAvg15m, Trend
```

**Correlation Analysis:**

```kql
// Finding correlations between different sensors
let TemperatureData = SensorReadings 
    | where SensorType == "Temperature" and Timestamp > ago(24h)
    | project Timestamp, DeviceId, TempValue = Value;
let HumidityData = SensorReadings 
    | where SensorType == "Humidity" and Timestamp > ago(24h)
    | project Timestamp, DeviceId, HumidityValue = Value;
TemperatureData
| join kind=inner (HumidityData) on DeviceId, Timestamp
| extend TimeBin = bin(Timestamp, 1h)
| summarize 
    AvgTemp = avg(TempValue),
    AvgHumidity = avg(HumidityValue),
    Correlation = corr(TempValue, HumidityValue)
    by TimeBin, DeviceId
```

**Pattern Recognition:**

```kql
// Identifying recurring patterns
SensorReadings
| where DeviceId == "pump_001" and Timestamp > ago(7d)
| extend Hour = hourofday(Timestamp), DayOfWeek = dayofweek(Timestamp)
| summarize AvgValue = avg(Value), ReadingCount = count() by Hour, DayOfWeek
| order by DayOfWeek asc, Hour asc
```

### Performance Optimization Techniques

**Query Optimization Strategies:**

- **Early Filtering**: Apply filters as early as possible in the query pipeline
- **Selective Projections**: Only select columns needed for analysis
- **Appropriate Aggregation Levels**: Use pre-aggregated data when possible
- **Efficient Joins**: Optimize join operations for large datasets

**Example Optimized Query:**

```kql
// Before optimization (inefficient)
SensorReadings
| join kind=inner (Devices) on DeviceId
| where Timestamp > ago(1h) and Location == "Building_A"
| summarize avg(Value) by DeviceType

// After optimization (efficient)
SensorReadings
| where Timestamp > ago(1h)  // Filter early
| join kind=inner (
    Devices 
    | where Location == "Building_A"  // Filter lookup table
    | project DeviceId, DeviceType    // Project only needed columns
) on DeviceId
| summarize avg(Value) by DeviceType
```

## SQL Support in KQL Databases

### SQL Compatibility Layer

**Supported SQL Features:**

- **Basic SELECT Statements**: Standard column selection and filtering
- **Aggregation Functions**: SUM, AVG, COUNT, MIN, MAX
- **GROUP BY and ORDER BY**: Data grouping and sorting
- **JOIN Operations**: INNER, LEFT, RIGHT joins with limitations
- **Window Functions**: ROW_NUMBER, RANK, LAG, LEAD
- **Common Table Expressions (CTEs)**: For complex query organization

**SQL Query Examples:**

**Basic Analytics:**

```sql
-- Top 10 devices by average value in last hour
SELECT TOP 10 
    DeviceId,
    AVG(CAST(Value AS DECIMAL(10,2))) as AvgValue,
    COUNT(*) as ReadingCount
FROM SensorReadings
WHERE Timestamp > DATEADD(hour, -1, GETUTCDATE())
GROUP BY DeviceId
ORDER BY AvgValue DESC;
```

**Time-Based Analysis:**

```sql
-- Hourly trends for specific device
SELECT 
    DATEPART(hour, Timestamp) as Hour,
    AVG(CAST(Value AS DECIMAL(10,2))) as AvgValue,
    MIN(CAST(Value AS DECIMAL(10,2))) as MinValue,
    MAX(CAST(Value AS DECIMAL(10,2))) as MaxValue
FROM SensorReadings
WHERE DeviceId = 'sensor_001' 
    AND Timestamp > DATEADD(day, -1, GETUTCDATE())
GROUP BY DATEPART(hour, Timestamp)
ORDER BY Hour;
```

**Window Functions:**

```sql
-- Running totals and comparisons
SELECT 
    Timestamp,
    DeviceId,
    Value,
    SUM(CAST(Value AS DECIMAL(10,2))) OVER (
        PARTITION BY DeviceId 
        ORDER BY Timestamp 
        ROWS UNBOUNDED PRECEDING
    ) as RunningTotal,
    LAG(Value, 1) OVER (
        PARTITION BY DeviceId 
        ORDER BY Timestamp
    ) as PreviousValue
FROM SensorReadings
WHERE Timestamp > DATEADD(hour, -2, GETUTCDATE())
ORDER BY DeviceId, Timestamp;
```

### Limitations and Considerations

**SQL Limitations in KQL Databases:**

- **No UPDATE/DELETE**: Read-only operations only
- **Limited JOIN Types**: Full outer joins not supported
- **No Stored Procedures**: Use KQL functions instead
- **Limited String Functions**: KQL provides richer text processing
- **No Transactions**: Single query execution model

**When to Use SQL vs. KQL:**

- **Use SQL for**: Familiar syntax, simple aggregations, basic reporting
- **Use KQL for**: Complex analytics, time-series operations, advanced functions

## Copilot for Real-Time Intelligence

### AI-Powered Query Development

**Natural Language to Query Translation:**

**Example Interactions:**

```
User: "Show me the average temperature for each device in the last 2 hours"
Copilot generates:
SensorReadings
| where SensorType == "Temperature" and Timestamp > ago(2h)
| summarize AvgTemperature = avg(Value) by DeviceId
| order by AvgTemperature desc
```

```
User: "Find devices that haven't reported in the last 30 minutes"
Copilot generates:
let ActiveDevices = SensorReadings
    | where Timestamp > ago(30m)
    | distinct DeviceId;
Devices
| where DeviceId !in (ActiveDevices)
| project DeviceId, Location, LastSeen = datetime(null)
```

### Advanced Copilot Capabilities

**Query Optimization Suggestions:**

- **Performance Recommendations**: Suggestions for improving query performance
- **Best Practice Guidance**: Following KQL best practices automatically
- **Alternative Approaches**: Multiple ways to achieve the same result
- **Error Correction**: Automatic fixing of syntax and logical errors

**Data Exploration Assistance:**

```
User: "What patterns can you find in the sensor data?"
Copilot might generate:
// Exploring data patterns
SensorReadings
| summarize 
    RecordCount = count(),
    UniqueDevices = dcount(DeviceId),
    DateRange = strcat(format_datetime(min(Timestamp), "yyyy-MM-dd"), " to ", format_datetime(max(Timestamp), "yyyy-MM-dd")),
    SensorTypes = make_set(SensorType),
    AvgReadingsPerDevice = count() / dcount(DeviceId)
| project RecordCount, UniqueDevices, DateRange, SensorTypes, AvgReadingsPerDevice

// Follow-up: Temporal patterns
SensorReadings
| extend Hour = hourofday(Timestamp)
| summarize ReadingCount = count() by Hour
| render columnchart with (title="Readings by Hour of Day")
```

**Business Intelligence Integration:**

- **Dashboard Query Generation**: Queries optimized for visualization tools
- **Report Automation**: Scheduled queries for regular business reports
- **Alert Query Creation**: Monitoring queries with appropriate thresholds
- **Data Quality Checks**: Queries to identify data issues and anomalies

This comprehensive approach to eventhouses and real-time data management enables organizations to build sophisticated analytics solutions that scale from simple monitoring to complex predictive analytics, all while maintaining the performance needed for real-time decision making.