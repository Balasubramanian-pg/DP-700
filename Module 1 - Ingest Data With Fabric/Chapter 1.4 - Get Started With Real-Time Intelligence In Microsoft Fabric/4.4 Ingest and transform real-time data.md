Eventstreams in Microsoft Fabric are used to capture, transform, and load real-time data from a wide range of streaming data sources. When you set up an eventstream in the system, you're defining a data processing engine that runs perpetually to ingest and transform real-time data. You tell it where to get data from, where to send it, and how to change it along the way if needed.

![Screenshot of an eventstream in Microsoft Fabric.](https://learn.microsoft.com/en-us/training/wwl/get-started-kusto-fabric/media/eventstream.png)

# **Comprehensive Guide: Data Sources for EventStreams in Microsoft Fabric**

Microsoft Fabric's EventStreams provide powerful real-time data ingestion capabilities, supporting a diverse range of data sources. These sources can be broadly categorized into **external services**, **Fabric-native events**, and **sample datasets** for testing and exploration. Below is a detailed breakdown of each category, including use cases, setup considerations, and best practices.

---

## **1. External Services (Cloud & On-Premises)**
EventStreams can ingest data from various external sources, enabling seamless integration with existing data pipelines.

### **A. Azure Services**
| Source              | Description | Use Cases |
|---------------------|------------|-----------|
| **Azure Event Hubs** | High-throughput event ingestion | IoT telemetry, clickstream analytics |
| **Azure IoT Hub** | Optimized for IoT device messaging | Sensor data, device monitoring |
| **Azure Storage (Blob/ADLS)** | Capture new files or changes | Log processing, batch-to-stream pipelines |
| **Azure SQL DB (CDC)** | Change Data Capture from databases | Real-time analytics on transactional data |
| **Apache Kafka** | Connect to on-prem or cloud Kafka clusters | Legacy stream processing migration |

#### **Key Features:**
- **Auto-ingestion** from Azure sources with minimal configuration.
- **Scalability** to handle millions of events per second.
- **Exactly-once processing** for reliable data pipelines.

#### **Example Workflow:**
1. **IoT Hub** streams sensor data → **EventStream** applies filters (e.g., `temperature > 100°C`).
2. **Aggregate transformation** computes avg. temperature per minute.
3. Data routed to **Eventhouse** for real-time dashboards.

---

### **B. Other External Sources**
| Source              | Description | Use Cases |
|---------------------|------------|-----------|
| **Database CDC (Oracle, PostgreSQL)** | Capture inserts/updates/deletes | Fraud detection, inventory tracking |
| **Custom REST APIs** | Ingest data from SaaS apps | Social media feeds, CRM updates |
| **Webhooks** | HTTP-based event notifications | Alert systems, third-party integrations |

#### **Setup Notes:**
- **CDC sources** require a **gateway** for on-prem databases.
- **Custom endpoints** may need **authentication** (OAuth, API keys).

---

## **2. Fabric Events (Native Microsoft Fabric Triggers)**
EventStreams can also process **internal Fabric activities**, enabling real-time monitoring and automation.

### **Supported Fabric Event Sources**
| Source | Description | Use Cases |
|--------|------------|-----------|
| **Workspace Item Changes** | Track creations/modifications/deletions of Fabric items (reports, notebooks, etc.) | Audit logging, governance |
| **OneLake Data Changes** | Monitor file additions/modifications in OneLake | Real-time data validation, trigger downstream pipelines |
| **Fabric Job Events** | Capture Spark job/SQL analytics job statuses | Failure alerts, performance monitoring |

#### **Example Workflow:**
1. A **new Power BI report** is published → **EventStream** captures the event.
2. **Fabric Activator** notifies the team via Teams.
3. Event metadata stored in **Lakehouse** for compliance.

---

## **3. Sample Data (Built-in Datasets)**
Microsoft Fabric provides **pre-built sample datasets** to help users explore real-time analytics without setting up external sources.

### **Available Samples**
| Dataset | Scenario |
|---------|----------|
| **Website Clickstream** | User behavior analytics |
| **IoT Device Telemetry** | Predictive maintenance |
| **Retail Sales** | Dynamic pricing analysis |

#### **When to Use Sample Data:**
- **Learning EventStreams** (testing transformations like `Filter` or `Aggregate`).
- **Demoing real-time dashboards** in Power BI.
- **Prototyping** before connecting production data.

---

## **Comparison: External vs. Fabric vs. Sample Sources**
| Criteria | External Sources | Fabric Events | Sample Data |
|----------|------------------|---------------|-------------|
| **Setup Complexity** | Moderate (auth, networking) | Low (native integration) | None |
| **Latency** | Sub-second (Event Hubs/Kafka) | Near real-time | Instant |
| **Use Case** | Production pipelines | Internal monitoring | Training/demos |
| **Cost** | Depends on source (e.g., Kafka clusters) | Included in Fabric | Free |

---

## **Best Practices for Source Selection**
1. **Start with samples** to prototype transformations.
2. **For production:**
   - Use **Event Hubs/Kafka** for high-volume streams.
   - Use **CDC** for database syncing.
3. **Monitor Fabric events** for governance (e.g., tracking PBI report changes).
4. **Consider latency needs**:
   - **<1 sec**: Event Hubs/IoT Hub.
   - **1-5 sec**: Database CDC.
   - **Batch**: OneLake file triggers.

---

## **Conclusion**
Microsoft Fabric EventStreams support:
✅ **External sources** (Azure, Kafka, databases) for enterprise pipelines.  
✅ **Fabric-native events** for workspace monitoring.  
✅ **Sample data** for rapid experimentation.  

By choosing the right source(s), you can build **end-to-end real-time solutions**—from IoT alerts to live business dashboards—all within Fabric’s unified platform.  


> [!Git]
> For more information about supported sources, see **[Supported sources for Fabric Real-Time hub](https://learn.microsoft.com/en-us/fabric/real-time-hub/supported-sources)**.

# **Data Transformations in EventStreams**  

Microsoft Fabric’s **EventStream** allows you to process and refine streaming data in real time using powerful **transformations**. These transformations enable filtering, aggregation, field management, and more—ensuring your data is clean, structured, and ready for analysis before being routed to destinations like **Eventhouse, Lakehouse, or external APIs**.  

Below is a detailed breakdown of each transformation type, including **use cases, syntax examples (where applicable), and best practices**.  

---

## **1. Filter Transformation**  
**Purpose:** Selectively include or exclude events based on conditions.  

### **Key Features:**  
- Supports **numeric, text, and boolean conditions** (e.g., `>`, `<`, `=`, `contains`, `is null`).  
- Useful for **data cleansing** (removing invalid records) and **routing logic** (sending only high-priority events to certain destinations).  

### **Example Use Cases:**  
✅ **Remove incomplete records:**  
   - `WHERE userId IS NOT NULL`  
✅ **Filter high-severity logs:**  
   - `WHERE logLevel = "ERROR"`  
✅ **Exclude low-value transactions:**  
   - `WHERE transactionAmount > 1000`  

### **Best Practices:**  
- Apply filters **early** to reduce unnecessary processing.  
- Combine with **Manage Fields** to drop unused columns after filtering.  

---

## **2. Manage Fields Transformation**  
**Purpose:** Modify the structure of incoming data by **adding, removing, renaming, or changing data types** of fields.  

### **Key Features:**  
- **Add derived fields** (e.g., `fullName = firstName + " " + lastName`).  
- **Remove sensitive/unused fields** (e.g., masking credit card numbers).  
- **Convert data types** (e.g., string → timestamp for time-based aggregations).  

### **Example Use Cases:**  
✅ **Anonymize PII:**  
   - Remove `email`, keep `userId`.  
✅ **Flatten nested JSON:**  
   - Extract `event.data.temperature` → `temperature`.  
✅ **Standardize schemas:**  
   - Rename `cust_id` → `customerId`.  

### **Best Practices:**  
- Use **consistent naming conventions** for easier downstream analysis.  
- Validate schema changes to avoid breaking queries in **Eventhouse/Lakehouse**.  

---

## **3. Aggregate Transformation**  
**Purpose:** Compute **sums, averages, min/max, or counts** over a **sliding or tumbling window**.  

### **Key Features:**  
- Supports **time-based windows** (e.g., "per minute," "last 5 minutes").  
- Can **rename aggregated columns** (e.g., `SUM(revenue) → totalRevenue`).  

### **Example Use Cases:**  
✅ **Real-time metrics:**  
   - `SUM(sales) GROUP BY productId, WINDOW 1 MINUTE`  
✅ **Anomaly detection:**  
   - `AVG(networkLatency) WINDOW 5 MINUTES → alert if > 500ms`  

### **Best Practices:**  
- Align window sizes with **downstream dashboard refresh rates** (e.g., 1m for Power BI).  
- Use **Group By** for multi-dimensional aggregations.  

---

## **4. Group By Transformation**  
**Purpose:** Like **Aggregate**, but with **more complex grouping logic** (e.g., multi-column splits).  

### **Key Features:**  
- Group by **multiple fields** (e.g., `region + productCategory`).  
- Supports **session windows** (group events with gaps < X seconds).  

### **Example Use Cases:**  
✅ **E-commerce analysis:**  
   - `COUNT(*) GROUP BY country, deviceType`  
✅ **User session tracking:**  
   - `SESSION(userId, TIMEOUT 30 MINUTES)`  

### **Best Practices:**  
- Avoid over-grouping (high cardinality → performance overhead).  
- Use **Manage Fields** to trim unused dimensions post-aggregation.  

---

## **5. Union Transformation**  
**Purpose:** Merge **two or more streams** with **matching schemas** into one.  

### **Key Features:**  
- **Non-matching fields are dropped** (ensure consistent schemas).  
- Useful for **combining data from multiple sources** (e.g., IoT devices).  

### **Example Use Cases:**  
✅ **Merge logs from multiple apps:**  
   - `App1_logs UNION App2_logs`  
✅ **Consolidate regional sales streams:**  
   - `US_sales UNION EU_sales`  

### **Best Practices:**  
- Use **Manage Fields** first to align schemas.  
- Monitor for **duplicate events** (add a `source` field if needed).  

---

## **6. Expand Transformation**  
**Purpose:** Convert **arrays into separate rows** (similar to SQL `UNNEST`).  

### **Key Features:**  
- Creates **one row per array element**.  
- Preserves all other fields in the original event.  

### **Example Use Cases:**  
✅ **Explode order items:**  
   - `orders → items[]` becomes multiple rows.  
✅ **Process multi-value sensor readings:**  
   - `sensors → temperatureReadings[]`  

### **Best Practices:**  
- Avoid expanding **large arrays** (can bloat event volume).  
- Filter **before expanding** to reduce unnecessary data.  

---

## **7. Join Transformation**  
**Purpose:** Combine **two streams** using a **key-based match** (like SQL `JOIN`).  

### **Key Features:**  
- Supports **inner, left, and outer joins**.  
- Requires a **common key** (e.g., `orderId`).  

### **Example Use Cases:**  
✅ **Enrich transactions with customer data:**  
   - `Transactions JOIN Customers ON customerId`  
✅ **Correlate logs from microservices:**  
   - `ServiceA_logs JOIN ServiceB_logs ON traceId`  

### **Best Practices:**  
- Use **small reference datasets** (joins on large streams are expensive).  
- Consider **Eventhouse** for complex joins instead.  

---

## **Comparison Table: When to Use Each Transformation**  

| Transformation | Best For                          | Example                              |
|---------------|----------------------------------|--------------------------------------|
| **Filter**    | Removing unwanted events         | `WHERE status = "ACTIVE"`            |
| **Manage Fields** | Schema cleanup               | `RENAME device_id → sensorId`        |
| **Aggregate** | Simple time-window metrics       | `SUM(sales) PER MINUTE`              |
| **Group By**  | Multi-dimensional aggregations   | `COUNT(*) BY country, deviceType`    |
| **Union**     | Merging similar streams          | `Stream1 + Stream2`                  |
| **Expand**    | Flattening arrays                | `UNNEST(items)`                      |
| **Join**      | Enriching with reference data    | `Orders JOIN Customers ON customerId`|

---

## **Best Practices for EventStream Transformations**  
1. **Order matters:**  
   - Filter early → Aggregate → Join/Union.  
2. **Monitor performance:**  
   - Complex joins/expansions can increase latency.  
3. **Test transformations** in a development environment before production.  
4. **Use Eventhouse for heavy aggregations** if transformations become too costly.  

---

## **Conclusion**  
Microsoft Fabric’s **EventStream transformations** let you:  
✔ **Clean and filter** data in real time.  
✔ **Aggregate metrics** for live dashboards.  
✔ **Combine streams** for unified analytics.  
✔ **Prepare data** for Lakehouse/Eventhouse storage.  

By strategically applying these transformations, you can build **efficient, real-time data pipelines** that power both **instant actions** (via Fabric Activator) and **historical analysis** (via Lakehouse).

> [!NOTE]
> For more information about supported transformations, see **[Process event data with event processor editor](https://learn.microsoft.com/en-us/fabric/real-time-intelligence/event-streams/process-events-using-event-processor-editor)**.

# **Expanded Explanation: Data Destinations in EventStreams**  

Microsoft Fabric’s **EventStream** allows you to process and route real-time streaming data to multiple destinations, enabling both real-time analytics and long-term storage. Below is a detailed breakdown of each destination option and its use cases.  

---

## **1. Eventhouse**  
**Purpose:** Optimized for high-speed ingestion and querying of real-time event data using **Kusto Query Language (KQL)**.  

### **Key Features & Use Cases:**  
- **High-speed analytics:** Ideal for log analytics, IoT telemetry, and clickstream analysis.  
- **KQL-powered queries:** Enables complex filtering, aggregations, and time-series analysis.  
- **Schema flexibility:** Supports semi-structured and structured data.  
- **Integration with Fabric:** Seamlessly connects with Power BI for real-time dashboards.  

**Example Workflow:**  
1. **Ingest** – IoT sensor data streams into Eventhouse.  
2. **Query** – Use KQL to detect anomalies (e.g., `sensor_data | where temperature > 100`).  
3. **Visualize** – Power BI dashboard shows live sensor health.  

---

## **2. Lakehouse**  
**Purpose:** Stores real-time events in **Delta Lake format** for historical analysis, combining batch and streaming data.  

### **Key Features & Use Cases:**  
- **Delta Lake storage:** Ensures ACID transactions and schema enforcement.  
- **ETL/ELT support:** Transform data before storage (e.g., filtering, enrichment).  
- **Unified analytics:** Combines real-time and batch data for ML and reporting.  
- **Time travel:** Query historical snapshots of streaming data.  

**Example Workflow:**  
1. **Ingest** – Social media posts stream into EventStream.  
2. **Transform** – Remove spam, enrich with sentiment analysis.  
3. **Store** – Save as Delta tables in Lakehouse.  
4. **Analyze** – Run Spark SQL for trend analysis.  

---

## **3. Derived Stream**  
**Purpose:** Routes processed data to **another EventStream** for further transformations or distribution.  

### **Key Features & Use Cases:**  
- **Multi-stage processing:** Chain multiple EventStreams (e.g., filter → aggregate → route).  
- **Decoupled workflows:** Separate transformation logic for modularity.  
- **Fan-out patterns:** Send the same stream to multiple destinations.  

**Example Workflow:**  
1. **First EventStream** – Filters raw logs to exclude errors.  
2. **Derived Stream** – Routes cleaned logs to a second EventStream.  
3. **Second EventStream** – Aggregates logs by user for analytics.  

---

## **4. Fabric Activator**  
**Purpose:** Triggers **automated actions** based on real-time data.  

### **Key Features & Use Cases:**  
- **Real-time automation:** Respond to events instantly (e.g., alerts, workflows).  
- **Low-code rules engine:** Define conditions without coding (e.g., "If stock price < $50, send alert").  
- **Integration with Power Automate:** Extend automation to external systems.  

**Example Workflow:**  
1. **Ingest** – Stock market trades stream into EventStream.  
2. **Detect** – Activator monitors for sudden price drops.  
3. **Act** – Triggers an email/SMS alert via Power Automate.  

---

## **5. Custom Endpoint**  
**Purpose:** Sends real-time data to **external systems** outside Microsoft Fabric.  

### **Key Features & Use Cases:**  
- **Third-party integrations:** Push data to APIs, databases, or SaaS tools.  
- **Hybrid architectures:** Connect on-premises and cloud systems.  
- **Protocol support:** REST, Webhooks, Azure Event Hubs, Kafka, etc.  

**Example Workflow:**  
1. **Ingest** – E-commerce orders stream into EventStream.  
2. **Route** – Send to a custom API for fraud detection.  
3. **Process** – External system validates transactions in real time.  

---

## **Comparison Table**  

| Destination       | Best For                          | Storage Format    | Query Language  | Automation Support |
|------------------|----------------------------------|------------------|----------------|--------------------|
| **Eventhouse**   | Real-time analytics, KQL queries | Columnar (Kusto) | KQL            | Limited            |
| **Lakehouse**    | Historical analysis, ML          | Delta Lake       | Spark SQL      | No (but integrates with Spark jobs) |
| **Derived Stream** | Multi-stage stream processing  | In-memory stream | N/A            | No                 |
| **Fabric Activator** | Real-time triggers & actions | N/A              | Rule-based     | Yes (Power Automate) |
| **Custom Endpoint** | External integrations         | Depends on target | N/A            | Yes (via API calls) |

---

## **Conclusion**  
Microsoft Fabric’s **EventStream** provides flexible routing options to support:  
✅ **Real-time analytics** (Eventhouse)  
✅ **Long-term storage & batch processing** (Lakehouse)  
✅ **Multi-stage stream processing** (Derived Stream)  
✅ **Automated actions** (Fabric Activator)  
✅ **External system integrations** (Custom Endpoint)  

By choosing the right destination(s), organizations can build **end-to-end real-time data pipelines** that power live dashboards, automated workflows, and advanced analytics - all within a unified Fabric ecosystem.


> [!Question]
> For more information about supported sources, see **[Add and manage a destination in an eventstream](https://learn.microsoft.com/en-us/fabric/real-time-intelligence/event-streams/add-manage-eventstream-destinations)**.